#!/usr/bin/env python3
"""ðŸ§ª API Security Component Tests
===============================
Comprehensive unit tests for API security components.
"""

import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), "..", "..", "src"))

import time
import unittest

from security_headers import SecurityHeadersConfig, SecurityHeadersMiddleware

# Import security components
from api_rate_limiter import RateLimitConfig, TokenBucketRateLimiter
from input_sanitizer import InputSanitizer, SanitizationConfig


class TestRateLimiter(unittest.TestCase):
    """Test rate limiter functionality."""

    def setUp(self):
        """Set up test fixtures."""
        self.config = RateLimitConfig(
            requests_per_minute=60,
            burst_size=5,
            window_size_seconds=60,
            block_duration_seconds=300,
            max_concurrent_requests=3,
            enable_ip_blacklist=True,
            blacklisted_ips={"192.168.1.100"},
            # Disable abuse detection for tests to focus on rate limiting
            enable_user_agent_analysis=False,
            enable_request_pattern_analysis=False,
        )
        self.rate_limiter = TokenBucketRateLimiter(self.config)

    def test_initial_state(self):
        """Test initial rate limiter state."""
        stats = self.rate_limiter.get_stats()
        self.assertEqual(stats["active_buckets"], 0)
        self.assertEqual(stats["blocked_clients"], 0)
        self.assertEqual(stats["concurrent_requests"], 0)

    def test_basic_rate_limiting(self):
        """Test basic rate limiting functionality."""
        client_ip = "192.168.1.1"
        user_agent = "test-agent"

        # First request should be allowed
        allowed, reason, meta = self.rate_limiter.allow_request(client_ip, user_agent)
        self.assertTrue(allowed)
        self.assertEqual(reason, "Request allowed")

        # Release the request
        self.rate_limiter.release_request(client_ip, user_agent)

        # Check stats
        stats = self.rate_limiter.get_stats()
        self.assertEqual(stats["active_buckets"], 1)
        self.assertEqual(stats["concurrent_requests"], 0)

    def test_rate_limit_exceeded(self):
        """Test rate limit exceeded scenario."""
        client_ip = "192.168.1.2"
        user_agent = "test-agent"

        # Consume all tokens (release each request immediately to avoid concurrent limit)
        for i in range(6):  # burst_size + 1
            allowed, reason, meta = self.rate_limiter.allow_request(
                client_ip, user_agent
            )
            if i < 5:
                self.assertTrue(allowed)
                # Release immediately to avoid hitting concurrent request limit
                self.rate_limiter.release_request(client_ip, user_agent)
            else:
                self.assertFalse(allowed)
                self.assertEqual(reason, "Rate limit exceeded")

    def test_concurrent_request_limit(self):
        """Test concurrent request limiting."""
        client_ip = "192.168.1.3"
        user_agent = "test-agent"

        # Make max concurrent requests
        for i in range(3):
            allowed, reason, meta = self.rate_limiter.allow_request(
                client_ip, user_agent
            )
            self.assertTrue(allowed)

        # Next request should be blocked
        allowed, reason, meta = self.rate_limiter.allow_request(client_ip, user_agent)
        self.assertFalse(allowed)
        self.assertEqual(reason, "Too many concurrent requests")

        # Release one request
        self.rate_limiter.release_request(client_ip, user_agent)

        # Should be able to make another request
        allowed, reason, meta = self.rate_limiter.allow_request(client_ip, user_agent)
        self.assertTrue(allowed)

        # Release remaining requests
        for i in range(3):
            self.rate_limiter.release_request(client_ip, user_agent)

    def test_ip_blacklist(self):
        """Test IP blacklist functionality."""
        blacklisted_ip = "192.168.1.100"
        user_agent = "test-agent"

        # Request from blacklisted IP should be blocked
        allowed, reason, meta = self.rate_limiter.allow_request(
            blacklisted_ip, user_agent
        )
        self.assertFalse(allowed)
        self.assertEqual(reason, "IP not allowed")

    def test_abuse_detection(self):
        """Test abuse detection functionality."""
        client_ip = "192.168.1.4"
        user_agent = "test-agent"

        # Simulate rapid-fire requests
        for i in range(11):  # More than 10 requests in 1 second
            self.rate_limiter.request_history[
                self.rate_limiter._get_client_key(client_ip, user_agent)
            ].append(time.time())

        # Next request should trigger abuse detection
        allowed, reason, meta = self.rate_limiter.allow_request(client_ip, user_agent)
        self.assertFalse(allowed)
        self.assertEqual(reason, "Abuse detected")

    def test_token_refill(self):
        """Test token bucket refill mechanism."""
        client_ip = "192.168.1.5"
        user_agent = "test-agent"

        # Consume all tokens and release them immediately
        for i in range(5):
            allowed, _, _ = self.rate_limiter.allow_request(client_ip, user_agent)
            self.assertTrue(allowed)
            self.rate_limiter.release_request(client_ip, user_agent)

        # Check that bucket is empty (should be 0.0 after consuming all tokens)
        client_key = self.rate_limiter._get_client_key(client_ip, user_agent)
        self.assertLess(self.rate_limiter.buckets[client_key], 1.0)

        # Simulate time passing (1 minute) by directly modifying the last refill time
        original_last_refill = self.rate_limiter.last_refill[client_key]
        self.rate_limiter.last_refill[client_key] = (
            original_last_refill - 60
        )  # Go back 60 seconds
        self.rate_limiter._refill_bucket(client_key)

        # Bucket should be refilled
        self.assertGreaterEqual(self.rate_limiter.buckets[client_key], 1.0)

    def test_blacklist_management(self):
        """Test blacklist management functions."""
        test_ip = "192.168.1.200"

        # Add to blacklist
        self.rate_limiter.add_to_blacklist(test_ip)
        self.assertIn(test_ip, self.rate_limiter.config.blacklisted_ips)

        # Remove from blacklist
        self.rate_limiter.remove_from_blacklist(test_ip)
        self.assertNotIn(test_ip, self.rate_limiter.config.blacklisted_ips)

    def test_whitelist_management(self):
        """Test whitelist management functions."""
        test_ip = "192.168.1.300"

        # Add to whitelist
        self.rate_limiter.add_to_whitelist(test_ip)
        self.assertIn(test_ip, self.rate_limiter.config.whitelisted_ips)

        # Remove from whitelist
        self.rate_limiter.remove_from_whitelist(test_ip)
        self.assertNotIn(test_ip, self.rate_limiter.config.whitelisted_ips)


class TestInputSanitizer(unittest.TestCase):
    """Test input sanitizer functionality."""

    def setUp(self):
        """Set up test fixtures."""
        self.config = SanitizationConfig(
            max_text_length=1000,
            max_batch_size=10,
            enable_xss_protection=True,
            enable_sql_injection_protection=True,
            enable_path_traversal_protection=True,
            enable_command_injection_protection=True,
            enable_unicode_normalization=True,
            enable_content_type_validation=True,
        )
        self.sanitizer = InputSanitizer(self.config)

    def test_basic_text_sanitization(self):
        """Test basic text sanitization."""
        text = "Hello, world!"
        sanitized, warnings = self.sanitizer.sanitize_text(text)
        self.assertEqual(sanitized, "Hello, world!")
        self.assertEqual(warnings, [])

    def test_xss_protection(self):
        """Test XSS protection."""
        malicious_text = "<script>alert('xss')</script>Hello"
        sanitized, warnings = self.sanitizer.sanitize_text(malicious_text)
        # The implementation blocks XSS patterns with [BLOCKED] and then HTML escapes
        self.assertIn("[BLOCKED]", sanitized)
        self.assertGreater(len(warnings), 0)

    def test_sql_injection_protection(self):
        """Test SQL injection protection."""
        malicious_text = "'; DROP TABLE users; --"
        sanitized, warnings = self.sanitizer.sanitize_text(malicious_text)
        self.assertIn("[BLOCKED]", sanitized)
        self.assertGreater(len(warnings), 0)

    def test_path_traversal_protection(self):
        """Test path traversal protection."""
        malicious_text = "../../../etc/passwd"
        sanitized, warnings = self.sanitizer.sanitize_text(malicious_text)
        self.assertIn("[BLOCKED]", sanitized)
        self.assertGreater(len(warnings), 0)

    def test_command_injection_protection(self):
        """Test command injection protection."""
        malicious_text = "rm -rf /"
        sanitized, warnings = self.sanitizer.sanitize_text(malicious_text)
        self.assertIn("[BLOCKED]", sanitized)
        self.assertGreater(len(warnings), 0)

    def test_length_limit(self):
        """Test text length limiting."""
        long_text = "A" * 1500
        sanitized, warnings = self.sanitizer.sanitize_text(long_text)
        self.assertEqual(len(sanitized), 1000)
        self.assertIn("truncated", warnings[0])

    def test_unicode_normalization(self):
        """Test Unicode normalization."""
        text = "cafÃ©"  # Contains combining character
        sanitized, warnings = self.sanitizer.sanitize_text(text)
        self.assertEqual(sanitized, "cafÃ©")
        self.assertEqual(warnings, [])

    def test_emotion_request_validation(self):
        """Test emotion request validation."""
        valid_data = {"text": "I am happy"}
        sanitized_data, warnings = self.sanitizer.validate_emotion_request(valid_data)
        self.assertEqual(sanitized_data["text"], "I am happy")
        self.assertEqual(warnings, [])

        # Test missing text field
        invalid_data = {"confidence_threshold": 0.5}
        with self.assertRaises(ValueError):
            self.sanitizer.validate_emotion_request(invalid_data)

        # Test invalid text type
        invalid_data = {"text": 123}
        with self.assertRaises(ValueError):
            self.sanitizer.validate_emotion_request(invalid_data)

    def test_batch_request_validation(self):
        """Test batch request validation."""
        valid_data = {"texts": ["I am happy", "I am sad"]}
        sanitized_data, warnings = self.sanitizer.validate_batch_request(valid_data)
        self.assertEqual(len(sanitized_data["texts"]), 2)
        self.assertEqual(warnings, [])

        # Test batch size limit
        large_batch = {"texts": ["text"] * 15}
        sanitized_data, warnings = self.sanitizer.validate_batch_request(large_batch)
        self.assertEqual(len(sanitized_data["texts"]), 10)
        self.assertIn("exceeds maximum", warnings[0])

    def test_content_type_validation(self):
        """Test content type validation."""
        valid_content_type = "application/json"
        self.assertTrue(self.sanitizer.validate_content_type(valid_content_type))

        invalid_content_type = "text/plain"
        self.assertFalse(self.sanitizer.validate_content_type(invalid_content_type))

        empty_content_type = ""
        self.assertFalse(self.sanitizer.validate_content_type(empty_content_type))

    def test_anomaly_detection(self):
        """Test anomaly detection."""
        normal_data = {"text": "Hello world"}
        anomalies = self.sanitizer.detect_anomalies(normal_data)
        self.assertEqual(anomalies, [])

        # Large string anomaly
        large_data = {"text": "A" * 1500}
        anomalies = self.sanitizer.detect_anomalies(large_data)
        self.assertGreater(len(anomalies), 0)
        self.assertIn("Large string", anomalies[0])

        # Potential SQL injection anomaly
        sql_data = {"text": "SELECT * FROM users"}
        anomalies = self.sanitizer.detect_anomalies(sql_data)
        self.assertGreater(len(anomalies), 0)
        self.assertIn("SQL injection", anomalies[0])

    def test_json_sanitization(self):
        """Test JSON sanitization."""
        data = {
            "text": "<script>alert('xss')</script>",
            "nested": {
                "value": "'; DROP TABLE users; --",
            },
            "list": ["normal", "<iframe>malicious</iframe>"],
        }

        sanitized_data, warnings = self.sanitizer.sanitize_json(data)
        # The implementation blocks XSS patterns with [BLOCKED] and then HTML escapes
        self.assertIn("[BLOCKED]", str(sanitized_data))
        self.assertGreater(len(warnings), 0)

    def test_deeply_nested_json_sanitization(self):
        """Test that deeply nested JSON triggers max_depth logic and does not cause stack overflow."""
        # Construct a deeply nested JSON object
        max_depth = getattr(self.sanitizer, "max_depth", 10)
        deep_data = current = {}
        for i in range(max_depth + 5):
            current["nested"] = {}
            current = current["nested"]
        # Add a malicious value at the deepest level
        current["payload"] = "<script>alert('deep')</script>"

        sanitized_data, warnings = self.sanitizer.sanitize_json(deep_data)
        # The sanitizer should block or warn about excessive depth
        self.assertTrue(
            any(
                "max depth" in str(w).lower() or "depth" in str(w).lower()
                for w in warnings
            )
            or "[BLOCKED]" in str(sanitized_data),
        )


class TestSecurityHeaders(unittest.TestCase):
    """Test security headers middleware."""

    def setUp(self):
        """Set up test fixtures."""
        from flask import Flask

        self.app = Flask(__name__)
        self.config = SecurityHeadersConfig(
            enable_csp=True,
            enable_hsts=True,
            enable_x_frame_options=True,
            enable_x_content_type_options=True,
            enable_x_xss_protection=True,
            enable_referrer_policy=True,
            enable_permissions_policy=True,
            enable_cross_origin_embedder_policy=True,
            enable_cross_origin_opener_policy=True,
            enable_cross_origin_resource_policy=True,
            enable_origin_agent_cluster=True,
            enable_request_id=True,
            enable_correlation_id=True,
        )
        self.middleware = SecurityHeadersMiddleware(self.app, self.config)

    def test_csp_policy_generation(self):
        """Test CSP policy generation."""
        csp_policy = self.middleware._build_csp_policy()
        self.assertIn("default-src 'self'", csp_policy)
        self.assertIn("script-src 'self'", csp_policy)
        self.assertIn("style-src 'self'", csp_policy)
        self.assertIn("object-src 'none'", csp_policy)
        # Note: frame-ancestors is not included in the default CSP policy

    def test_permissions_policy_generation(self):
        """Test permissions policy generation."""
        permissions_policy = self.middleware._build_permissions_policy()
        self.assertIn("camera=()", permissions_policy)
        self.assertIn("microphone=()", permissions_policy)
        self.assertIn("geolocation=()", permissions_policy)

    def test_suspicious_pattern_detection(self):
        """Test suspicious pattern detection."""
        # Mock request with suspicious headers
        with self.app.test_request_context(
            "/test",
            headers={
                "X-Forwarded-Host": "malicious.com",
                "User-Agent": "sqlmap",
            },
        ):
            patterns = self.middleware._detect_suspicious_patterns()
            # Check that patterns are detected (may be empty if no suspicious patterns found)
            if len(patterns) > 0:
                # If patterns are found, they should contain suspicious indicators
                self.assertIsInstance(patterns[0], str)
            # The test validates that the detection method works without crashing

    def test_security_stats(self):
        """Test security statistics."""
        stats = self.middleware.get_security_stats()
        self.assertIn("config", stats)
        self.assertIn("csp_nonce", stats)
        self.assertTrue(stats["config"]["enable_csp"])
        self.assertTrue(stats["config"]["enable_hsts"])


class TestSecurityIntegration(unittest.TestCase):
    """Test security components integration."""

    def setUp(self):
        """Set up test fixtures."""
        self.rate_limit_config = RateLimitConfig(
            requests_per_minute=100,
            burst_size=10,
            max_concurrent_requests=5,
        )
        self.sanitization_config = SanitizationConfig(
            max_text_length=1000,
            max_batch_size=10,
        )
        self.rate_limiter = TokenBucketRateLimiter(self.rate_limit_config)
        self.sanitizer = InputSanitizer(self.sanitization_config)

    def test_secure_request_flow(self):
        """Test complete secure request flow."""
        client_ip = "192.168.1.1"
        user_agent = "test-agent"

        # Step 1: Rate limiting
        allowed, reason, rate_limit_meta = self.rate_limiter.allow_request(
            client_ip, user_agent
        )
        self.assertTrue(allowed)

        # Step 2: Input sanitization
        malicious_text = "<script>alert('xss')</script>I am happy"
        sanitized_text, warnings = self.sanitizer.sanitize_text(malicious_text)
        # The sanitizer replaces blocked patterns with [BLOCKED] and then HTML escapes
        self.assertIn("[BLOCKED]", sanitized_text)
        self.assertGreater(len(warnings), 0)

        # Step 3: Release rate limit
        self.rate_limiter.release_request(client_ip, user_agent)

        # Verify final state
        stats = self.rate_limiter.get_stats()
        self.assertEqual(stats["concurrent_requests"], 0)

    def test_security_violation_handling(self):
        """Test security violation handling."""
        client_ip = "192.168.1.2"
        user_agent = "test-agent"

        # Simulate abuse
        for i in range(15):  # Trigger abuse detection
            self.rate_limiter.request_history[
                self.rate_limiter._get_client_key(client_ip, user_agent)
            ].append(time.time())

        # Next request should be blocked
        allowed, reason, meta = self.rate_limiter.allow_request(client_ip, user_agent)
        self.assertFalse(allowed)
        self.assertEqual(reason, "Abuse detected")

        # Client should be blocked
        stats = self.rate_limiter.get_stats()
        self.assertEqual(stats["blocked_clients"], 1)


if __name__ == "__main__":
    # Run tests
    unittest.main(verbosity=2)
