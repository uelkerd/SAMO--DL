{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uelkerd/SAMO--DL/blob/main/notebooks/domain_adaptation_gpu_training2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A1klsR924hZ"
      },
      "source": [
        "# SAMO Deep Learning - Domain Adaptation GPU Training\n",
        "\n",
        "## 🎯 REQ-DL-012: Domain-Adapted Emotion Detection\n",
        "\n",
        "**Target**: Achieve 70% F1 score on journal entries through domain adaptation from GoEmotions (Reddit comments) to personal journal writing style.\n",
        "\n",
        "### Key Objectives:\n",
        "- Bridge domain gap between Reddit comments and journal entries\n",
        "- Implement focal loss for class imbalance\n",
        "- Use domain adaptation techniques for better transfer learning\n",
        "- Optimize for GPU training on Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwDTzWp024ha"
      },
      "source": [
        "## 🚀 Environment Setup & GPU Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2dada6dd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "3c3f62be",
        "outputId": "0e29c591-25d2-4760-fc0a-10d49e4da23c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "diffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "gradio 5.38.2 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "peft 0.16.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "accelerate 1.9.0 requires huggingface_hub>=0.21.0, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "775b08da869d48b6aa44b95f6ef50414",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.1.0\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Force reinstall compatible versions to ensure a clean environment\n",
        "!pip uninstall numpy -y\n",
        "!pip install numpy==1.26.4\n",
        "!pip install --force-reinstall torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install --force-reinstall transformers==4.35.0\n",
        "!pip install --force-reinstall requests==2.32.4 fsspec==2025.3.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fYZxpNb24ha",
        "outputId": "3bc4828a-f56a-4970-d5fc-c99066aaa5b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-3023728975.py\", line 2, in <cell line: 0>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1382, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available: True\n",
            "GPU: Tesla T4\n",
            "Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Verify GPU availability\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ No GPU available. Training will be slow on CPU.\")\n",
        "\n",
        "# Enable cudnn benchmarking for faster training\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXmPBLK524ha"
      },
      "source": [
        "## 📦 Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "arIQO8Oy24ha",
        "outputId": "696ed694-bb68-4eeb-fa64-61cba0342831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "diffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.0 which is incompatible.\n",
            "gradio-client 1.11.0 requires huggingface-hub>=0.19.3, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "gradio 5.38.2 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "peft 0.16.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "accelerate 1.9.0 requires huggingface_hub>=0.21.0, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20250625)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Collecting huggingface_hub>=0.21.0 (from accelerate)\n",
            "  Using cached huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.34.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.1.0)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2023.10.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.7.14)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2025.7.34)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Using cached huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
            "Installing collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.17.3\n",
            "    Uninstalling huggingface-hub-0.17.3:\n",
            "      Successfully uninstalled huggingface-hub-0.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.34.3 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface_hub-0.34.3\n",
            "/content/SAMO--DL\n",
            "From https://github.com/uelkerd/SAMO--DL\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "/content/SAMO--DL\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch>=2.1.0 torchvision>=0.16.0 torchaudio>=2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers>=4.30.0 datasets>=2.13.0 evaluate scikit-learn pandas numpy matplotlib seaborn\n",
        "!pip install accelerate wandb pydub openai-whisper jiwer\n",
        "\n",
        "\n",
        "%cd SAMO--DL\n",
        "!git pull origin main\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxMRbAyX24ha"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## 🔍 Domain Gap Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "kt1882fN24ha",
        "outputId": "23cceb51-7227-4590-f399-17496964d127"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Loading datasets...\n",
            "\n",
            "🔍 Domain Gap Analysis:\n",
            "GoEmotions (Reddit) Style Analysis:\n",
            "  Average length: 12.4 words\n",
            "  Personal pronouns: 40.3%\n",
            "  Reflection words: 5.2%\n",
            "Journal Entries Style Analysis:\n",
            "  Average length: 39.3 words\n",
            "  Personal pronouns: 100.0%\n",
            "  Reflection words: 76.7%\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎯 Key Insights:\n",
            "- Journal entries are 3.2x longer\n",
            "- Journal entries use 2.5x more personal pronouns\n",
            "- Journal entries contain 14.7x more reflection words\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def analyze_writing_style(texts, domain_name):\n",
        "    \"\"\"Analyze writing style characteristics of a domain.\"\"\"\n",
        "    avg_length = np.mean([len(text.split()) for text in texts])\n",
        "    personal_pronouns = sum(['I ' in text or 'my ' in text or 'me ' in text for text in texts]) / len(texts)\n",
        "    reflection_words = sum(['think' in text.lower() or 'feel' in text.lower() or 'believe' in text.lower()\n",
        "                           for text in texts]) / len(texts)\n",
        "\n",
        "    print(f\"{domain_name} Style Analysis:\")\n",
        "    print(f\"  Average length: {avg_length:.1f} words\")\n",
        "    print(f\"  Personal pronouns: {personal_pronouns:.1%}\")\n",
        "    print(f\"  Reflection words: {reflection_words:.1%}\")\n",
        "\n",
        "    return {\n",
        "        'avg_length': avg_length,\n",
        "        'personal_pronouns': personal_pronouns,\n",
        "        'reflection_words': reflection_words\n",
        "    }\n",
        "\n",
        "# Load datasets\n",
        "print(\"📊 Loading datasets...\")\n",
        "\n",
        "# Load GoEmotions dataset\n",
        "go_emotions = load_dataset(\"go_emotions\", \"simplified\")\n",
        "go_texts = go_emotions['train']['text'][:1000]  # Sample for analysis\n",
        "\n",
        "# Load journal dataset\n",
        "with open('data/journal_test_dataset.json', 'r') as f:\n",
        "    journal_entries = json.load(f)\n",
        "\n",
        "journal_df = pd.DataFrame(journal_entries)\n",
        "journal_texts = journal_df['content'].tolist()\n",
        "\n",
        "# Analyze domains\n",
        "print(\"\\n🔍 Domain Gap Analysis:\")\n",
        "go_analysis = analyze_writing_style(go_texts, \"GoEmotions (Reddit)\")\n",
        "journal_analysis = analyze_writing_style(journal_texts, \"Journal Entries\")\n",
        "\n",
        "# Visualize differences\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "metrics = ['avg_length', 'personal_pronouns', 'reflection_words']\n",
        "labels = ['Avg Length (words)', 'Personal Pronouns', 'Reflection Words']\n",
        "\n",
        "for i, (metric, label) in enumerate(zip(metrics, labels)):\n",
        "    axes[i].bar(['GoEmotions', 'Journal'],\n",
        "                [go_analysis[metric], journal_analysis[metric]])\n",
        "    axes[i].set_title(label)\n",
        "    axes[i].set_ylabel('Percentage' if 'pronouns' in metric or 'reflection' in metric else 'Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n🎯 Key Insights:\")\n",
        "print(f\"- Journal entries are {journal_analysis['avg_length']/go_analysis['avg_length']:.1f}x longer\")\n",
        "print(f\"- Journal entries use {journal_analysis['personal_pronouns']/go_analysis['personal_pronouns']:.1f}x more personal pronouns\")\n",
        "print(f\"- Journal entries contain {journal_analysis['reflection_words']/go_analysis['reflection_words']:.1f}x more reflection words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa4RoFII24ha"
      },
      "source": [
        "## 🏗️ Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ySgAwqr524ha"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss for addressing class imbalance in emotion detection.\"\"\"\n",
        "\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "class DomainAdaptedEmotionClassifier(nn.Module):\n",
        "    \"\"\"BERT-based emotion classifier with domain adaptation capabilities.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", num_labels=12, dropout=0.3):\n",
        "        super().__init__()\n",
        "        print(f\"Initializing DomainAdaptedEmotionClassifier with num_labels = {num_labels}\")\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        # Emotion classification\n",
        "        emotion_logits = self.classifier(self.dropout(pooled_output))\n",
        "\n",
        "        return emotion_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "223c52f7",
        "outputId": "655f5cb7-56a6-48c8-8e43-d59ac66f71b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏗️ Initializing model...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'label_encoder' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1875660754.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDomainAdaptedEmotionClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'label_encoder' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss for addressing class imbalance in emotion detection.\"\"\"\n",
        "\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "class DomainAdaptedEmotionClassifier(nn.Module):\n",
        "    \"\"\"BERT-based emotion classifier with domain adaptation capabilities.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", num_labels=12, dropout=0.3):\n",
        "        super().__init__()\n",
        "        print(f\"Initializing DomainAdaptedEmotionClassifier with num_labels = {num_labels}\")\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        # Emotion classification\n",
        "        emotion_logits = self.classifier(self.dropout(pooled_output))\n",
        "\n",
        "        return emotion_logits\n",
        "# Initialize model and tokenizer\n",
        "print(\"🏗️ Initializing model...\")\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = DomainAdaptedEmotionClassifier(model_name=model_name, num_labels=len(label_encoder.classes_))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"✅ Model loaded on {device}\")\n",
        "print(f\"📊 Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "9e1f2350",
        "outputId": "5212666c-c163-49a9-fb35-cdddf4471857"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏗️ Initializing model...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'label_encoder' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1021039676.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDomainAdaptedEmotionClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'label_encoder' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize model and tokenizer\n",
        "print(\"🏗️ Initializing model...\")\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = DomainAdaptedEmotionClassifier(model_name=model_name, num_labels=len(label_encoder.classes_))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"✅ Model loaded on {device}\")\n",
        "print(f\"📊 Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "1a5ecdb4",
        "outputId": "6ae974d6-73ab-42d6-bcee-368e5c5f9d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Debug: Label Analysis\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'label_encoder' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3031607729.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Debug: Check label ranges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🔍 Debug: Label Analysis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Label encoder classes: {len(label_encoder.classes_)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model num_labels: {model.classifier.out_features}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"GoEmotions label range: {go_encoded_labels.min()} to {go_encoded_labels.max()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'label_encoder' is not defined"
          ]
        }
      ],
      "source": [
        "# Debug: Check label ranges\n",
        "print(\"🔍 Debug: Label Analysis\")\n",
        "print(f\"Label encoder classes: {len(label_encoder.classes_)}\")\n",
        "print(f\"Model num_labels: {model.classifier.out_features}\")\n",
        "print(f\"GoEmotions label range: {go_encoded_labels.min()} to {go_encoded_labels.max()}\")\n",
        "print(f\"Journal label range: {journal_encoded_labels.min()} to {journal_encoded_labels.max()}\")\n",
        "\n",
        "# Check for any labels >= model output size\n",
        "max_label = max(go_encoded_labels.max(), journal_encoded_labels.max())\n",
        "if max_label >= model.classifier.out_features:\n",
        "    print(f\"❌ ERROR: Max label {max_label} >= model output size {model.classifier.out_features}\")\n",
        "else:\n",
        "    print(f\"✅ Labels are within valid range (0 to {model.classifier.out_features - 1})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMs0Tkbz24hb"
      },
      "source": [
        "## 📊 Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWpVZcSc24hb",
        "outputId": "0bb3a41f-372a-420f-cbaa-7fa8698699fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Preparing GoEmotions data...\n",
            "📊 Preparing journal data...\n",
            "🧬 Creating a unified label encoder...\n",
            "✅ Data prepared:\n",
            "  GoEmotions: 10000 samples\n",
            "  Journal Train: 120 samples\n",
            "  Journal Val: 30 samples\n",
            "  Total classes: 40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    \"\"\"Custom dataset for emotion classification.\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            text = str(self.texts[idx])\n",
        "            label = self.labels[idx]\n",
        "\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].flatten(),\n",
        "                'attention_mask': encoding['attention_mask'].flatten(),\n",
        "                'labels': torch.tensor(label, dtype=torch.long)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing text at index {idx}: {text}\")\n",
        "            raise e\n",
        "\n",
        "# Prepare GoEmotions data\n",
        "print(\"📊 Preparing GoEmotions data...\")\n",
        "go_train = go_emotions['train']\n",
        "go_texts = go_train['text'][:10000]  # Use subset for faster training\n",
        "go_labels = go_train['labels'][:10000]\n",
        "\n",
        "# Get the label names from the go_emotions dataset features\n",
        "go_label_names = go_train.features['labels'].feature.names\n",
        "\n",
        "# Convert multi-label to single-label and map to string names\n",
        "go_single_labels_int = [label[0] if label else 0 for label in go_labels]\n",
        "go_single_labels_str = [go_label_names[i] for i in go_single_labels_int]\n",
        "\n",
        "# Prepare journal data\n",
        "print(\"📊 Preparing journal data...\")\n",
        "journal_texts = journal_df['content'].tolist()\n",
        "journal_emotions = journal_df['emotion'].tolist()\n",
        "\n",
        "# Create a unified label encoder from all string labels\n",
        "print(\"🧬 Creating a unified label encoder...\")\n",
        "label_encoder = LabelEncoder()\n",
        "all_emotions = list(set(go_single_labels_str) | set(journal_emotions))\n",
        "label_encoder.fit(all_emotions)\n",
        "\n",
        "# Encode all labels using the unified encoder\n",
        "go_encoded_labels = label_encoder.transform(go_single_labels_str)\n",
        "journal_encoded_labels = label_encoder.transform(journal_emotions)\n",
        "\n",
        "# Split journal data\n",
        "journal_train_texts, journal_val_texts, journal_train_labels, journal_val_labels = train_test_split(\n",
        "    journal_texts, journal_encoded_labels, test_size=0.2, random_state=42, stratify=journal_encoded_labels\n",
        ")\n",
        "\n",
        "# Create datasets\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "go_dataset = EmotionDataset(go_texts, go_encoded_labels, tokenizer)\n",
        "journal_train_dataset = EmotionDataset(journal_train_texts, journal_train_labels, tokenizer)\n",
        "journal_val_dataset = EmotionDataset(journal_val_texts, journal_val_labels, tokenizer)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 16\n",
        "go_loader = DataLoader(go_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "journal_train_loader = DataLoader(journal_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "journal_val_loader = DataLoader(journal_val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"✅ Data prepared:\")\n",
        "print(f\"  GoEmotions: {len(go_dataset)} samples\")\n",
        "print(f\"  Journal Train: {len(journal_train_dataset)} samples\")\n",
        "print(f\"  Journal Val: {len(journal_val_dataset)} samples\")\n",
        "print(f\"  Total classes: {len(label_encoder.classes_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN1EUUot24hb"
      },
      "source": [
        "## 🎯 Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "qpYoSOEI24hb",
        "outputId": "c078cdeb-662e-472e-ea27-4c64f76bd252"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3603373179.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Initialize trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDomainAdaptationTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, f1_score\n",
        "import wandb\n",
        "\n",
        "class DomainAdaptationTrainer:\n",
        "    \"\"\"Trainer for domain adaptation training.\"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.criterion = FocalLoss(alpha=1, gamma=2)\n",
        "        self.domain_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train_step(self, batch, domain_labels=None, lambda_domain=0.1):\n",
        "        \"\"\"Single training step with domain adaptation.\"\"\"\n",
        "        self.model.train()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(self.device)\n",
        "        attention_mask = batch['attention_mask'].to(self.device)\n",
        "        labels = batch['labels'].to(self.device)\n",
        "\n",
        "        # Forward pass\n",
        "        emotion_logits = self.model(input_ids, attention_mask)\n",
        "\n",
        "        # Calculate losses\n",
        "        emotion_loss = self.criterion(emotion_logits, labels)\n",
        "\n",
        "        return {\n",
        "            'total_loss': emotion_loss,\n",
        "            'emotion_loss': emotion_loss,\n",
        "            'domain_loss': torch.tensor(0.0)\n",
        "        }\n",
        "\n",
        "    def evaluate(self, dataloader):\n",
        "        \"\"\"Evaluate model on validation set.\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                emotion_logits = self.model(input_ids, attention_mask)\n",
        "                loss = self.criterion(emotion_logits, labels)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                predictions = torch.argmax(emotion_logits, dim=1)\n",
        "\n",
        "                all_predictions.extend(predictions.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
        "        f1_weighted = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "        return {\n",
        "            'loss': total_loss / len(dataloader),\n",
        "            'f1_macro': f1_macro,\n",
        "            'f1_weighted': f1_weighted\n",
        "        }\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = DomainAdaptationTrainer(model, tokenizer, device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "# Initialize wandb (optional)\n",
        "try:\n",
        "    wandb.init(project=\"samo-domain-adaptation\", name=\"journal-emotion-detection\")\n",
        "    use_wandb = True\n",
        "except:\n",
        "    print(\"⚠️ Wandb not available, continuing without logging\")\n",
        "    use_wandb = False\n",
        "\n",
        "print(\"🎯 Starting domain adaptation training...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "IbBVCqow24hb",
        "outputId": "0d63f2eb-1a14-40fb-bc6a-0eb04e72b01c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔄 Epoch 1/5\n",
            "  📚 Training on GoEmotions data...\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3287900490.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  📚 Training on GoEmotions data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgo_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3603373179.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch, domain_labels, lambda_domain)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 5\n",
        "best_f1 = 0\n",
        "training_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n🔄 Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Train on GoEmotions data\n",
        "    print(\"  📚 Training on GoEmotions data...\")\n",
        "    for i, batch in enumerate(go_loader):\n",
        "        losses = trainer.train_step(batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses['total_loss'].backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += losses['total_loss'].item()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"    Batch {i}/{len(go_loader)}, Loss: {losses['total_loss'].item():.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    print(\"  🎯 Validating on journal test set...\")\n",
        "    val_results = trainer.evaluate(journal_val_loader)\n",
        "\n",
        "    avg_loss = total_loss / len(go_loader)\n",
        "\n",
        "    print(f\"  📊 Epoch {epoch + 1} Results:\")\n",
        "    print(f\"    Average Loss: {avg_loss:.4f}\")\n",
        "    print(f\"    Validation F1 (Macro): {val_results['f1_macro']:.4f}\")\n",
        "    print(f\"    Validation F1 (Weighted): {val_results['f1_weighted']:.4f}\")\n",
        "\n",
        "    # Log to wandb\n",
        "    if use_wandb:\n",
        "        wandb.log({\n",
        "            'epoch': epoch,\n",
        "            'train_loss': avg_loss,\n",
        "            'val_loss': val_results['loss'],\n",
        "            'val_f1_macro': val_results['f1_macro'],\n",
        "            'val_f1_weighted': val_results['f1_weighted']\n",
        "        })\n",
        "\n",
        "    # Save best model\n",
        "    if val_results['f1_macro'] > best_f1:\n",
        "        best_f1 = val_results['f1_macro']\n",
        "        torch.save(model.state_dict(), 'best_domain_adapted_model.pth')\n",
        "        print(f\"    💾 New best model saved! F1: {best_f1:.4f}\")\n",
        "\n",
        "    training_history.append({\n",
        "        'epoch': epoch,\n",
        "        'train_loss': avg_loss,\n",
        "        'val_f1_macro': val_results['f1_macro'],\n",
        "        'val_f1_weighted': val_results['f1_weighted']\n",
        "    })\n",
        "\n",
        "    # Clear GPU cache\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n🎉 Training completed! Best F1 Score: {best_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gald13aL24hb"
      },
      "source": [
        "## 📈 Results Analysis & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTuKJQw024hb"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "history_df = pd.DataFrame(training_history)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(history_df['epoch'], history_df['train_loss'], 'b-', label='Training Loss')\n",
        "axes[0].set_title('Training Loss Over Time')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# F1 Score plot\n",
        "axes[1].plot(history_df['epoch'], history_df['val_f1_macro'], 'r-', label='F1 Macro')\n",
        "axes[1].plot(history_df['epoch'], history_df['val_f1_weighted'], 'g-', label='F1 Weighted')\n",
        "axes[1].axhline(y=0.7, color='orange', linestyle='--', label='Target (70%)')\n",
        "axes[1].set_title('Validation F1 Score Over Time')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('F1 Score')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final evaluation\n",
        "print(\"\\n🎯 Final Model Evaluation:\")\n",
        "model.load_state_dict(torch.load('best_domain_adapted_model.pth'))\n",
        "final_results = trainer.evaluate(journal_val_loader)\n",
        "\n",
        "print(f\"📊 Final Results:\")\n",
        "print(f\"  F1 Score (Macro): {final_results['f1_macro']:.4f}\")\n",
        "print(f\"  F1 Score (Weighted): {final_results['f1_weighted']:.4f}\")\n",
        "print(f\"  Target Met (70%): {'✅' if final_results['f1_macro'] >= 0.7 else '❌'}\")\n",
        "\n",
        "# REQ-DL-012 Validation\n",
        "print(f\"\\n🎯 REQ-DL-012 Validation:\")\n",
        "print(f\"  Target: 70% F1 score on journal entries\")\n",
        "print(f\"  Achieved: {final_results['f1_macro']:.1%} F1 score\")\n",
        "print(f\"  Status: {'✅ SUCCESS' if final_results['f1_macro'] >= 0.7 else '❌ NEEDS IMPROVEMENT'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoBAaaSt24hb"
      },
      "source": [
        "## 💾 Model Export & Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1jMj0P224hb"
      },
      "outputs": [],
      "source": [
        "# Save model artifacts\n",
        "import pickle\n",
        "\n",
        "# Save label encoder\n",
        "with open('label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained('./domain_adapted_model')\n",
        "\n",
        "# Save model config\n",
        "model_config = {\n",
        "    'model_name': model_name,\n",
        "    'num_labels': 12,\n",
        "    'max_length': 128,\n",
        "    'label_encoder_path': 'label_encoder.pkl',\n",
        "    'model_path': 'best_domain_adapted_model.pth'\n",
        "}\n",
        "\n",
        "with open('model_config.json', 'w') as f:\n",
        "    json.dump(model_config, f, indent=2)\n",
        "\n",
        "print(\"💾 Model artifacts saved:\")\n",
        "print(\"  - best_domain_adapted_model.pth (model weights)\")\n",
        "print(\"  - label_encoder.pkl (label encoder)\")\n",
        "print(\"  - domain_adapted_model/ (tokenizer)\")\n",
        "print(\"  - model_config.json (configuration)\")\n",
        "\n",
        "# Download files (for Colab)\n",
        "from google.colab import files\n",
        "files.download('best_domain_adapted_model.pth')\n",
        "files.download('label_encoder.pkl')\n",
        "files.download('model_config.json')\n",
        "\n",
        "print(\"\\n🚀 Model ready for deployment!\")\n",
        "print(\"📋 Next steps:\")\n",
        "print(\"  1. Integrate model into SAMO-DL pipeline\")\n",
        "print(\"  2. Update emotion detection API\")\n",
        "print(\"  3. Deploy to production environment\")\n",
        "print(\"  4. Update PRD with achieved metrics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12857821"
      },
      "outputs": [],
      "source": [
        "!ls -lR SAMO--DL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "267ee8c5"
      },
      "source": [
        "## 🔍 Domain Gap Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fed0c2e"
      },
      "source": [
        "## 🏗️ Model Architecture"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "history_visible": true,
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
