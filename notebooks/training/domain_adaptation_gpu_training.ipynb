{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAMO Deep Learning - Domain Adaptation GPU Training\n",
    "\n",
    "## \ud83c\udfaf REQ-DL-012: Domain-Adapted Emotion Detection\n",
    "\n",
    "**Target**: Achieve 70% F1 score on journal entries through domain adaptation from GoEmotions (Reddit comments) to personal journal writing style.\n",
    "\n",
    "### Key Objectives:\n",
    "- Bridge domain gap between Reddit comments and journal entries\n",
    "- Implement focal loss for class imbalance\n",
    "- Use domain adaptation techniques for better transfer learning\n",
    "- Optimize for GPU training on Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Environment Setup & GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No GPU available. Training will be slow on CPU.\")\n",
    "\n",
    "# Enable cudnn benchmarking for faster training\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch>=2.1.0 torchvision>=0.16.0 torchaudio>=2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers>=4.30.0 datasets>=2.13.0 evaluate scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install accelerate wandb pydub openai-whisper jiwer\n",
    "\n",
    "# Clone repository if not already done\n",
    "!git clone https://github.com/uelkerd/SAMO--DL.git\n",
    "%cd SAMO--DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd0d Domain Gap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_writing_style(texts, domain_name):\n",
    "    \"\"\"Analyze writing style characteristics of a domain.\"\"\"\n",
    "    avg_length = np.mean([len(text.split()) for text in texts])\n",
    "    personal_pronouns = sum(['I ' in text or 'my ' in text or 'me ' in text for text in texts]) / len(texts)\n",
    "    reflection_words = sum(['think' in text.lower() or 'feel' in text.lower() or 'believe' in text.lower() \n",
    "                           for text in texts]) / len(texts)\n",
    "    \n",
    "    print(f\"{domain_name} Style Analysis:\")\n",
    "    print(f\"  Average length: {avg_length:.1f} words\")\n",
    "    print(f\"  Personal pronouns: {personal_pronouns:.1%}\")\n",
    "    print(f\"  Reflection words: {reflection_words:.1%}\")\n",
    "    \n",
    "    return {\n",
    "        'avg_length': avg_length,\n",
    "        'personal_pronouns': personal_pronouns,\n",
    "        'reflection_words': reflection_words\n",
    "    }\n",
    "\n",
    "# Load datasets\n",
    "print(\"\ud83d\udcca Loading datasets...\")\n",
    "\n",
    "# Load GoEmotions dataset\n",
    "go_emotions = load_dataset(\"go_emotions\", \"simplified\")\n",
    "go_texts = go_emotions['train']['text'][:1000]  # Sample for analysis\n",
    "\n",
    "# Load journal dataset\n",
    "with open('data/journal_test_dataset.json', 'r') as f:\n",
    "    journal_entries = json.load(f)\n",
    "\n",
    "journal_df = pd.DataFrame(journal_entries)\n",
    "journal_texts = journal_df['content'].tolist()\n",
    "\n",
    "# Analyze domains\n",
    "print(\"\\n\ud83d\udd0d Domain Gap Analysis:\")\n",
    "go_analysis = analyze_writing_style(go_texts, \"GoEmotions (Reddit)\")\n",
    "journal_analysis = analyze_writing_style(journal_texts, \"Journal Entries\")\n",
    "\n",
    "# Visualize differences\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['avg_length', 'personal_pronouns', 'reflection_words']\n",
    "labels = ['Avg Length (words)', 'Personal Pronouns', 'Reflection Words']\n",
    "\n",
    "for i, (metric, label) in enumerate(zip(metrics, labels)):\n",
    "    axes[i].bar(['GoEmotions', 'Journal'], \n",
    "                [go_analysis[metric], journal_analysis[metric]])\n",
    "    axes[i].set_title(label)\n",
    "    axes[i].set_ylabel('Percentage' if 'pronouns' in metric or 'reflection' in metric else 'Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83c\udfaf Key Insights:\")\n",
    "print(f\"- Journal entries are {journal_analysis['avg_length']/go_analysis['avg_length']:.1f}x longer\")\n",
    "print(f\"- Journal entries use {journal_analysis['personal_pronouns']/go_analysis['personal_pronouns']:.1f}x more personal pronouns\")\n",
    "print(f\"- Journal entries contain {journal_analysis['reflection_words']/go_analysis['reflection_words']:.1f}x more reflection words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfd7\ufe0f Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for addressing class imbalance in emotion detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class DomainAdaptedEmotionClassifier(nn.Module):\n",
    "    \"\"\"BERT-based emotion classifier with domain adaptation capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"bert-base-uncased\", num_labels=12, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        \n",
    "        # Domain adaptation layer\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 2)  # 2 domains: GoEmotions vs Journal\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, domain_labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Emotion classification\n",
    "        emotion_logits = self.classifier(self.dropout(pooled_output))\n",
    "        \n",
    "        # Domain classification (for domain adaptation)\n",
    "        domain_logits = self.domain_classifier(pooled_output)\n",
    "        \n",
    "        if domain_labels is not None:\n",
    "            return emotion_logits, domain_logits\n",
    "        return emotion_logits\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "print(\"\ud83c\udfd7\ufe0f Initializing model...\")\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = DomainAdaptedEmotionClassifier(model_name=model_name, num_labels=12)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\u2705 Model loaded on {device}\")\n",
    "print(f\"\ud83d\udcca Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"Custom dataset for emotion classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Prepare GoEmotions data\n",
    "print(\"\ud83d\udcca Preparing GoEmotions data...\")\n",
    "go_train = go_emotions['train']\n",
    "go_texts = go_train['text'][:10000]  # Use subset for faster training\n",
    "go_labels = go_train['labels'][:10000]\n",
    "\n",
    "# Convert multi-label to single label (take first emotion)\n",
    "go_single_labels = [label[0] if label else 0 for label in go_labels]\n",
    "\n",
    "# Prepare journal data\n",
    "print(\"\ud83d\udcca Preparing journal data...\")\n",
    "journal_texts = journal_df['content'].tolist()\n",
    "journal_emotions = journal_df['emotion'].tolist()\n",
    "\n",
    "# Create label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "all_emotions = list(set(go_single_labels + journal_emotions))\n",
    "label_encoder.fit(all_emotions)\n",
    "\n",
    "# Encode labels\n",
    "go_encoded_labels = label_encoder.transform(go_single_labels)\n",
    "journal_encoded_labels = label_encoder.transform(journal_emotions)\n",
    "\n",
    "# Split journal data\n",
    "journal_train_texts, journal_val_texts, journal_train_labels, journal_val_labels = train_test_split(\n",
    "    journal_texts, journal_encoded_labels, test_size=0.2, random_state=42, stratify=journal_encoded_labels\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "go_dataset = EmotionDataset(go_texts, go_encoded_labels, tokenizer)\n",
    "journal_train_dataset = EmotionDataset(journal_train_texts, journal_train_labels, tokenizer)\n",
    "journal_val_dataset = EmotionDataset(journal_val_texts, journal_val_labels, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 16\n",
    "go_loader = DataLoader(go_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "journal_train_loader = DataLoader(journal_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "journal_val_loader = DataLoader(journal_val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\u2705 Data prepared:\")\n",
    "print(f\"  GoEmotions: {len(go_dataset)} samples\")\n",
    "print(f\"  Journal Train: {len(journal_train_dataset)} samples\")\n",
    "print(f\"  Journal Val: {len(journal_val_dataset)} samples\")\n",
    "print(f\"  Total classes: {len(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "import wandb\n",
    "\n",
    "class DomainAdaptationTrainer:\n",
    "    \"\"\"Trainer for domain adaptation training.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.criterion = FocalLoss(alpha=1, gamma=2)\n",
    "        self.domain_criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def train_step(self, batch, domain_labels, lambda_domain=0.1):\n",
    "        \"\"\"Single training step with domain adaptation.\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(self.device)\n",
    "        attention_mask = batch['attention_mask'].to(self.device)\n",
    "        labels = batch['labels'].to(self.device)\n",
    "        domain_labels = domain_labels.to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        emotion_logits, domain_logits = self.model(input_ids, attention_mask, domain_labels)\n",
    "        \n",
    "        # Calculate losses\n",
    "        emotion_loss = self.criterion(emotion_logits, labels)\n",
    "        domain_loss = self.domain_criterion(domain_logits, domain_labels)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = emotion_loss + lambda_domain * domain_loss\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'emotion_loss': emotion_loss,\n",
    "            'domain_loss': domain_loss\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        \"\"\"Evaluate model on validation set.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                emotion_logits = self.model(input_ids, attention_mask)\n",
    "                loss = self.criterion(emotion_logits, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(emotion_logits, dim=1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
    "        f1_weighted = f1_score(all_labels, all_predictions, average='weighted')\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss / len(dataloader),\n",
    "            'f1_macro': f1_macro,\n",
    "            'f1_weighted': f1_weighted\n",
    "        }\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = DomainAdaptationTrainer(model, tokenizer, device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# Initialize wandb (optional)\n",
    "try:\n",
    "    wandb.init(project=\"samo-domain-adaptation\", name=\"journal-emotion-detection\")\n",
    "    use_wandb = True\n",
    "except:\n",
    "    print(\"\u26a0\ufe0f Wandb not available, continuing without logging\")\n",
    "    use_wandb = False\n",
    "\n",
    "print(\"\ud83c\udfaf Starting domain adaptation training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "best_f1 = 0\n",
    "training_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n\ud83d\udd04 Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Train on GoEmotions data\n",
    "    print(\"  \ud83d\udcda Training on GoEmotions data...\")\n",
    "    for i, batch in enumerate(go_loader):\n",
    "        domain_labels = torch.zeros(batch['input_ids'].size(0), dtype=torch.long)\n",
    "        losses = trainer.train_step(batch, domain_labels, lambda_domain=0.1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses['total_loss'].backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses['total_loss'].item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"    Batch {i}/{len(go_loader)}, Loss: {losses['total_loss'].item():.4f}\")\n",
    "    \n",
    "    # Train on journal data\n",
    "    print(\"  \ud83d\udcdd Training on journal data...\")\n",
    "    for i, batch in enumerate(journal_train_loader):\n",
    "        domain_labels = torch.ones(batch['input_ids'].size(0), dtype=torch.long)\n",
    "        losses = trainer.train_step(batch, domain_labels, lambda_domain=0.1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses['total_loss'].backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses['total_loss'].item()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"    Batch {i}/{len(journal_train_loader)}, Loss: {losses['total_loss'].item():.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    print(\"  \ud83c\udfaf Validating on journal test set...\")\n",
    "    val_results = trainer.evaluate(journal_val_loader)\n",
    "    \n",
    "    avg_loss = total_loss / (len(go_loader) + len(journal_train_loader))\n",
    "    \n",
    "    print(f\"  \ud83d\udcca Epoch {epoch + 1} Results:\")\n",
    "    print(f\"    Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"    Validation F1 (Macro): {val_results['f1_macro']:.4f}\")\n",
    "    print(f\"    Validation F1 (Weighted): {val_results['f1_weighted']:.4f}\")\n",
    "    \n",
    "    # Log to wandb\n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': avg_loss,\n",
    "            'val_loss': val_results['loss'],\n",
    "            'val_f1_macro': val_results['f1_macro'],\n",
    "            'val_f1_weighted': val_results['f1_weighted']\n",
    "        })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_results['f1_macro'] > best_f1:\n",
    "        best_f1 = val_results['f1_macro']\n",
    "        torch.save(model.state_dict(), 'best_domain_adapted_model.pth')\n",
    "        print(f\"    \ud83d\udcbe New best model saved! F1: {best_f1:.4f}\")\n",
    "    \n",
    "    training_history.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': avg_loss,\n",
    "        'val_f1_macro': val_results['f1_macro'],\n",
    "        'val_f1_weighted': val_results['f1_weighted']\n",
    "    })\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n\ud83c\udf89 Training completed! Best F1 Score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "history_df = pd.DataFrame(training_history)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history_df['epoch'], history_df['train_loss'], 'b-', label='Training Loss')\n",
    "axes[0].set_title('Training Loss Over Time')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# F1 Score plot\n",
    "axes[1].plot(history_df['epoch'], history_df['val_f1_macro'], 'r-', label='F1 Macro')\n",
    "axes[1].plot(history_df['epoch'], history_df['val_f1_weighted'], 'g-', label='F1 Weighted')\n",
    "axes[1].axhline(y=0.7, color='orange', linestyle='--', label='Target (70%)')\n",
    "axes[1].set_title('Validation F1 Score Over Time')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n\ud83c\udfaf Final Model Evaluation:\")\n",
    "model.load_state_dict(torch.load('best_domain_adapted_model.pth'))\n",
    "final_results = trainer.evaluate(journal_val_loader)\n",
    "\n",
    "print(f\"\ud83d\udcca Final Results:\")\n",
    "print(f\"  F1 Score (Macro): {final_results['f1_macro']:.4f}\")\n",
    "print(f\"  F1 Score (Weighted): {final_results['f1_weighted']:.4f}\")\n",
    "print(f\"  Target Met (70%): {'\u2705' if final_results['f1_macro'] >= 0.7 else '\u274c'}\")\n",
    "\n",
    "# REQ-DL-012 Validation\n",
    "print(f\"\\n\ud83c\udfaf REQ-DL-012 Validation:\")\n",
    "print(f\"  Target: 70% F1 score on journal entries\")\n",
    "print(f\"  Achieved: {final_results['f1_macro']:.1%} F1 score\")\n",
    "print(f\"  Status: {'\u2705 SUCCESS' if final_results['f1_macro'] >= 0.7 else '\u274c NEEDS IMPROVEMENT'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcbe Model Export & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model artifacts\n",
    "import pickle\n",
    "\n",
    "# Save label encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('./domain_adapted_model')\n",
    "\n",
    "# Save model config\n",
    "model_config = {\n",
    "    'model_name': model_name,\n",
    "    'num_labels': 12,\n",
    "    'max_length': 128,\n",
    "    'label_encoder_path': 'label_encoder.pkl',\n",
    "    'model_path': 'best_domain_adapted_model.pth'\n",
    "}\n",
    "\n",
    "with open('model_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(\"\ud83d\udcbe Model artifacts saved:\")\n",
    "print(\"  - best_domain_adapted_model.pth (model weights)\")\n",
    "print(\"  - label_encoder.pkl (label encoder)\")\n",
    "print(\"  - domain_adapted_model/ (tokenizer)\")\n",
    "print(\"  - model_config.json (configuration)\")\n",
    "\n",
    "# Download files (for Colab)\n",
    "from google.colab import files\n",
    "files.download('best_domain_adapted_model.pth')\n",
    "files.download('label_encoder.pkl')\n",
    "files.download('model_config.json')\n",
    "\n",
    "print(\"\\n\ud83d\ude80 Model ready for deployment!\")\n",
    "print(\"\ud83d\udccb Next steps:\")\n",
    "print(\"  1. Integrate model into SAMO-DL pipeline\")\n",
    "print(\"  2. Update emotion detection API\")\n",
    "print(\"  3. Deploy to production environment\")\n",
    "print(\"  4. Update PRD with achieved metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}