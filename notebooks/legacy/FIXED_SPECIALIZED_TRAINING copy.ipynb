{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqPhmyFk8q3x"
      },
      "source": [
        "# CORRECTED EMOTION DETECTION TRAINING\n",
        "## Using j-hartmann/emotion-english-distilroberta-base with Verification\n",
        "\n",
        "**CRITICAL**: This notebook ensures we use the correct specialized emotion model\n",
        "and verifies it's working properly before training.\n",
        "\n",
        "**Target**: Reliable 75-85% F1 score with proper emotion-specialized model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/uelkerd/SAMO--DL.git\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shuF5EFdPLA4",
        "outputId": "6be3a19c-153b-45f6-cd9b-5b72f9d21c7a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SAMO--DL'...\n",
            "remote: Enumerating objects: 2901, done.\u001b[K\n",
            "remote: Counting objects: 100% (254/254), done.\u001b[K\n",
            "remote: Compressing objects: 100% (177/177), done.\u001b[K\n",
            "remote: Total 2901 (delta 140), reused 163 (delta 74), pack-reused 2647 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2901/2901), 24.28 MiB | 15.20 MiB/s, done.\n",
            "Resolving deltas: 100% (2023/2023), done.\n",
            "/content/SAMO--DL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0gI8Wy3Y8q3y",
        "outputId": "4a77a1ca-52a7-4ee2-89d6-114f21f7de1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SAMO--DL\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "a218602022174da096dd0e1fc569706d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%cd SAMO--DL\n",
        "# Install required packages\n",
        "!pip install transformers datasets torch scikit-learn numpy pandas huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVPLcqlS8q3y",
        "outputId": "6674669a-d72a-4498-8113-952f5592b747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Packages imported successfully\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('‚úÖ Packages imported successfully')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UMgnzGg8q3y",
        "outputId": "39973bb6-7ffb-4427-c46e-b225f19cdcab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç VERIFYING SPECIALIZED MODEL ACCESS\n",
            "==================================================\n",
            "Testing access to: j-hartmann/emotion-english-distilroberta-base\n",
            "‚úÖ SUCCESS: Specialized model loaded!\n",
            "Model type: roberta\n",
            "Architecture: RobertaForSequenceClassification\n",
            "Hidden layers: 6\n",
            "Hidden size: 768\n",
            "Number of labels: 7\n",
            "Original labels: {0: 'anger', 1: 'disgust', 2: 'fear', 3: 'joy', 4: 'neutral', 5: 'sadness', 6: 'surprise'}\n",
            "‚ö†Ô∏è  WARNING: This may not be the expected DistilRoBERTa model\n"
          ]
        }
      ],
      "source": [
        "# CRITICAL: Verify we can access the specialized model\n",
        "print('üîç VERIFYING SPECIALIZED MODEL ACCESS')\n",
        "print('=' * 50)\n",
        "\n",
        "specialized_model_name = 'j-hartmann/emotion-english-distilroberta-base'\n",
        "\n",
        "try:\n",
        "    print(f'Testing access to: {specialized_model_name}')\n",
        "    test_tokenizer = AutoTokenizer.from_pretrained(specialized_model_name)\n",
        "    test_model = AutoModelForSequenceClassification.from_pretrained(specialized_model_name)\n",
        "\n",
        "    print('‚úÖ SUCCESS: Specialized model loaded!')\n",
        "    print(f'Model type: {test_model.config.model_type}')\n",
        "    print(f'Architecture: {test_model.config.architectures[0]}')\n",
        "    print(f'Hidden layers: {test_model.config.num_hidden_layers}')\n",
        "    print(f'Hidden size: {test_model.config.hidden_size}')\n",
        "    print(f'Number of labels: {test_model.config.num_labels}')\n",
        "    print(f'Original labels: {test_model.config.id2label}')\n",
        "\n",
        "    # Verify it's actually DistilRoBERTa\n",
        "    if test_model.config.num_hidden_layers == 6 and 'distil' in test_model.config.model_type.lower():\n",
        "        print('‚úÖ CONFIRMED: This is DistilRoBERTa architecture')\n",
        "    else:\n",
        "        print('‚ö†Ô∏è  WARNING: This may not be the expected DistilRoBERTa model')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f'‚ùå ERROR: Cannot access specialized model: {str(e)}')\n",
        "    print('\\nüîß FALLBACK: Using roberta-base instead')\n",
        "    specialized_model_name = 'roberta-base'\n",
        "    test_tokenizer = AutoTokenizer.from_pretrained(specialized_model_name)\n",
        "    test_model = AutoModelForSequenceClassification.from_pretrained(specialized_model_name, num_labels=12)\n",
        "    print(f'‚úÖ Fallback model loaded: {specialized_model_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT0oQl8z8q3y",
        "outputId": "e0b12b15-77ec-4a38-ef93-dcc9b71dee5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Our emotion classes: ['anxious', 'calm', 'content', 'excited', 'frustrated', 'grateful', 'happy', 'hopeful', 'overwhelmed', 'proud', 'sad', 'tired']\n",
            "üìä Number of emotions: 12\n"
          ]
        }
      ],
      "source": [
        "# Define our emotion classes\n",
        "emotions = ['anxious', 'calm', 'content', 'excited', 'frustrated', 'grateful', 'happy', 'hopeful', 'overwhelmed', 'proud', 'sad', 'tired']\n",
        "print(f'üéØ Our emotion classes: {emotions}')\n",
        "print(f'üìä Number of emotions: {len(emotions)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxvqCvke8q3y",
        "outputId": "d1809570-c96b-47d4-d7b2-fd5a6833dbc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä CREATING BALANCED DATASET\n",
            "========================================\n",
            "‚úÖ Created balanced dataset with 144 samples\n",
            "üìä Samples per emotion: 12\n",
            "\n",
            "üìà Emotion distribution:\n",
            "  anxious: 12 samples\n",
            "  calm: 12 samples\n",
            "  content: 12 samples\n",
            "  excited: 12 samples\n",
            "  frustrated: 12 samples\n",
            "  grateful: 12 samples\n",
            "  happy: 12 samples\n",
            "  hopeful: 12 samples\n",
            "  overwhelmed: 12 samples\n",
            "  proud: 12 samples\n",
            "  sad: 12 samples\n",
            "  tired: 12 samples\n"
          ]
        }
      ],
      "source": [
        "# Create balanced training dataset\n",
        "print('üìä CREATING BALANCED DATASET')\n",
        "print('=' * 40)\n",
        "\n",
        "balanced_data = [\n",
        "    # anxious (12 samples)\n",
        "    {'text': 'I feel anxious about the presentation.', 'label': 0},\n",
        "    {'text': 'I am anxious about the future.', 'label': 0},\n",
        "    {'text': 'This makes me feel anxious.', 'label': 0},\n",
        "    {'text': 'I am feeling anxious today.', 'label': 0},\n",
        "    {'text': 'The uncertainty makes me anxious.', 'label': 0},\n",
        "    {'text': 'I feel anxious about the results.', 'label': 0},\n",
        "    {'text': 'This situation is making me anxious.', 'label': 0},\n",
        "    {'text': 'I am anxious about the meeting.', 'label': 0},\n",
        "    {'text': 'The pressure is making me anxious.', 'label': 0},\n",
        "    {'text': 'I feel anxious about the decision.', 'label': 0},\n",
        "    {'text': 'This is causing me anxiety.', 'label': 0},\n",
        "    {'text': 'I am anxious about the changes.', 'label': 0},\n",
        "\n",
        "    # calm (12 samples)\n",
        "    {'text': 'I feel calm and peaceful.', 'label': 1},\n",
        "    {'text': 'I am feeling calm today.', 'label': 1},\n",
        "    {'text': 'This makes me feel calm.', 'label': 1},\n",
        "    {'text': 'I am calm about the situation.', 'label': 1},\n",
        "    {'text': 'I feel calm and relaxed.', 'label': 1},\n",
        "    {'text': 'This gives me a sense of calm.', 'label': 1},\n",
        "    {'text': 'I am feeling calm and centered.', 'label': 1},\n",
        "    {'text': 'This brings me calm.', 'label': 1},\n",
        "    {'text': 'I feel calm and at peace.', 'label': 1},\n",
        "    {'text': 'I am calm about the outcome.', 'label': 1},\n",
        "    {'text': 'This creates a feeling of calm.', 'label': 1},\n",
        "    {'text': 'I feel calm and collected.', 'label': 1},\n",
        "\n",
        "    # content (12 samples)\n",
        "    {'text': 'I feel content with my life.', 'label': 2},\n",
        "    {'text': 'I am content with the results.', 'label': 2},\n",
        "    {'text': 'This makes me feel content.', 'label': 2},\n",
        "    {'text': 'I am feeling content today.', 'label': 2},\n",
        "    {'text': 'I feel content and satisfied.', 'label': 2},\n",
        "    {'text': 'This gives me contentment.', 'label': 2},\n",
        "    {'text': 'I am content with my choices.', 'label': 2},\n",
        "    {'text': 'I feel content and fulfilled.', 'label': 2},\n",
        "    {'text': 'This brings me contentment.', 'label': 2},\n",
        "    {'text': 'I am content with the situation.', 'label': 2},\n",
        "    {'text': 'I feel content and at ease.', 'label': 2},\n",
        "    {'text': 'This creates contentment in me.', 'label': 2},\n",
        "\n",
        "    # excited (12 samples)\n",
        "    {'text': 'I am excited about the new opportunity.', 'label': 3},\n",
        "    {'text': 'I feel excited about the future.', 'label': 3},\n",
        "    {'text': 'This makes me feel excited.', 'label': 3},\n",
        "    {'text': 'I am feeling excited today.', 'label': 3},\n",
        "    {'text': 'I feel excited and enthusiastic.', 'label': 3},\n",
        "    {'text': 'This gives me excitement.', 'label': 3},\n",
        "    {'text': 'I am excited about the project.', 'label': 3},\n",
        "    {'text': 'I feel excited and motivated.', 'label': 3},\n",
        "    {'text': 'This brings me excitement.', 'label': 3},\n",
        "    {'text': 'I am excited about the possibilities.', 'label': 3},\n",
        "    {'text': 'I feel excited and energized.', 'label': 3},\n",
        "    {'text': 'This creates excitement in me.', 'label': 3},\n",
        "\n",
        "    # frustrated (12 samples)\n",
        "    {'text': 'I am so frustrated with this project.', 'label': 4},\n",
        "    {'text': 'I feel frustrated about the situation.', 'label': 4},\n",
        "    {'text': 'This makes me feel frustrated.', 'label': 4},\n",
        "    {'text': 'I am feeling frustrated today.', 'label': 4},\n",
        "    {'text': 'I feel frustrated and annoyed.', 'label': 4},\n",
        "    {'text': 'This gives me frustration.', 'label': 4},\n",
        "    {'text': 'I am frustrated with the results.', 'label': 4},\n",
        "    {'text': 'I feel frustrated and irritated.', 'label': 4},\n",
        "    {'text': 'This brings me frustration.', 'label': 4},\n",
        "    {'text': 'I am frustrated with the process.', 'label': 4},\n",
        "    {'text': 'I feel frustrated and upset.', 'label': 4},\n",
        "    {'text': 'This creates frustration in me.', 'label': 4},\n",
        "\n",
        "    # grateful (12 samples)\n",
        "    {'text': 'I am grateful for all the support.', 'label': 5},\n",
        "    {'text': 'I feel grateful for the opportunity.', 'label': 5},\n",
        "    {'text': 'This makes me feel grateful.', 'label': 5},\n",
        "    {'text': 'I am feeling grateful today.', 'label': 5},\n",
        "    {'text': 'I feel grateful and thankful.', 'label': 5},\n",
        "    {'text': 'This gives me gratitude.', 'label': 5},\n",
        "    {'text': 'I am grateful for the help.', 'label': 5},\n",
        "    {'text': 'I feel grateful and appreciative.', 'label': 5},\n",
        "    {'text': 'This brings me gratitude.', 'label': 5},\n",
        "    {'text': 'I am grateful for the kindness.', 'label': 5},\n",
        "    {'text': 'I feel grateful and blessed.', 'label': 5},\n",
        "    {'text': 'This creates gratitude in me.', 'label': 5},\n",
        "\n",
        "    # happy (12 samples)\n",
        "    {'text': 'I am feeling really happy today!', 'label': 6},\n",
        "    {'text': 'I feel happy about the news.', 'label': 6},\n",
        "    {'text': 'This makes me feel happy.', 'label': 6},\n",
        "    {'text': 'I am feeling happy today.', 'label': 6},\n",
        "    {'text': 'I feel happy and joyful.', 'label': 6},\n",
        "    {'text': 'This gives me happiness.', 'label': 6},\n",
        "    {'text': 'I am happy with the results.', 'label': 6},\n",
        "    {'text': 'I feel happy and delighted.', 'label': 6},\n",
        "    {'text': 'This brings me happiness.', 'label': 6},\n",
        "    {'text': 'I am happy about the success.', 'label': 6},\n",
        "    {'text': 'I feel happy and cheerful.', 'label': 6},\n",
        "    {'text': 'This creates happiness in me.', 'label': 6},\n",
        "\n",
        "    # hopeful (12 samples)\n",
        "    {'text': 'I am hopeful for the future.', 'label': 7},\n",
        "    {'text': 'I feel hopeful about the outcome.', 'label': 7},\n",
        "    {'text': 'This makes me feel hopeful.', 'label': 7},\n",
        "    {'text': 'I am feeling hopeful today.', 'label': 7},\n",
        "    {'text': 'I feel hopeful and optimistic.', 'label': 7},\n",
        "    {'text': 'This gives me hope.', 'label': 7},\n",
        "    {'text': 'I am hopeful about the changes.', 'label': 7},\n",
        "    {'text': 'I feel hopeful and positive.', 'label': 7},\n",
        "    {'text': 'This brings me hope.', 'label': 7},\n",
        "    {'text': 'I am hopeful about the possibilities.', 'label': 7},\n",
        "    {'text': 'I feel hopeful and confident.', 'label': 7},\n",
        "    {'text': 'This creates hope in me.', 'label': 7},\n",
        "\n",
        "    # overwhelmed (12 samples)\n",
        "    {'text': 'I am feeling overwhelmed with tasks.', 'label': 8},\n",
        "    {'text': 'I feel overwhelmed by the workload.', 'label': 8},\n",
        "    {'text': 'This makes me feel overwhelmed.', 'label': 8},\n",
        "    {'text': 'I am feeling overwhelmed today.', 'label': 8},\n",
        "    {'text': 'I feel overwhelmed and stressed.', 'label': 8},\n",
        "    {'text': 'This gives me overwhelm.', 'label': 8},\n",
        "    {'text': 'I am overwhelmed by the situation.', 'label': 8},\n",
        "    {'text': 'I feel overwhelmed and exhausted.', 'label': 8},\n",
        "    {'text': 'This brings me overwhelm.', 'label': 8},\n",
        "    {'text': 'I am overwhelmed by the pressure.', 'label': 8},\n",
        "    {'text': 'I feel overwhelmed and drained.', 'label': 8},\n",
        "    {'text': 'This creates overwhelm in me.', 'label': 8},\n",
        "\n",
        "    # proud (12 samples)\n",
        "    {'text': 'I am proud of my accomplishments.', 'label': 9},\n",
        "    {'text': 'I feel proud of the results.', 'label': 9},\n",
        "    {'text': 'This makes me feel proud.', 'label': 9},\n",
        "    {'text': 'I am feeling proud today.', 'label': 9},\n",
        "    {'text': 'I feel proud and accomplished.', 'label': 9},\n",
        "    {'text': 'This gives me pride.', 'label': 9},\n",
        "    {'text': 'I am proud of the achievement.', 'label': 9},\n",
        "    {'text': 'I feel proud and satisfied.', 'label': 9},\n",
        "    {'text': 'This brings me pride.', 'label': 9},\n",
        "    {'text': 'I am proud of the success.', 'label': 9},\n",
        "    {'text': 'I feel proud and confident.', 'label': 9},\n",
        "    {'text': 'This creates pride in me.', 'label': 9},\n",
        "\n",
        "    # sad (12 samples)\n",
        "    {'text': 'I feel sad about the loss.', 'label': 10},\n",
        "    {'text': 'I am sad about the situation.', 'label': 10},\n",
        "    {'text': 'This makes me feel sad.', 'label': 10},\n",
        "    {'text': 'I am feeling sad today.', 'label': 10},\n",
        "    {'text': 'I feel sad and down.', 'label': 10},\n",
        "    {'text': 'This gives me sadness.', 'label': 10},\n",
        "    {'text': 'I am sad about the outcome.', 'label': 10},\n",
        "    {'text': 'I feel sad and depressed.', 'label': 10},\n",
        "    {'text': 'This brings me sadness.', 'label': 10},\n",
        "    {'text': 'I am sad about the news.', 'label': 10},\n",
        "    {'text': 'I feel sad and heartbroken.', 'label': 10},\n",
        "    {'text': 'This creates sadness in me.', 'label': 10},\n",
        "\n",
        "    # tired (12 samples)\n",
        "    {'text': 'I am tired from working all day.', 'label': 11},\n",
        "    {'text': 'I feel tired of the routine.', 'label': 11},\n",
        "    {'text': 'This makes me feel tired.', 'label': 11},\n",
        "    {'text': 'I am feeling tired today.', 'label': 11},\n",
        "    {'text': 'I feel tired and exhausted.', 'label': 11},\n",
        "    {'text': 'This gives me tiredness.', 'label': 11},\n",
        "    {'text': 'I am tired of the situation.', 'label': 11},\n",
        "    {'text': 'I feel tired and worn out.', 'label': 11},\n",
        "    {'text': 'This brings me tiredness.', 'label': 11},\n",
        "    {'text': 'I am tired of the stress.', 'label': 11},\n",
        "    {'text': 'I feel tired and fatigued.', 'label': 11},\n",
        "    {'text': 'This creates tiredness in me.', 'label': 11}\n",
        "]\n",
        "\n",
        "print(f'‚úÖ Created balanced dataset with {len(balanced_data)} samples')\n",
        "print(f'üìä Samples per emotion: {len(balanced_data) // len(emotions)}')\n",
        "\n",
        "# Verify balance\n",
        "emotion_counts = {}\n",
        "for item in balanced_data:\n",
        "    emotion = emotions[item['label']]\n",
        "    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
        "\n",
        "print('\\nüìà Emotion distribution:')\n",
        "for emotion, count in emotion_counts.items():\n",
        "    print(f'  {emotion}: {count} samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5JF7LKd8q3z",
        "outputId": "880e08cb-de99-4377-ee35-afbd55548d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÄ SPLITTING DATA WITH VALIDATION\n",
            "========================================\n",
            "Training samples: 115\n",
            "Validation samples: 29\n",
            "‚úÖ Datasets created successfully\n"
          ]
        }
      ],
      "source": [
        "# Split data with proper validation\n",
        "print('üîÄ SPLITTING DATA WITH VALIDATION')\n",
        "print('=' * 40)\n",
        "\n",
        "train_data, val_data = train_test_split(balanced_data, test_size=0.2, random_state=42, stratify=[d['label'] for d in balanced_data])\n",
        "\n",
        "print(f'Training samples: {len(train_data)}')\n",
        "print(f'Validation samples: {len(val_data)}')\n",
        "\n",
        "# Convert to datasets\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "\n",
        "print('‚úÖ Datasets created successfully')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qmdtASn8q3z",
        "outputId": "24372bc9-4e1a-4ffa-e7a1-cdcc19b83555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß LOADING SPECIALIZED MODEL\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at j-hartmann/emotion-english-distilroberta-base and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([12, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([12]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded specialized emotion model and resized for 12 emotions\n",
            "Model type: roberta\n",
            "Architecture: RobertaForSequenceClassification\n",
            "Hidden layers: 6\n",
            "Hidden size: 768\n",
            "Number of labels: 12\n",
            "Our labels: {0: 'anxious', 1: 'calm', 2: 'content', 3: 'excited', 4: 'frustrated', 5: 'grateful', 6: 'happy', 7: 'hopeful', 8: 'overwhelmed', 9: 'proud', 10: 'sad', 11: 'tired'}\n"
          ]
        }
      ],
      "source": [
        "# Load the CORRECT specialized model\n",
        "print('üîß LOADING SPECIALIZED MODEL')\n",
        "print('=' * 40)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(specialized_model_name)\n",
        "\n",
        "# For specialized model, we need to resize the classifier for our 12 emotions\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    specialized_model_name,\n",
        "    num_labels=12,\n",
        "    ignore_mismatched_sizes=True  # This is the key to resizing the classifier\n",
        ")\n",
        "\n",
        "print('‚úÖ Loaded specialized emotion model and resized for 12 emotions')\n",
        "\n",
        "\n",
        "# Update model config with our emotion labels\n",
        "model.config.id2label = {i: emotion for i, emotion in enumerate(emotions)}\n",
        "model.config.label2id = {emotion: i for i, emotion in enumerate(emotions)}\n",
        "\n",
        "print(f'Model type: {model.config.model_type}')\n",
        "print(f'Architecture: {model.config.architectures[0]}')\n",
        "print(f'Hidden layers: {model.config.num_hidden_layers}')\n",
        "print(f'Hidden size: {model.config.hidden_size}')\n",
        "print(f'Number of labels: {model.config.num_labels}')\n",
        "print(f'Our labels: {model.config.id2label}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "eb69f44615dd49b0980390c62423afc4",
            "7b78a0d818614ca4860a14351c2236a4",
            "b67cb71868274fe095be2fad8501b8c1",
            "151c06fd676742fa92e5ecd7df4e72ae",
            "bc27d89f217442c0b479f241a0ec511a",
            "e67e7fc4984f4abdb3c595aeceafa2c4",
            "959f149366b54d8e9b855fdb19990827",
            "1fa5ee18a18a42928a2303200814adfc",
            "7ceda2e5dc99415dbb54389309a855e4",
            "bb1a72a58c944ad5a1e536354cc0f200",
            "7c749483038847f98b28d8f040095aa9",
            "a37b944dd1c54b5ba750f57ca91e92ba",
            "e65d5b15ff5e472c9471b19345c49b8f",
            "35c9f9d4928349b09709cfd3a940a844",
            "f03bced8119f47c0a3da22392f7b23c8",
            "adf163c85a344cbd8dcf68f96bc0b543",
            "5bba41e8415b41b7997b9299bbfcca5e",
            "1157c02bacba464e9a7ed02c3c5122d6",
            "472fed8652d343508be19f5e20d6975f",
            "92fc96efd0ea4fcbb04d995846c1b1cd",
            "4d573ba5366d4f0ab20ff7751e5232f6",
            "a7eb62fb89084f4bbdefd08698e85364"
          ]
        },
        "id": "hoxkvVYA8q3z",
        "outputId": "9055954d-936e-4c41-83d5-1a22526b5416"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/115 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb69f44615dd49b0980390c62423afc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/29 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b78a0d818614ca4860a14351c2236a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data tokenized successfully\n"
          ]
        }
      ],
      "source": [
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "print('‚úÖ Data tokenized successfully')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAPPb3s_8q3z",
        "outputId": "293064c2-0022-4030-da9d-dfebee384e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è  CONFIGURING TRAINING ARGUMENTS\n",
            "========================================\n",
            "‚úÖ Training arguments configured\n"
          ]
        }
      ],
      "source": [
        "# Training arguments with proper settings\n",
        "print('‚öôÔ∏è  CONFIGURING TRAINING ARGUMENTS')\n",
        "print('=' * 40)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./corrected_emotion_model',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,  # Increased for A100\n",
        "    per_device_eval_batch_size=16,   # Increased for A100\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,  # Regularization\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1',\n",
        "    warmup_steps=100,\n",
        "    dataloader_num_workers=0,\n",
        "    save_total_limit=3  # Keep only best 3 checkpoints\n",
        ")\n",
        "\n",
        "print('‚úÖ Training arguments configured')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "BGZqnoRN8q3z"
      },
      "outputs": [],
      "source": [
        "# Custom metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    report = classification_report(labels, predictions, target_names=emotions, output_dict=True)\n",
        "\n",
        "    return {\n",
        "        'f1': report['weighted avg']['f1-score'],\n",
        "        'accuracy': report['accuracy'],\n",
        "        'precision': report['weighted avg']['precision'],\n",
        "        'recall': report['weighted avg']['recall']\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osbuiIBW8q3z",
        "outputId": "1bfd51f7-f9e6-47cd-d51f-d01acee9d2a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Trainer initialized successfully\n"
          ]
        }
      ],
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print('‚úÖ Trainer initialized successfully')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "A7F-xBHt8q3z",
        "outputId": "a19b3487-cbae-4666-bd8a-d3b3be5d17c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ STARTING TRAINING\n",
            "========================================\n",
            "Using model: j-hartmann/emotion-english-distilroberta-base\n",
            "Training samples: 115\n",
            "Validation samples: 29\n",
            "\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 00:05, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.507600</td>\n",
              "      <td>2.450205</td>\n",
              "      <td>0.128736</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.137931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.444900</td>\n",
              "      <td>2.365234</td>\n",
              "      <td>0.179803</td>\n",
              "      <td>0.206897</td>\n",
              "      <td>0.213793</td>\n",
              "      <td>0.206897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.341800</td>\n",
              "      <td>2.226004</td>\n",
              "      <td>0.346470</td>\n",
              "      <td>0.413793</td>\n",
              "      <td>0.312069</td>\n",
              "      <td>0.413793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.203500</td>\n",
              "      <td>2.038876</td>\n",
              "      <td>0.754789</td>\n",
              "      <td>0.793103</td>\n",
              "      <td>0.755337</td>\n",
              "      <td>0.793103</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training completed successfully\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "print('üöÄ STARTING TRAINING')\n",
        "print('=' * 40)\n",
        "print(f'Using model: {specialized_model_name}')\n",
        "print(f'Training samples: {len(train_data)}')\n",
        "print(f'Validation samples: {len(val_data)}')\n",
        "print('\\nTraining...')\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print('‚úÖ Training completed successfully')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "_ywLl65v8q3z",
        "outputId": "e346bd64-1d60-4c30-d302-ee127021561a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä EVALUATING MODEL\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final F1 Score: 0.755\n",
            "Final Accuracy: 0.793\n",
            "Final Precision: 0.755\n",
            "Final Recall: 0.793\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "print('üìä EVALUATING MODEL')\n",
        "print('=' * 40)\n",
        "\n",
        "results = trainer.evaluate()\n",
        "print(f'Final F1 Score: {results[\"eval_f1\"]:.3f}')\n",
        "print(f'Final Accuracy: {results[\"eval_accuracy\"]:.3f}')\n",
        "print(f'Final Precision: {results[\"eval_precision\"]:.3f}')\n",
        "print(f'Final Recall: {results[\"eval_recall\"]:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ3AUNui8q3z",
        "outputId": "a03b4c37-68a8-42c4-f958-4914f4744d9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ RELIABILITY TESTING\n",
            "========================================\n",
            "Testing on diverse examples...\n",
            "‚úÖ I am feeling really happy today! ‚Üí happy (expected: happy, confidence: 0.137)\n",
            "‚úÖ I am so frustrated with this project. ‚Üí frustrated (expected: frustrated, confidence: 0.160)\n",
            "‚úÖ I feel anxious about the presentation. ‚Üí anxious (expected: anxious, confidence: 0.133)\n",
            "‚úÖ I am grateful for all the support. ‚Üí grateful (expected: grateful, confidence: 0.204)\n",
            "‚úÖ I am feeling overwhelmed with tasks. ‚Üí overwhelmed (expected: overwhelmed, confidence: 0.139)\n",
            "‚úÖ I am proud of my accomplishments. ‚Üí proud (expected: proud, confidence: 0.149)\n",
            "‚ùå I feel sad about the loss. ‚Üí anxious (expected: sad, confidence: 0.139)\n",
            "‚ùå I am tired from working all day. ‚Üí anxious (expected: tired, confidence: 0.133)\n",
            "‚úÖ I feel calm and peaceful. ‚Üí calm (expected: calm, confidence: 0.122)\n",
            "‚ùå I am excited about the new opportunity. ‚Üí proud (expected: excited, confidence: 0.110)\n",
            "‚ùå I feel content with my life. ‚Üí happy (expected: content, confidence: 0.110)\n",
            "‚úÖ I am hopeful for the future. ‚Üí hopeful (expected: hopeful, confidence: 0.143)\n",
            "\n",
            "üìä Test Accuracy: 66.7%\n",
            "\n",
            "üéØ Bias Analysis:\n",
            "  anxious: 3 predictions (25.0%)\n",
            "  calm: 1 predictions (8.3%)\n",
            "  content: 0 predictions (0.0%)\n",
            "  excited: 0 predictions (0.0%)\n",
            "  frustrated: 1 predictions (8.3%)\n",
            "  grateful: 1 predictions (8.3%)\n",
            "  happy: 2 predictions (16.7%)\n",
            "  hopeful: 1 predictions (8.3%)\n",
            "  overwhelmed: 1 predictions (8.3%)\n",
            "  proud: 2 predictions (16.7%)\n",
            "  sad: 0 predictions (0.0%)\n",
            "  tired: 0 predictions (0.0%)\n",
            "\n",
            "‚ö†Ô∏è  MODEL NEEDS IMPROVEMENT\n",
            "‚ùå Accuracy too low: 66.7% (need >80%)\n"
          ]
        }
      ],
      "source": [
        "# CRITICAL: Test on diverse examples to verify reliability\n",
        "print('üß™ RELIABILITY TESTING')\n",
        "print('=' * 40)\n",
        "\n",
        "test_examples = [\n",
        "    'I am feeling really happy today!',\n",
        "    'I am so frustrated with this project.',\n",
        "    'I feel anxious about the presentation.',\n",
        "    'I am grateful for all the support.',\n",
        "    'I am feeling overwhelmed with tasks.',\n",
        "    'I am proud of my accomplishments.',\n",
        "    'I feel sad about the loss.',\n",
        "    'I am tired from working all day.',\n",
        "    'I feel calm and peaceful.',\n",
        "    'I am excited about the new opportunity.',\n",
        "    'I feel content with my life.',\n",
        "    'I am hopeful for the future.'\n",
        "]\n",
        "\n",
        "print('Testing on diverse examples...')\n",
        "correct = 0\n",
        "predictions_by_emotion = {emotion: 0 for emotion in emotions}\n",
        "\n",
        "device = model.device  # Get the device the model is on\n",
        "\n",
        "for text in test_examples:\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to the correct device\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.softmax(outputs.logits, dim=1)\n",
        "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
        "        confidence = predictions[0][predicted_class].item()\n",
        "\n",
        "    predicted_emotion = emotions[predicted_class]\n",
        "    predictions_by_emotion[predicted_emotion] += 1\n",
        "\n",
        "    expected_emotion = None\n",
        "    for emotion in emotions:\n",
        "        if emotion in text.lower():\n",
        "            expected_emotion = emotion\n",
        "            break\n",
        "\n",
        "    if expected_emotion and predicted_emotion == expected_emotion:\n",
        "        correct += 1\n",
        "        status = '‚úÖ'\n",
        "    else:\n",
        "        status = '‚ùå'\n",
        "\n",
        "    print(f'{status} {text} ‚Üí {predicted_emotion} (expected: {expected_emotion}, confidence: {confidence:.3f})')\n",
        "\n",
        "accuracy = correct / len(test_examples)\n",
        "print(f'\\nüìä Test Accuracy: {accuracy:.1%}')\n",
        "\n",
        "# Check for bias\n",
        "print('\\nüéØ Bias Analysis:')\n",
        "for emotion, count in predictions_by_emotion.items():\n",
        "    percentage = count / len(test_examples) * 100\n",
        "    print(f'  {emotion}: {count} predictions ({percentage:.1f}%)')\n",
        "\n",
        "# Determine if model is reliable\n",
        "max_bias = max(predictions_by_emotion.values()) / len(test_examples)\n",
        "\n",
        "if accuracy >= 0.8 and max_bias <= 0.3:\n",
        "    print('\\nüéâ MODEL PASSES RELIABILITY TEST!')\n",
        "    print('‚úÖ Ready for deployment!')\n",
        "else:\n",
        "    print('\\n‚ö†Ô∏è  MODEL NEEDS IMPROVEMENT')\n",
        "    if accuracy < 0.8:\n",
        "        print(f'‚ùå Accuracy too low: {accuracy:.1%} (need >80%)')\n",
        "    if max_bias > 0.3:\n",
        "        print(f'‚ùå Too much bias: {max_bias:.1%} (need <30%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "aSLGo8uI8q3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a1810e-dc26-496f-a177-dc0ff42629b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ SAVING MODEL\n",
            "========================================\n",
            "‚úÖ Model saved to: ./corrected_emotion_model_final\n",
            "‚úÖ Training info saved: ./corrected_emotion_model_final/training_info.json\n",
            "\n",
            "üìã Next steps:\n",
            "1. Download the model files\n",
            "2. Test locally with validation script\n",
            "3. Deploy if all tests pass\n"
          ]
        }
      ],
      "source": [
        "# Save the model with proper configuration\n",
        "print('üíæ SAVING MODEL')\n",
        "print('=' * 40)\n",
        "\n",
        "output_dir = './corrected_emotion_model_final'\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Save training info\n",
        "training_info = {\n",
        "    'base_model': specialized_model_name,\n",
        "    'emotions': emotions,\n",
        "    'training_samples': len(train_data),\n",
        "    'validation_samples': len(val_data),\n",
        "    'final_f1': results['eval_f1'],\n",
        "    'final_accuracy': results['eval_accuracy'],\n",
        "    'test_accuracy': accuracy,\n",
        "    'model_type': model.config.model_type,\n",
        "    'hidden_layers': model.config.num_hidden_layers,\n",
        "    'hidden_size': model.config.hidden_size\n",
        "}\n",
        "\n",
        "with open(f'{output_dir}/training_info.json', 'w') as f:\n",
        "    json.dump(training_info, f, indent=2)\n",
        "\n",
        "print(f'‚úÖ Model saved to: {output_dir}')\n",
        "print(f'‚úÖ Training info saved: {output_dir}/training_info.json')\n",
        "print('\\nüìã Next steps:')\n",
        "print('1. Download the model files')\n",
        "print('2. Test locally with validation script')\n",
        "print('3. Deploy if all tests pass')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a37a5e34",
        "outputId": "1c9312c6-413e-4041-f27e-f5f06f9fd8bf"
      },
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print('‚úÖ Trainer initialized successfully with data collator')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Trainer initialized successfully with data collator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b22def88",
        "outputId": "f9b8bc17-4921-45ea-8b40-aba3b333edf4"
      },
      "source": [
        "class DebugModel(AutoModelForSequenceClassification):\n",
        "    def forward(self, *args, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                kwargs[k] = v.to(self.device)\n",
        "        outputs = super().forward(*args, **kwargs)\n",
        "        print(\"Logits shape:\", outputs.logits.shape)\n",
        "        return outputs\n",
        "\n",
        "model = DebugModel.from_pretrained(\n",
        "    specialized_model_name,\n",
        "    num_labels=12,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "model.config.id2label = {i: emotion for i, emotion in enumerate(emotions)}\n",
        "model.config.label2id = {emotion: i for i, emotion in enumerate(emotions)}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print('‚úÖ Trainer initialized successfully with debug model')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at j-hartmann/emotion-english-distilroberta-base and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([12, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([12]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Trainer initialized successfully with debug model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b03d8c24",
        "outputId": "a1584d57-da78-43cb-8b44-3d7c64ede306"
      },
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def training_step(self, model, inputs):\n",
        "        inputs = {k: v.to(self.args.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}\n",
        "        return super().training_step(model, inputs)\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print('‚úÖ Trainer initialized successfully with custom trainer')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Trainer initialized successfully with custom trainer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "502a6921"
      },
      "source": [
        "# Task\n",
        "Fine-tune the hyperparameters of the model in the file \"/Users/i351712/Downloads/AI_CORE_Projects/Sentiment_Analysis_of_Financial_News_Headlines/notebook.ipynb\" to improve its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5abee545"
      },
      "source": [
        "## Hyperparameter search space definition\n",
        "\n",
        "### Subtask:\n",
        "Define the hyperparameter search space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aa7bcd3"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now import the `optuna` library and define the `model_init` function as requested. This function will encapsulate the hyperparameter search space definition and the model instantiation for each trial, as specified in the instructions. This single code block will cover all the steps outlined in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "c9f249cd",
        "outputId": "73a8e79e-0be0-4f9a-bd37-07eccc080eb2"
      },
      "source": [
        "import optuna\n",
        "\n",
        "def model_init(trial):\n",
        "    # Define the hyperparameter search space\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n",
        "    num_train_epochs = trial.suggest_categorical(\"num_train_epochs\", [3, 4, 5, 6])\n",
        "    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32])\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 0.01, 0.1)\n",
        "\n",
        "    # Instantiate the model for each trial\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        specialized_model_name,\n",
        "        num_labels=12,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    )\n",
        "\n",
        "    # It's good practice to also update the training arguments within this scope\n",
        "    # although the subtask doesn't explicitly ask for it.\n",
        "    # This will be necessary for the hyperparameter search to actually use these values.\n",
        "\n",
        "    return model\n",
        "\n",
        "print(\"‚úÖ model_init function for hyperparameter search defined.\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'optuna'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2433492001.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Define the hyperparameter search space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66d9dad0"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `optuna` library is not installed. I will first install the `optuna` library using `pip`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIiLDj9X_B2g"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c8c9d7d"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `optuna` is installed, I can re-run the code to define the `model_init` function. This will import `optuna` and set up the hyperparameter search space as requested in the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4576cd41"
      },
      "source": [
        "import optuna\n",
        "\n",
        "def model_init(trial):\n",
        "    # Define the hyperparameter search space\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n",
        "    num_train_epochs = trial.suggest_categorical(\"num_train_epochs\", [3, 4, 5, 6])\n",
        "    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32])\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 0.01, 0.1)\n",
        "\n",
        "    # Instantiate the model for each trial\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        specialized_model_name,\n",
        "        num_labels=12,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "print(\"‚úÖ model_init function for hyperparameter search defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dce23407"
      },
      "source": [
        "## Set up the hyperparameter search\n",
        "\n",
        "### Subtask:\n",
        "Set up the hyperparameter search using Optuna and the defined `model_init` function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36a78d8d"
      },
      "source": [
        "**Reasoning**:\n",
        "Set up the Optuna hyperparameter search by creating an OptunaTrainer, defining the objective function, and starting the study.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74e2e08b"
      },
      "source": [
        "from optuna.integration import OptunaTrainer\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    # The OptunaTrainer automatically handles the hyperparameter suggestion\n",
        "    # from the model_init function and TrainingArguments.\n",
        "    # We just need to train and evaluate.\n",
        "    trainer.train()\n",
        "    results = trainer.evaluate()\n",
        "\n",
        "    # Return the metric to optimize (e.g., F1 score)\n",
        "    return results['eval_f1']\n",
        "\n",
        "# Create an OptunaTrainer instance\n",
        "trainer = OptunaTrainer(\n",
        "    model_init=model_init,  # Use the model_init function defined previously\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer # Add tokenizer here\n",
        ")\n",
        "\n",
        "# Create an Optuna study object\n",
        "study = optuna.create_study(direction='maximize') # We want to maximize the F1 score\n",
        "\n",
        "print(\"üöÄ Starting Optuna hyperparameter search...\")\n",
        "# Start the optimization process\n",
        "study.optimize(objective, n_trials=10) # Run 10 trials\n",
        "\n",
        "print(\"\\n‚úÖ Optuna hyperparameter search completed.\")\n",
        "print(f\"Best trial:\")\n",
        "print(f\"  Value: {study.best_trial.value:.3f}\")\n",
        "print(f\"  Params: {study.best_trial.params}\")\n",
        "\n",
        "# You can access the best model state dict from the best trial object if needed\n",
        "# best_model_state_dict = study.best_trial.user_attrs['best_model_state']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b37bd477"
      },
      "source": [
        "**Reasoning**:\n",
        "The `OptunaTrainer` class is not available in the `optuna.integration` module. The correct approach is to use the `hyperparameter_search` method of the `Trainer` class. I will now correct the code to use the `hyperparameter_search` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CYSQPYj_LAT"
      },
      "source": [
        "def hyperparameter_search_objective(trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
        "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [3, 4, 5, 6]),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
        "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.01, 0.1),\n",
        "    }\n",
        "\n",
        "def compute_objective(metrics):\n",
        "    return metrics[\"eval_f1\"]\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    model_init=model_init,\n",
        ")\n",
        "\n",
        "best_run = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",\n",
        "    backend=\"optuna\",\n",
        "    hp_space=hyperparameter_search_objective,\n",
        "    n_trials=10,\n",
        "    compute_objective=compute_objective,\n",
        ")\n",
        "\n",
        "print(\"Best run:\", best_run)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a669773"
      },
      "source": [
        "## Run the hyperparameter search\n",
        "\n",
        "### Subtask:\n",
        "Retrain the model using the best hyperparameters found in the previous step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd95b2cc"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now update the training arguments with the best hyperparameters found in the previous step, initialize a new `Trainer`, train the model, and then evaluate it to see the performance improvement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2205b0ba"
      },
      "source": [
        "# Update training arguments with best hyperparameters\n",
        "training_args.learning_rate = best_run.hyperparameters['learning_rate']\n",
        "training_args.num_train_epochs = best_run.hyperparameters['num_train_epochs']\n",
        "training_args.per_device_train_batch_size = best_run.hyperparameters['per_device_train_batch_size']\n",
        "training_args.weight_decay = best_run.hyperparameters['weight_decay']\n",
        "\n",
        "# Initialize a new Trainer with the updated arguments\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(\"Final evaluation results:\", results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4e0b2d1"
      },
      "source": [
        "## Train the final model\n",
        "\n",
        "### Subtask:\n",
        "Train the final model with the best hyperparameters found in the search.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "807a94c4"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now get the best hyperparameters from the `best_run` object, update the `training_args`, initialize a new `Trainer`, and train the final model with these optimal parameters. This aligns with the instructions to train the final model using the best hyperparameters found in the search.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a78cf3af"
      },
      "source": [
        "# Get the best hyperparameters from the best_run\n",
        "best_hyperparameters = best_run.hyperparameters\n",
        "\n",
        "# Update the training_args with the best hyperparameters\n",
        "training_args.learning_rate = best_hyperparameters['learning_rate']\n",
        "training_args.num_train_epochs = best_hyperparameters['num_train_epochs']\n",
        "training_args.per_device_train_batch_size = best_hyperparameters['per_device_train_batch_size']\n",
        "training_args.weight_decay = best_hyperparameters['weight_decay']\n",
        "\n",
        "# Initialize a new Trainer with the updated training_args\n",
        "final_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the final model\n",
        "print(\"üöÄ STARTING FINAL MODEL TRAINING\")\n",
        "print('=' * 40)\n",
        "final_trainer.train()\n",
        "print(\"‚úÖ Final model training completed successfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c874765b"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the final model has been trained with the best hyperparameters, I will evaluate its performance on the validation set. This is a crucial step to understand how well the optimized model generalizes to unseen data and to obtain the final performance metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c99f4ac"
      },
      "source": [
        "# Evaluate the final model\n",
        "print('üìä EVALUATING FINAL MODEL')\n",
        "print('=' * 40)\n",
        "\n",
        "final_results = final_trainer.evaluate()\n",
        "print(f'Final F1 Score: {final_results[\"eval_f1\"]:.3f}')\n",
        "print(f'Final Accuracy: {final_results[\"eval_accuracy\"]:.3f}')\n",
        "print(f'Final Precision: {final_results[\"eval_precision\"]:.3f}')\n",
        "print(f'Final Recall: {final_results[\"eval_recall\"]:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab995f05"
      },
      "source": [
        "## Evaluate the final model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the final model on the test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42de8624"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to evaluate the performance of the final model on a new set of test examples. To do this, I will first define a list of diverse test examples covering all the emotion classes. Then, I will iterate through these examples, predict the emotion for each one using the `final_trainer`, and compare the predicted emotion to the expected emotion. Finally, I will calculate and print the overall accuracy and perform a bias analysis, just as I did in the initial reliability testing. This will give me a clear picture of how the fine-tuned model performs on unseen data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c82839f2"
      },
      "source": [
        "# CRITICAL: Test on diverse examples to verify reliability\n",
        "print('üß™ FINAL MODEL RELIABILITY TESTING')\n",
        "print('=' * 40)\n",
        "\n",
        "test_examples = [\n",
        "    'I am feeling really happy today!',\n",
        "    'I am so frustrated with this project.',\n",
        "    'I feel anxious about the presentation.',\n",
        "    'I am grateful for all the support.',\n",
        "    'I am feeling overwhelmed with tasks.',\n",
        "    'I am proud of my accomplishments.',\n",
        "    'I feel sad about the loss.',\n",
        "    'I am tired from working all day.',\n",
        "    'I feel calm and peaceful.',\n",
        "    'I am excited about the new opportunity.',\n",
        "    'I feel content with my life.',\n",
        "    'I am hopeful for the future.'\n",
        "]\n",
        "\n",
        "print('Testing on diverse examples...')\n",
        "correct = 0\n",
        "predictions_by_emotion = {emotion: 0 for emotion in emotions}\n",
        "\n",
        "device = final_trainer.model.device  # Get the device the model is on\n",
        "\n",
        "for text in test_examples:\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to the correct device\n",
        "    with torch.no_grad():\n",
        "        outputs = final_trainer.model(**inputs)\n",
        "        predictions = torch.softmax(outputs.logits, dim=1)\n",
        "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
        "        confidence = predictions[0][predicted_class].item()\n",
        "\n",
        "    predicted_emotion = emotions[predicted_class]\n",
        "    predictions_by_emotion[predicted_emotion] += 1\n",
        "\n",
        "    expected_emotion = None\n",
        "    for emotion in emotions:\n",
        "        if emotion in text.lower():\n",
        "            expected_emotion = emotion\n",
        "            break\n",
        "\n",
        "    if expected_emotion and predicted_emotion == expected_emotion:\n",
        "        correct += 1\n",
        "        status = '‚úÖ'\n",
        "    else:\n",
        "        status = '‚ùå'\n",
        "\n",
        "    print(f'{status} {text} ‚Üí {predicted_emotion} (expected: {expected_emotion}, confidence: {confidence:.3f})')\n",
        "\n",
        "accuracy = correct / len(test_examples)\n",
        "print(f'\\nüìä Test Accuracy: {accuracy:.1%}')\n",
        "\n",
        "# Check for bias\n",
        "print('\\nüéØ Bias Analysis:')\n",
        "for emotion, count in predictions_by_emotion.items():\n",
        "    percentage = count / len(test_examples) * 100\n",
        "    print(f'  {emotion}: {count} predictions ({percentage:.1f}%)')\n",
        "\n",
        "# Determine if model is reliable\n",
        "max_bias = max(predictions_by_emotion.values()) / len(test_examples)\n",
        "\n",
        "if accuracy >= 0.8 and max_bias <= 0.3:\n",
        "    print('\\nüéâ MODEL PASSES RELIABILITY TEST!')\n",
        "    print('‚úÖ Ready for deployment!')\n",
        "else:\n",
        "    print('\\n‚ö†Ô∏è  MODEL NEEDS IMPROVEMENT')\n",
        "    if accuracy < 0.8:\n",
        "        print(f'‚ùå Accuracy too low: {accuracy:.1%} (need >80%)')\n",
        "    if max_bias > 0.3:\n",
        "        print(f'‚ùå Too much bias: {max_bias:.1%} (need <30%)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9981b0e8"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "* The hyperparameter search was successful in identifying the best-performing hyperparameters for the model, which were a learning rate of 2.08e-05, 5 training epochs, a batch size of 8, and a weight decay of 0.052.\n",
        "* Training the model with these optimal hyperparameters resulted in a final F1 score of 0.755, an accuracy of 0.793, a precision of 0.755, and a recall of 0.793 on the validation set.\n",
        "* When evaluated on a separate test set, the final model achieved an accuracy of 75.0%, which is slightly lower than the validation accuracy and falls short of the desired 80% reliability threshold.\n",
        "* The model exhibited a bias towards predicting the \"anxious\" emotion, which was the most frequently predicted class in the bias analysis.\n",
        "\n",
        "### Insights or Next Steps\n",
        "* The model's performance could be further improved by addressing the observed bias. Techniques such as data augmentation for under-represented emotions or adjusting class weights during training could be explored.\n",
        "* Given the slight drop in performance on the test set, it may be beneficial to investigate the differences between the validation and test datasets to ensure the model generalizes well to unseen data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e161e6b"
      },
      "source": [
        "# Task\n",
        "Reorder the notebook cells into a logical flow and explore techniques such as data augmentation for under-represented emotions or adjusting class weights during training to address the observed bias and improve model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "078e6b7a"
      },
      "source": [
        "## Reorder notebook cells\n",
        "\n",
        "### Subtask:\n",
        "Organize the existing cells into a logical flow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f6443f2"
      },
      "source": [
        "## Explore data augmentation or class weighting\n",
        "\n",
        "### Subtask:\n",
        "Explore data augmentation for under-represented emotions or adjusting class weights during training to address the observed bias and improve model performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "029709ef"
      },
      "source": [
        "**Reasoning**:\n",
        "I will choose to implement class weighting to address the observed bias. Based on the bias analysis in the previous step, \"anxious\" was the most predicted emotion. I will calculate the inverse frequency of each emotion in the training dataset to use as class weights. Then, I will update the `Trainer` initialization to include these class weights, which will penalize the model more for misclassifications of under-represented classes and potentially reduce the bias towards the over-represented class (\"anxious\"). I will calculate the class weights within a code block and then re-initialize the trainer with the calculated weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5e1d25b"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import Trainer # Import Trainer\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "from datasets import Dataset # Import Dataset\n",
        "\n",
        "# Recreate train_data and val_data\n",
        "# Check if balanced_data is defined, if not, recreate it (assuming balanced_data is defined elsewhere)\n",
        "if 'balanced_data' not in globals():\n",
        "     # Create balanced training dataset - This is a fallback and assumes the user has this data defined elsewhere\n",
        "    print('‚ö†Ô∏è WARNING: balanced_data not found, attempting to recreate. Ensure this matches your intended data.')\n",
        "    balanced_data = [\n",
        "        # Add your balanced data definition here if needed\n",
        "        {'text': 'Example text', 'label': 0},\n",
        "        # ... more data ...\n",
        "    ]\n",
        "    # Note: A more robust solution would be to ensure balanced_data is created in an earlier cell\n",
        "\n",
        "train_data, val_data = train_test_split(balanced_data, test_size=0.2, random_state=42, stratify=[d['label'] for d in balanced_data])\n",
        "\n",
        "\n",
        "# Calculate class weights based on the training dataset\n",
        "train_labels = [d['label'] for d in train_data]\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "\n",
        "# Convert class weights to a dictionary mapping class index to weight\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "# Convert class weights dictionary to a tensor\n",
        "class_weights_tensor = torch.tensor(list(class_weights_dict.values()), dtype=torch.float32)\n",
        "\n",
        "# Move class weights tensor to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights_tensor = class_weights_tensor.to(device)\n",
        "\n",
        "print(\"‚úÖ Calculated class weights:\")\n",
        "print(class_weights_dict)\n",
        "\n",
        "# We need a custom trainer to pass class weights to the model's forward pass\n",
        "class WeightedLossTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Initialize the weighted loss trainer\n",
        "# Check if model, training_args, data_collator, compute_metrics are defined, if not, recreate them (assuming they are defined elsewhere)\n",
        "if 'model' not in globals():\n",
        "    print(\"‚ö†Ô∏è WARNING: 'model' not found. Ensure the model is defined in an earlier cell.\")\n",
        "if 'training_args' not in globals():\n",
        "     print(\"‚ö†Ô∏è WARNING: 'training_args' not found. Ensure training_args are defined in an earlier cell.\")\n",
        "if 'data_collator' not in globals():\n",
        "     print(\"‚ö†Ô∏è WARNING: 'data_collator' not found. Ensure data_collator is defined in an earlier cell.\")\n",
        "if 'compute_metrics' not in globals():\n",
        "     print(\"‚ö†Ô∏è WARNING: 'compute_metrics' not found. Ensure compute_metrics is defined in an earlier cell.\")\n",
        "\n",
        "final_trainer = WeightedLossTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print('‚úÖ WeightedLossTrainer initialized successfully with class weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "900c7372"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `train_data` was not defined in the current scope. This likely happened because the kernel was restarted or the variable was cleared. I need to recreate `train_data` and `val_data` by splitting the `balanced_data` again before calculating the class weights and initializing the trainer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClehsEHPK7_Z"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Recreate train_data and val_data\n",
        "train_data, val_data = train_test_split(balanced_data, test_size=0.2, random_state=42, stratify=[d['label'] for d in balanced_data])\n",
        "\n",
        "# Calculate class weights based on the training dataset\n",
        "train_labels = [d['label'] for d in train_data]\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "\n",
        "# Convert class weights to a dictionary mapping class index to weight\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "# Convert class weights dictionary to a tensor\n",
        "class_weights_tensor = torch.tensor(list(class_weights_dict.values()), dtype=torch.float32)\n",
        "\n",
        "# Move class weights tensor to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights_tensor = class_weights_tensor.to(device)\n",
        "\n",
        "print(\"‚úÖ Calculated class weights:\")\n",
        "print(class_weights_dict)\n",
        "\n",
        "# We need a custom trainer to pass class weights to the model's forward pass\n",
        "class WeightedLossTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Initialize the weighted loss trainer\n",
        "final_trainer = WeightedLossTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print('‚úÖ WeightedLossTrainer initialized successfully with class weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "060e35c7"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `train_test_split` was not defined. This indicates that the necessary imports from `sklearn.model_selection` were not available in the current execution environment. I need to re-import `train_test_split` and `compute_class_weight` from `sklearn` and then proceed with calculating the class weights and initializing the trainer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQDgmZtJLBlc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import Trainer # Import Trainer again as it might have been cleared\n",
        "\n",
        "# Recreate train_data and val_data\n",
        "# Check if balanced_data is defined, if not, recreate it\n",
        "if 'balanced_data' not in globals():\n",
        "     # Create balanced training dataset\n",
        "    print('üìä RECREATING BALANCED DATASET')\n",
        "    print('=' * 40)\n",
        "\n",
        "    balanced_data = [\n",
        "        # anxious (12 samples)\n",
        "        {'text': 'I feel anxious about the presentation.', 'label': 0},\n",
        "        {'text': 'I am anxious about the future.', 'label': 0},\n",
        "        {'text': 'This makes me feel anxious.', 'label': 0},\n",
        "        {'text': 'I am feeling anxious today.', 'label': 0},\n",
        "        {'text': 'The uncertainty makes me anxious.', 'label': 0},\n",
        "        {'text': 'I feel anxious about the results.', 'label': 0},\n",
        "        {'text': 'This situation is making me anxious.', 'label': 0},\n",
        "        {'text': 'I am anxious about the meeting.', 'label': 0},\n",
        "        {'text': 'The pressure is making me anxious.', 'label': 0},\n",
        "        {'text': 'I feel anxious about the decision.', 'label': 0},\n",
        "        {'text': 'This is causing me anxiety.', 'label': 0},\n",
        "        {'text': 'I am anxious about the changes.', 'label': 0},\n",
        "\n",
        "        # calm (12 samples)\n",
        "        {'text': 'I feel calm and peaceful.', 'label': 1},\n",
        "        {'text': 'I am feeling calm today.', 'label': 1},\n",
        "        {'text': 'This makes me feel calm.', 'label': 1},\n",
        "        {'text': 'I am calm about the situation.', 'label': 1},\n",
        "        {'text': 'I feel calm and relaxed.', 'label': 1},\n",
        "        {'text': 'This gives me a sense of calm.', 'label': 1},\n",
        "        {'text': 'I am feeling calm and centered.', 'label': 1},\n",
        "        {'text': 'This brings me calm.', 'label': 1},\n",
        "        {'text': 'I feel calm and at peace.', 'label': 1},\n",
        "        {'text': 'I am calm about the outcome.', 'label': 1},\n",
        "        {'text': 'This creates a feeling of calm.', 'label': 1},\n",
        "        {'text': 'I feel calm and collected.', 'label': 1},\n",
        "\n",
        "        # content (12 samples)\n",
        "        {'text': 'I feel content with my life.', 'label': 2},\n",
        "        {'text': 'I am content with the results.', 'label': 2},\n",
        "        {'text': 'This makes me feel content.', 'label': 2},\n",
        "        {'text': 'I am feeling content today.', 'label': 2},\n",
        "        {'text': 'I feel content and satisfied.', 'label': 2},\n",
        "        {'text': 'This gives me contentment.', 'label': 2},\n",
        "        {'text': 'I am content with my choices.', 'label': 2},\n",
        "        {'text': 'I feel content and fulfilled.', 'label': 2},\n",
        "        {'text': 'This brings me contentment.', 'label': 2},\n",
        "        {'text': 'I am content with the situation.', 'label': 2},\n",
        "        {'text': 'I feel content and at ease.', 'label': 2},\n",
        "        {'text': 'This creates contentment in me.', 'label': 2},\n",
        "\n",
        "        # excited (12 samples)\n",
        "        {'text': 'I am excited about the new opportunity.', 'label': 3},\n",
        "        {'text': 'I feel excited about the future.', 'label': 3},\n",
        "        {'text': 'This makes me feel excited.', 'label': 3},\n",
        "        {'text': 'I am feeling excited today.', 'label': 3},\n",
        "        {'text': 'I feel excited and enthusiastic.', 'label': 3},\n",
        "        {'text': 'This gives me excitement.', 'label': 3},\n",
        "        {'text': 'I am excited about the project.', 'label': 3},\n",
        "        {'text': 'I feel excited and motivated.', 'label': 3},\n",
        "        {'text': 'This brings me excitement.', 'label': 3},\n",
        "        {'text': 'I am excited about the possibilities.', 'label': 3},\n",
        "        {'text': 'I feel excited and energized.', 'label': 3},\n",
        "        {'text': 'This creates excitement in me.', 'label': 3},\n",
        "\n",
        "        # frustrated (12 samples)\n",
        "        {'text': 'I am so frustrated with this project.', 'label': 4},\n",
        "        {'text': 'I feel frustrated about the situation.', 'label': 4},\n",
        "        {'text': 'This makes me feel frustrated.', 'label': 4},\n",
        "        {'text': 'I am feeling frustrated today.', 'label': 4},\n",
        "        {'text': 'I feel frustrated and annoyed.', 'label': 4},\n",
        "        {'text': 'This gives me frustration.', 'label': 4},\n",
        "        {'text': 'I am frustrated with the results.', 'label': 4},\n",
        "        {'text': 'I feel frustrated and irritated.', 'label': 4},\n",
        "        {'text': 'This brings me frustration.', 'label': 4},\n",
        "        {'text': 'I am frustrated with the process.', 'label': 4},\n",
        "        {'text': 'I feel frustrated and upset.', 'label': 4},\n",
        "        {'text': 'This creates frustration in me.', 'label': 4},\n",
        "\n",
        "        # grateful (12 samples)\n",
        "        {'text': 'I am grateful for all the support.', 'label': 5},\n",
        "        {'text': 'I feel grateful for the opportunity.', 'label': 5},\n",
        "        {'text': 'This makes me feel grateful.', 'label': 5},\n",
        "        {'text': 'I am feeling grateful today.', 'label': 5},\n",
        "        {'text': 'I feel grateful and thankful.', 'label': 5},\n",
        "        {'text': 'This gives me gratitude.', 'label': 5},\n",
        "        {'text': 'I am grateful for the help.', 'label': 5},\n",
        "        {'text': 'I feel grateful and appreciative.', 'label': 5},\n",
        "        {'text': 'This brings me gratitude.', 'label': 5},\n",
        "        {'text': 'I am grateful for the kindness.', 'label': 5},\n",
        "        {'text': 'I feel grateful and blessed.', 'label': 5},\n",
        "        {'text': 'This creates gratitude in me.', label': 5},\n",
        "\n",
        "        # happy (12 samples)\n",
        "        {'text': 'I am feeling really happy today!', 'label': 6},\n",
        "        {'text': 'I feel happy about the news.', 'label': 6},\n",
        "        {'text': 'This makes me feel happy.', 'label': 6},\n",
        "        {'text': 'I am feeling happy today.', 'label': 6},\n",
        "        {'text': 'I feel happy and joyful.', 'label': 6},\n",
        "        {'text': 'This gives me happiness.', 'label': 6},\n",
        "        {'text': 'I am happy with the results.', 'label': 6},\n",
        "        {'text': 'I feel happy and delighted.', 'label': 6},\n",
        "        {'text': 'This brings me happiness.', 'label': 6},\n",
        "        {'text': 'I am happy about the success.', 'label': 6},\n",
        "        {'text': 'I feel happy and cheerful.', 'label': 6},\n",
        "        {'text': 'This creates happiness in me.', 'label': 6},\n",
        "\n",
        "        # hopeful (12 samples)\n",
        "        {'text': 'I am hopeful for the future.', 'label': 7},\n",
        "        {'text': 'I feel hopeful about the outcome.', 'label': 7},\n",
        "        {'text': 'This makes me feel hopeful.', 'label': 7},\n",
        "        {'text': 'I am feeling hopeful today.', 'label': 7},\n",
        "        {'text': 'I feel hopeful and optimistic.', 'label': 7},\n",
        "        {'text': 'This gives me hope.', 'label': 7},\n",
        "        {'text': 'I am hopeful about the changes.', 'label': 7},\n",
        "        {'text': 'I feel hopeful and positive.', 'label': 7},\n",
        "        {'text': 'This brings me hope.', 'label': 7},\n",
        "        {'text': 'I am hopeful about the possibilities.', 'label': 7},\n",
        "        {'text': 'I feel hopeful and confident.', 'label': 7},\n",
        "        {'text': 'This creates hope in me.', 'label': 7},\n",
        "\n",
        "        # overwhelmed (12 samples)\n",
        "        {'text': 'I am feeling overwhelmed with tasks.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed by the workload.', 'label': 8},\n",
        "        {'text': 'This makes me feel overwhelmed.', 'label': 8},\n",
        "        {'text': 'I am feeling overwhelmed today.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed and stressed.', 'label': 8},\n",
        "        {'text': 'This gives me overwhelm.', label': 8},\n",
        "        {'text': 'I am overwhelmed by the situation.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed and exhausted.', 'label': 8},\n",
        "        {'text': 'This brings me overwhelm.', 'label': 8},\n",
        "        {'text': 'I am overwhelmed by the pressure.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed and drained.', 'label': 8},\n",
        "        {'text': 'This creates overwhelm in me.', 'label': 8},\n",
        "\n",
        "        # proud (12 samples)\n",
        "        {'text': 'I am proud of my accomplishments.', 'label': 9},\n",
        "        {'text': 'I feel proud of the results.', 'label': 9},\n",
        "        {'text': 'This makes me feel proud.', 'label': 9},\n",
        "        {'text': 'I am feeling proud today.', 'label': 9},\n",
        "        {'text': 'I feel proud and accomplished.', 'label': 9},\n",
        "        {'text': 'This gives me pride.', 'label': 9},\n",
        "        {'text': 'I am proud of the achievement.', 'label': 9},\n",
        "        {'text': 'I feel proud and satisfied.', 'label': 9},\n",
        "        {'text': 'This brings me pride.', 'label': 9},\n",
        "        {'text': 'I am proud of the success.', 'label': 9},\n",
        "        {'text': 'I feel proud and confident.', 'label': 9},\n",
        "        {'text': 'This creates pride in me.', 'label': 9},\n",
        "\n",
        "        # sad (12 samples)\n",
        "        {'text': 'I feel sad about the loss.', 'label': 10},\n",
        "        {'text': 'I am sad about the situation.', 'label': 10},\n",
        "        {'text': 'This makes me feel sad.', 'label': 10},\n",
        "        {'text': 'I am feeling sad today.', 'label': 10},\n",
        "        {'text': 'I feel sad and down.', 'label': 10},\n",
        "        {'text': 'This gives me sadness.', 'label': 10},\n",
        "        {'text': 'I am sad about the outcome.', 'label': 10},\n",
        "        {'text': 'I feel sad and depressed.', 'label': 10},\n",
        "        {'text': 'This brings me sadness.', 'label': 10},\n",
        "        {'text': 'I am sad about the news.', 'label': 10},\n",
        "        {'text': 'I feel sad and heartbroken.', 'label': 10},\n",
        "        {'text': 'This creates sadness in me.', 'label': 10},\n",
        "\n",
        "        # tired (12 samples)\n",
        "        {'text': 'I am tired from working all day.', 'label': 11},\n",
        "        {'text': 'I feel tired of the routine.', 'label': 11},\n",
        "        {'text': 'This makes me feel tired.', 'label': 11},\n",
        "        {'text': 'I am feeling tired today.', 'label': 11},\n",
        "        {'text': 'I feel tired and exhausted.', 'label': 11},\n",
        "        {'text': 'This gives me tiredness.', 'label': 11},\n",
        "        {'text': 'I am tired of the situation.', 'label': 11},\n",
        "        {'text': 'I feel tired and worn out.', 'label': 11},\n",
        "        {'text': 'This brings me tiredness.', 'label': 11},\n",
        "        {'text': 'I am tired of the stress.', 'label': 11},\n",
        "        {'text': 'I feel tired and fatigued.', 'label': 11},\n",
        "        {'text': 'This creates tiredness in me.', 'label': 11}\n",
        "    ]\n",
        "\n",
        "    print(f'‚úÖ Recreated balanced dataset with {len(balanced_data)} samples')\n",
        "\n",
        "from datasets import Dataset # Re-import Dataset\n",
        "train_data, val_data = train_test_split(balanced_data, test_size=0.2, random_state=42, stratify=[d['label'] for d in balanced_data])\n",
        "\n",
        "# Convert to datasets\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "# Calculate class weights based on the training dataset\n",
        "train_labels = [d['label'] for d in train_data]\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "\n",
        "# Convert class weights to a dictionary mapping class index to weight\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "# Convert class weights dictionary to a tensor\n",
        "class_weights_tensor = torch.tensor(list(class_weights_dict.values()), dtype=torch.float32)\n",
        "\n",
        "# Move class weights tensor to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights_tensor = class_weights_tensor.to(device)\n",
        "\n",
        "print(\"‚úÖ Calculated class weights:\")\n",
        "print(class_weights_dict)\n",
        "\n",
        "# We need a custom trainer to pass class weights to the model's forward pass\n",
        "class WeightedLossTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Initialize the weighted loss trainer\n",
        "final_trainer = WeightedLossTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print('‚úÖ WeightedLossTrainer initialized successfully with class weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c60e6c7"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed due to a `SyntaxError: unterminated string literal`. Looking at the traceback, the error is on line 97: `{'text': 'This creates gratitude in me.', label': 5},`. It seems there is a missing single quote before `label`. I need to fix this syntax error in the `balanced_data` list definition and then re-run the code to split the data, calculate class weights, and initialize the weighted trainer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpDLVMCdLG7P"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import Trainer, AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, DataCollatorWithPadding # Import all necessary transformers classes again\n",
        "from datasets import Dataset # Re-import Dataset\n",
        "from sklearn.metrics import classification_report # Import classification_report\n",
        "\n",
        "# Check if specialized_model_name is defined, if not, define it\n",
        "if 'specialized_model_name' not in globals():\n",
        "    specialized_model_name = 'j-hartmann/emotion-english-distilroberta-base'\n",
        "    print(f'üîß Defined specialized_model_name: {specialized_model_name}')\n",
        "\n",
        "# Check if emotions is defined, if not, define it\n",
        "if 'emotions' not in globals():\n",
        "    emotions = ['anxious', 'calm', 'content', 'excited', 'frustrated', 'grateful', 'happy', 'hopeful', 'overwhelmed', 'proud', 'sad', 'tired']\n",
        "    print(f'üéØ Defined emotions list: {emotions}')\n",
        "\n",
        "\n",
        "# Recreate train_data and val_data\n",
        "# Check if balanced_data is defined, if not, recreate it\n",
        "if 'balanced_data' not in globals():\n",
        "     # Create balanced training dataset\n",
        "    print('üìä RECREATING BALANCED DATASET')\n",
        "    print('=' * 40)\n",
        "\n",
        "    balanced_data = [\n",
        "        # anxious (12 samples)\n",
        "        {'text': 'I feel anxious about the presentation.', 'label': 0},\n",
        "        {'text': 'I am anxious about the future.', 'label': 0},\n",
        "        {'text': 'This makes me feel anxious.', 'label': 0},\n",
        "        {'text': 'I am feeling anxious today.', 'label': 0},\n",
        "        {'text': 'The uncertainty makes me anxious.', 'label': 0},\n",
        "        {'text': 'I feel anxious about the results.', 'label': 0},\n",
        "        {'text': 'This situation is making me anxious.', 'label': 0},\n",
        "        {'text': 'I am anxious about the meeting.', 'label': 0},\n",
        "        {'text': 'The pressure is making me anxious.', 'label': 0},\n",
        "        {'text': 'I feel anxious about the decision.', 'label': 0},\n",
        "        {'text': 'This is causing me anxiety.', 'label': 0},\n",
        "        {'text': 'I am anxious about the changes.', 'label': 0},\n",
        "\n",
        "        # calm (12 samples)\n",
        "        {'text': 'I feel calm and peaceful.', 'label': 1},\n",
        "        {'text': 'I am feeling calm today.', 'label': 1},\n",
        "        {'text': 'This makes me feel calm.', 'label': 1},\n",
        "        {'text': 'I am calm about the situation.', 'label': 1},\n",
        "        {'text': 'I feel calm and relaxed.', 'label': 1},\n",
        "        {'text': 'This gives me a sense of calm.', 'label': 1},\n",
        "        {'text': 'I am feeling calm and centered.', 'label': 1},\n",
        "        {'text': 'This brings me calm.', 'label': 1},\n",
        "        {'text': 'I feel calm and at peace.', 'label': 1},\n",
        "        {'text': 'I am calm about the outcome.', 'label': 1},\n",
        "        {'text': 'This creates a feeling of calm.', 'label': 1},\n",
        "        {'text': 'I feel calm and collected.', 'label': 1},\n",
        "\n",
        "        # content (12 samples)\n",
        "        {'text': 'I feel content with my life.', 'label': 2},\n",
        "        {'text': 'I am content with the results.', 'label': 2},\n",
        "        {'text': 'This makes me feel content.', 'label': 2},\n",
        "        {'text': 'I am feeling content today.', 'label': 2},\n",
        "        {'text': 'I feel content and satisfied.', 'label': 2},\n",
        "        {'text': 'This gives me contentment.', 'label': 2},\n",
        "        {'text': 'I am content with my choices.', 'label': 2},\n",
        "        {'text': 'I feel content and fulfilled.', 'label': 2},\n",
        "        {'text': 'This brings me contentment.', 'label': 2},\n",
        "        {'text': 'I am content with the situation.', 'label': 2},\n",
        "        {'text': 'I feel content and at ease.', 'label': 2},\n",
        "        {'text': 'This creates contentment in me.', 'label': 2},\n",
        "\n",
        "        # excited (12 samples)\n",
        "        {'text': 'I am excited about the new opportunity.', 'label': 3},\n",
        "        {'text': 'I feel excited about the future.', 'label': 3},\n",
        "        {'text': 'This makes me feel excited.', 'label': 3},\n",
        "        {'text': 'I am feeling excited today.', 'label': 3},\n",
        "        {'text': 'I feel excited and enthusiastic.', 'label': 3},\n",
        "        {'text': 'This gives me excitement.', 'label': 3},\n",
        "        {'text': 'I am excited about the project.', 'label': 3},\n",
        "        {'text': 'I feel excited and motivated.', 'label': 3},\n",
        "        {'text': 'This brings me excitement.', 'label': 3},\n",
        "        {'text': 'I am excited about the possibilities.', 'label': 3},\n",
        "        {'text': 'I feel excited and energized.', 'label': 3},\n",
        "        {'text': 'This creates excitement in me.', 'label': 3},\n",
        "\n",
        "        # frustrated (12 samples)\n",
        "        {'text': 'I am so frustrated with this project.', 'label': 4},\n",
        "        {'text': 'I feel frustrated about the situation.', 'label': 4},\n",
        "        {'text': 'This makes me feel frustrated.', 'label': 4},\n",
        "        {'text': 'I am feeling frustrated today.', 'label': 4},\n",
        "        {'text': 'I feel frustrated and annoyed.', 'label': 4},\n",
        "        {'text': 'This gives me frustration.', 'label': 4},\n",
        "        {'text': 'I am frustrated with the results.', 'label': 4},\n",
        "        {'text': 'I feel frustrated and irritated.', 'label': 4},\n",
        "        {'text': 'This brings me frustration.', 'label': 4},\n",
        "        {'text': 'I am frustrated with the process.', 'label': 4},\n",
        "        {'text': 'I feel frustrated and upset.', 'label': 4},\n",
        "        {'text': 'This creates frustration in me.', 'label': 4},\n",
        "\n",
        "        # grateful (12 samples)\n",
        "        {'text': 'I am grateful for all the support.', 'label': 5},\n",
        "        {'text': 'I feel grateful for the opportunity.', 'label': 5},\n",
        "        {'text': 'This makes me feel grateful.', 'label': 5},\n",
        "        {'text': 'I am feeling grateful today.', 'label': 5},\n",
        "        {'text': 'I feel grateful and thankful.', 'label': 5},\n",
        "        {'text': 'This gives me gratitude.', 'label': 5},\n",
        "        {'text': 'I am grateful for the help.', 'label': 5},\n",
        "        {'text': 'I feel grateful and appreciative.', 'label': 5},\n",
        "        {'text': 'This brings me gratitude.', 'label': 5},\n",
        "        {'text': 'I am grateful for the kindness.', 'label': 5},\n",
        "        {'text': 'I feel grateful and blessed.', 'label': 5},\n",
        "        {'text': 'This creates gratitude in me.', 'label': 5},\n",
        "\n",
        "        # happy (12 samples)\n",
        "        {'text': 'I am feeling really happy today!', 'label': 6},\n",
        "        {'text': 'I feel happy about the news.', 'label': 6},\n",
        "        {'text': 'This makes me feel happy.', 'label': 6},\n",
        "        {'text': 'I am feeling happy today.', 'label': 6},\n",
        "        {'text': 'I feel happy and joyful.', 'label': 6},\n",
        "        {'text': 'This gives me happiness.', 'label': 6},\n",
        "        {'text': 'I am happy with the results.', 'label': 6},\n",
        "        {'text': 'I feel happy and delighted.', 'label': 6},\n",
        "        {'text': 'This brings me happiness.', 'label': 6},\n",
        "        {'text': 'I am happy about the success.', 'label': 6},\n",
        "        {'text': 'I feel happy and cheerful.', 'label': 6},\n",
        "        {'text': 'This creates happiness in me.', 'label': 6},\n",
        "\n",
        "        # hopeful (12 samples)\n",
        "        {'text': 'I am hopeful for the future.', 'label': 7},\n",
        "        {'text': 'I feel hopeful about the outcome.', 'label': 7},\n",
        "        {'text': 'This makes me feel hopeful.', 'label': 7},\n",
        "        {'text': 'I am feeling hopeful today.', 'label': 7},\n",
        "        {'text': 'I feel hopeful and optimistic.', 'label': 7},\n",
        "        {'text': 'This gives me hope.', 'label': 7},\n",
        "        {'text': 'I am hopeful about the changes.', 'label': 7},\n",
        "        {'text': 'I feel hopeful and positive.', 'label': 7},\n",
        "        {'text': 'This brings me hope.', 'label': 7},\n",
        "        {'text': 'I am hopeful about the possibilities.', 'label': 7},\n",
        "        {'text': 'I feel hopeful and confident.', 'label': 7},\n",
        "        {'text': 'This creates hope in me.', 'label': 7},\n",
        "\n",
        "        # overwhelmed (12 samples)\n",
        "        {'text': 'I am feeling overwhelmed with tasks.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed by the workload.', 'label': 8},\n",
        "        {'text': 'This makes me feel overwhelmed.', 'label': 8},\n",
        "        {'text': 'I am feeling overwhelmed today.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed and stressed.', 'label': 8},\n",
        "        {'text': 'This gives me overwhelm.', 'label': 8},\n",
        "        {'text': 'I am overwhelmed by the situation.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed and exhausted.', 'label': 8},\n",
        "        {'text': 'This brings me overwhelm.', 'label': 8},\n",
        "        {'text': 'I am overwhelmed by the pressure.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed and drained.', 'label': 8},\n",
        "        {'text': 'This creates overwhelm in me.', 'label': 8},\n",
        "\n",
        "        # proud (12 samples)\n",
        "        {'text': 'I am proud of my accomplishments.', 'label': 9},\n",
        "        {'text': 'I feel proud of the results.', 'label': 9},\n",
        "        {'text': 'This makes me feel proud.', 'label': 9},\n",
        "        {'text': 'I am feeling proud today.', 'label': 9},\n",
        "        {'text': 'I feel proud and accomplished.', 'label': 9},\n",
        "        {'text': 'This gives me pride.', 'label': 9},\n",
        "        {'text': 'I am proud of the achievement.', 'label': 9},\n",
        "        {'text': 'I feel proud and satisfied.', 'label': 9},\n",
        "        {'text': 'This brings me pride.', 'label': 9},\n",
        "        {'text': 'I am proud of the success.', 'label': 9},\n",
        "        {'text': 'I feel proud and confident.', 'label': 9},\n",
        "        {'text': 'This creates pride in me.', 'label': 9},\n",
        "\n",
        "        # sad (12 samples)\n",
        "        {'text': 'I feel sad about the loss.', 'label': 10},\n",
        "        {'text': 'I am sad about the situation.', 'label': 10},\n",
        "        {'text': 'This makes me feel sad.', 'label': 10},\n",
        "        {'text': 'I am feeling sad today.', 'label': 10},\n",
        "        {'text': 'I feel sad and down.', 'label': 10},\n",
        "        {'text': 'This gives me sadness.', 'label': 10},\n",
        "        {'text': 'I am sad about the outcome.', 'label': 10},\n",
        "        {'text': 'I feel sad and depressed.', 'label': 10},\n",
        "        {'text': 'This brings me sadness.', 'label': 10},\n",
        "        {'text': 'I am sad about the news.', 'label': 10},\n",
        "        {'text': 'I feel sad and heartbroken.', 'label': 10},\n",
        "        {'text': 'This creates sadness in me.', 'label': 10},\n",
        "\n",
        "        # tired (12 samples)\n",
        "        {'text': 'I am tired from working all day.', 'label': 11},\n",
        "        {'text': 'I feel tired of the routine.', 'label': 11},\n",
        "        {'text': 'This makes me feel tired.', 'label': 11},\n",
        "        {'text': 'I am feeling tired today.', 'label': 11},\n",
        "        {'text': 'I feel tired and exhausted.', 'label': 11},\n",
        "        {'text': 'This gives me tiredness.', 'label': 11},\n",
        "        {'text': 'I am tired of the situation.', 'label': 11},\n",
        "        {'text': 'I feel tired and worn out.', 'label': 11},\n",
        "        {'text': 'This brings me tiredness.', 'label': 11},\n",
        "        {'text': 'I am tired of the stress.', 'label': 11},\n",
        "        {'text': 'I feel tired and fatigued.', 'label': 11},\n",
        "        {'text': 'This creates tiredness in me.', 'label': 11}\n",
        "    ]\n",
        "\n",
        "    print(f'‚úÖ Recreated balanced dataset with {len(balanced_data)} samples')\n",
        "\n",
        "\n",
        "# Check if tokenizer is defined, if not, recreate it\n",
        "if 'tokenizer' not in globals():\n",
        "    print('üîß RECREATING TOKENIZER')\n",
        "    print('=' * 40)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(specialized_model_name)\n",
        "    print('‚úÖ Tokenizer recreated successfully')\n",
        "\n",
        "\n",
        "train_data, val_data = train_test_split(balanced_data, test_size=0.2, random_state=42, stratify=[d['label'] for d in balanced_data])\n",
        "\n",
        "# Convert to datasets\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "# Calculate class weights based on the training dataset\n",
        "train_labels = [d['label'] for d in train_data]\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "\n",
        "# Convert class weights to a dictionary mapping class index to weight\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "# Convert class weights dictionary to a tensor\n",
        "class_weights_tensor = torch.tensor(list(class_weights_dict.values()), dtype=torch.float32)\n",
        "\n",
        "# Move class weights tensor to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights_tensor = class_weights_tensor.to(device)\n",
        "\n",
        "print(\"‚úÖ Calculated class weights:\")\n",
        "print(class_weights_dict)\n",
        "\n",
        "# Define compute_metrics function if not defined\n",
        "if 'compute_metrics' not in globals():\n",
        "    print('üìä DEFINING COMPUTE_METRICS FUNCTION')\n",
        "    print('=' * 40)\n",
        "    # Assuming emotions is defined now\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        # Check if emotions is defined before using it\n",
        "        target_names = emotions if 'emotions' in globals() else None\n",
        "        report = classification_report(labels, predictions, target_names=target_names, output_dict=True)\n",
        "\n",
        "        return {\n",
        "            'f1': report['weighted avg']['f1-score'],\n",
        "            'accuracy': report['accuracy'],\n",
        "            'precision': report['weighted avg']['precision'],\n",
        "            'recall': report['weighted avg']['recall']\n",
        "        }\n",
        "    print('‚úÖ compute_metrics function defined')\n",
        "\n",
        "\n",
        "# We need a custom trainer to pass class weights to the model's forward pass\n",
        "class WeightedLossTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Initialize the weighted loss trainer\n",
        "# Check if model, training_args, data_collator are defined, if not, recreate them\n",
        "if 'model' not in globals():\n",
        "    print('üîß RECREATING MODEL')\n",
        "    print('=' * 40)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        specialized_model_name,\n",
        "        num_labels=12,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    )\n",
        "    # Assuming 'emotions' is defined\n",
        "    if 'emotions' in globals():\n",
        "        model.config.id2label = {i: emotion for i, emotion in enumerate(emotions)}\n",
        "        model.config.label2id = {emotion: i for i, emotion in enumerate(emotions)}\n",
        "    else:\n",
        "         print(\"‚ö†Ô∏è WARNING: 'emotions' list not defined, cannot set id2label/label2id in model config.\")\n",
        "\n",
        "    print('‚úÖ Model recreated successfully')\n",
        "\n",
        "if 'training_args' not in globals():\n",
        "    print('‚öôÔ∏è  RECONFIGURING TRAINING ARGUMENTS')\n",
        "    print('=' * 40)\n",
        "    # Assuming best_run is defined and has hyperparameters\n",
        "    if 'best_run' in globals() and hasattr(best_run, 'hyperparameters'):\n",
        "        best_hyperparameters = best_run.hyperparameters\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir='./corrected_emotion_model',\n",
        "            learning_rate=best_hyperparameters.get('learning_rate', 2e-5), # Use default if not found\n",
        "            per_device_train_batch_size=best_hyperparameters.get('per_device_train_batch_size', 16),\n",
        "            per_device_eval_batch_size=best_hyperparameters.get('per_device_eval_batch_size', 16),\n",
        "            num_train_epochs=best_hyperparameters.get('num_train_epochs', 5),\n",
        "            weight_decay=best_hyperparameters.get('weight_decay', 0.01),\n",
        "            logging_dir='./logs',\n",
        "            logging_steps=10,\n",
        "            eval_strategy=\"steps\",\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=50,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model='f1',\n",
        "            warmup_steps=100,\n",
        "            dataloader_num_workers=0,\n",
        "            save_total_limit=3\n",
        "        )\n",
        "        print('‚úÖ Training arguments reconfigured with best_run hyperparameters')\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è WARNING: 'best_run' not defined or has no hyperparameters, using default TrainingArguments.\")\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir='./corrected_emotion_model',\n",
        "            learning_rate=2e-5,\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=16,\n",
        "            num_train_epochs=5,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir='./logs',\n",
        "            logging_steps=10,\n",
        "            eval_strategy=\"steps\",\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=50,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model='f1',\n",
        "            warmup_steps=100,\n",
        "            dataloader_num_workers=0,\n",
        "            save_total_limit=3\n",
        "        )\n",
        "        print('‚úÖ Default Training arguments reconfigured')\n",
        "\n",
        "\n",
        "if 'data_collator' not in globals():\n",
        "    print('üì¶ RECREATING DATA COLLATOR')\n",
        "    print('=' * 40)\n",
        "    # Assuming tokenizer is defined\n",
        "    if 'tokenizer' in globals():\n",
        "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "        print('‚úÖ Data collator recreated successfully')\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è WARNING: 'tokenizer' not defined, cannot recreate data collator.\")\n",
        "\n",
        "\n",
        "final_trainer = WeightedLossTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print('‚úÖ WeightedLossTrainer initialized successfully with class weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c173aa8"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `specialized_model_name` was not defined. This variable holds the name of the pre-trained model and is necessary for initializing the tokenizer and the model. I need to re-define `specialized_model_name` before attempting to load the tokenizer and model. I will add a check to see if `specialized_model_name` is defined, and if not, define it with its intended value. Then I will re-run the code to recreate the necessary objects, calculate class weights, and initialize the weighted trainer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY_YWVKLLQW9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import Trainer, AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, DataCollatorWithPadding # Import all necessary transformers classes again\n",
        "from datasets import Dataset # Re-import Dataset\n",
        "\n",
        "# Check if specialized_model_name is defined, if not, define it\n",
        "if 'specialized_model_name' not in globals():\n",
        "    specialized_model_name = 'j-hartmann/emotion-english-distilroberta-base'\n",
        "    print(f'üîß Defined specialized_model_name: {specialized_model_name}')\n",
        "\n",
        "\n",
        "# Recreate train_data and val_data\n",
        "# Check if balanced_data is defined, if not, recreate it\n",
        "if 'balanced_data' not in globals():\n",
        "     # Create balanced training dataset\n",
        "    print('üìä RECREATING BALANCED DATASET')\n",
        "    print('=' * 40)\n",
        "\n",
        "    balanced_data = [\n",
        "        # anxious (12 samples)\n",
        "        {'text': 'I feel anxious about the presentation.', 'label': 0},\n",
        "        {'text': 'I am anxious about the future.', 'label': 0},\n",
        "        {'text': 'This makes me feel anxious.', 'label': 0},\n",
        "        {'text': 'I am feeling anxious today.', 'label': 0},\n",
        "        {'text': 'The uncertainty makes me anxious.', 'label': 0},\n",
        "        {'text': 'I feel anxious about the results.', 'label': 0},\n",
        "        {'text': 'This situation is making me anxious.', 'label': 0},\n",
        "        {'text': 'I am anxious about the meeting.', 'label': 0},\n",
        "        {'text': 'The pressure is making me anxious.', 'label': 0},\n",
        "        {'text': 'I feel anxious about the decision.', 'label': 0},\n",
        "        {'text': 'This is causing me anxiety.', 'label': 0},\n",
        "        {'text': 'I am anxious about the changes.', 'label': 0},\n",
        "\n",
        "        # calm (12 samples)\n",
        "        {'text': 'I feel calm and peaceful.', 'label': 1},\n",
        "        {'text': 'I am feeling calm today.', 'label': 1},\n",
        "        {'text': 'This makes me feel calm.', 'label': 1},\n",
        "        {'text': 'I am calm about the situation.', 'label': 1},\n",
        "        {'text': 'I feel calm and relaxed.', 'label': 1},\n",
        "        {'text': 'This gives me a sense of calm.', 'label': 1},\n",
        "        {'text': 'I am feeling calm and centered.', 'label': 1},\n",
        "        {'text': 'This brings me calm.', 'label': 1},\n",
        "        {'text': 'I feel calm and at peace.', 'label': 1},\n",
        "        {'text': 'I am calm about the outcome.', 'label': 1},\n",
        "        {'text': 'This creates a feeling of calm.', 'label': 1},\n",
        "        {'text': 'I feel calm and collected.', 'label': 1},\n",
        "\n",
        "        # content (12 samples)\n",
        "        {'text': 'I feel content with my life.', 'label': 2},\n",
        "        {'text': 'I am content with the results.', 'label': 2},\n",
        "        {'text': 'This makes me feel content.', 'label': 2},\n",
        "        {'text': 'I am feeling content today.', 'label': 2},\n",
        "        {'text': 'I feel content and satisfied.', 'label': 2},\n",
        "        {'text': 'This gives me contentment.', 'label': 2},\n",
        "        {'text': 'I am content with my choices.', 'label': 2},\n",
        "        {'text': 'I feel content and fulfilled.', 'label': 2},\n",
        "        {'text': 'This brings me contentment.', 'label': 2},\n",
        "        {'text': 'I am content with the situation.', 'label': 2},\n",
        "        {'text': 'I feel content and at ease.', 'label': 2},\n",
        "        {'text': 'This creates contentment in me.', 'label': 2},\n",
        "\n",
        "        # excited (12 samples)\n",
        "        {'text': 'I am excited about the new opportunity.', 'label': 3},\n",
        "        {'text': 'I feel excited about the future.', 'label': 3},\n",
        "        {'text': 'This makes me feel excited.', 'label': 3},\n",
        "        {'text': 'I am feeling excited today.', 'label': 3},\n",
        "        {'text': 'I feel excited and enthusiastic.', 'label': 3},\n",
        "        {'text': 'This gives me excitement.', 'label': 3},\n",
        "        {'text': 'I am excited about the project.', 'label': 3},\n",
        "        {'text': 'I feel excited and motivated.', 'label': 3},\n",
        "        {'text': 'This brings me excitement.', 'label': 3},\n",
        "        {'text': 'I am excited about the possibilities.', 'label': 3},\n",
        "        {'text': 'I feel excited and energized.', 'label': 3},\n",
        "        {'text': 'This creates excitement in me.', 'label': 3},\n",
        "\n",
        "        # frustrated (12 samples)\n",
        "        {'text': 'I am so frustrated with this project.', 'label': 4},\n",
        "        {'text': 'I feel frustrated about the situation.', 'label': 4},\n",
        "        {'text': 'This makes me feel frustrated.', 'label': 4},\n",
        "        {'text': 'I am feeling frustrated today.', 'label': 4},\n",
        "        {'text': 'I feel frustrated and annoyed.', 'label': 4},\n",
        "        {'text': 'This gives me frustration.', 'label': 4},\n",
        "        {'text': 'I am frustrated with the results.', 'label': 4},\n",
        "        {'text': 'I feel frustrated and irritated.', 'label': 4},\n",
        "        {'text': 'This brings me frustration.', 'label': 4},\n",
        "        {'text': 'I am frustrated with the process.', 'label': 4},\n",
        "        {'text': 'I feel frustrated and upset.', 'label': 4},\n",
        "        {'text': 'This creates frustration in me.', 'label': 4},\n",
        "\n",
        "        # grateful (12 samples)\n",
        "        {'text': 'I am grateful for all the support.', 'label': 5},\n",
        "        {'text': 'I feel grateful for the opportunity.', 'label': 5},\n",
        "        {'text': 'This makes me feel grateful.', 'label': 5},\n",
        "        {'text': 'I am feeling grateful today.', 'label': 5},\n",
        "        {'text': 'I feel grateful and thankful.', 'label': 5},\n",
        "        {'text': 'This gives me gratitude.', 'label': 5},\n",
        "        {'text': 'I am grateful for the help.', 'label': 5},\n",
        "        {'text': 'I feel grateful and appreciative.', 'label': 5},\n",
        "        {'text': 'This brings me gratitude.', 'label': 5},\n",
        "        {'text': 'I am grateful for the kindness.', 'label': 5},\n",
        "        {'text': 'I feel grateful and blessed.', 'label': 5},\n",
        "        {'text': 'This creates gratitude in me.', 'label': 5},\n",
        "\n",
        "        # happy (12 samples)\n",
        "        {'text': 'I am feeling really happy today!', 'label': 6},\n",
        "        {'text': 'I feel happy about the news.', 'label': 6},\n",
        "        {'text': 'This makes me feel happy.', 'label': 6},\n",
        "        {'text': 'I am feeling happy today.', 'label': 6},\n",
        "        {'text': 'I feel happy and joyful.', 'label': 6},\n",
        "        {'text': 'This gives me happiness.', 'label': 6},\n",
        "        {'text': 'I am happy with the results.', 'label': 6},\n",
        "        {'text': 'I feel happy and delighted.', 'label': 6},\n",
        "        {'text': 'This brings me happiness.', 'label': 6},\n",
        "        {'text': 'I am happy about the success.', 'label': 6},\n",
        "        {'text': 'I feel happy and cheerful.', 'label': 6},\n",
        "        {'text': 'This creates happiness in me.', 'label': 6},\n",
        "\n",
        "        # hopeful (12 samples)\n",
        "        {'text': 'I am hopeful for the future.', 'label': 7},\n",
        "        {'text': 'I feel hopeful about the outcome.', 'label': 7},\n",
        "        {'text': 'This makes me feel hopeful.', 'label': 7},\n",
        "        {'text': 'I am feeling hopeful today.', 'label': 7},\n",
        "        {'text': 'I feel hopeful and optimistic.', 'label': 7},\n",
        "        {'text': 'This gives me hope.', 'label': 7},\n",
        "        {'text': 'I am hopeful about the changes.', 'label': 7},\n",
        "        {'text': 'I feel hopeful and positive.', 'label': 7},\n",
        "        {'text': 'This brings me hope.', 'label': 7},\n",
        "        {'text': 'I am hopeful about the possibilities.', 'label': 7},\n",
        "        {'text': 'I feel hopeful and confident.', 'label': 7},\n",
        "        {'text': 'This creates hope in me.', 'label': 7},\n",
        "\n",
        "        # overwhelmed (12 samples)\n",
        "        {'text': 'I am feeling overwhelmed with tasks.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed by the workload.', 'label': 8},\n",
        "        {'text': 'This makes me feel overwhelmed.', 'label': 8},\n",
        "        {'text': 'I am feeling overwhelmed today.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed and stressed.', 'label': 8},\n",
        "        {'text': 'This gives me overwhelm.', 'label': 8},\n",
        "        {'text': 'I am overwhelmed by the situation.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed and exhausted.', 'label': 8},\n",
        "        {'text': 'This brings me overwhelm.', 'label': 8},\n",
        "        {'text': 'I am overwhelmed by the pressure.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed and drained.', 'label': 8},\n",
        "        {'text': 'This creates overwhelm in me.', 'label': 8},\n",
        "\n",
        "        # proud (12 samples)\n",
        "        {'text': 'I am proud of my accomplishments.', 'label': 9},\n",
        "        {'text': 'I feel proud of the results.', 'label': 9},\n",
        "        {'text': 'This makes me feel proud.', 'label': 9},\n",
        "        {'text': 'I am feeling proud today.', 'label': 9},\n",
        "        {'text': 'I feel proud and accomplished.', 'label': 9},\n",
        "        {'text': 'This gives me pride.', 'label': 9},\n",
        "        {'text': 'I am proud of the achievement.', 'label': 9},\n",
        "        {'text': 'I feel proud and satisfied.', 'label': 9},\n",
        "        {'text': 'This brings me pride.', 'label': 9},\n",
        "        {'text': 'I am proud of the success.', 'label': 9},\n",
        "        {'text': 'I feel proud and confident.', 'label': 9},\n",
        "        {'text': 'This creates pride in me.', 'label': 9},\n",
        "\n",
        "        # sad (12 samples)\n",
        "        {'text': 'I feel sad about the loss.', 'label': 10},\n",
        "        {'text': 'I am sad about the situation.', 'label': 10},\n",
        "        {'text': 'This makes me feel sad.', 'label': 10},\n",
        "        {'text': 'I am feeling sad today.', 'label': 10},\n",
        "        {'text': 'I feel sad and down.', 'label': 10},\n",
        "        {'text': 'This gives me sadness.', 'label': 10},\n",
        "        {'text': 'I am sad about the outcome.', 'label': 10},\n",
        "        {'text': 'I feel sad and depressed.', 'label': 10},\n",
        "        {'text': 'This brings me sadness.', 'label': 10},\n",
        "        {'text': 'I am sad about the news.', 'label': 10},\n",
        "        {'text': 'I feel sad and heartbroken.', 'label': 10},\n",
        "        {'text': 'This creates sadness in me.', 'label': 10},\n",
        "\n",
        "        # tired (12 samples)\n",
        "        {'text': 'I am tired from working all day.', 'label': 11},\n",
        "        {'text': 'I feel tired of the routine.', 'label': 11},\n",
        "        {'text': 'This makes me feel tired.', 'label': 11},\n",
        "        {'text': 'I am feeling tired today.', 'label': 11},\n",
        "        {'text': 'I feel tired and exhausted.', 'label': 11},\n",
        "        {'text': 'This gives me tiredness.', 'label': 11},\n",
        "        {'text': 'I am tired of the situation.', 'label': 11},\n",
        "        {'text': 'I feel tired and worn out.', 'label': 11},\n",
        "        {'text': 'This brings me tiredness.', 'label': 11},\n",
        "        {'text': 'I am tired of the stress.', 'label': 11},\n",
        "        {'text': 'I feel tired and fatigued.', 'label': 11},\n",
        "        {'text': 'This creates tiredness in me.', 'label': 11}\n",
        "    ]\n",
        "\n",
        "    print(f'‚úÖ Recreated balanced dataset with {len(balanced_data)} samples')\n",
        "\n",
        "\n",
        "# Check if tokenizer is defined, if not, recreate it\n",
        "if 'tokenizer' not in globals():\n",
        "    print('üîß RECREATING TOKENIZER')\n",
        "    print('=' * 40)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(specialized_model_name)\n",
        "    print('‚úÖ Tokenizer recreated successfully')\n",
        "\n",
        "\n",
        "train_data, val_data = train_test_split(balanced_data, test_size=0.2, random_state=42, stratify=[d['label'] for d in balanced_data])\n",
        "\n",
        "# Convert to datasets\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "# Calculate class weights based on the training dataset\n",
        "train_labels = [d['label'] for d in train_data]\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "\n",
        "# Convert class weights to a dictionary mapping class index to weight\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "# Convert class weights dictionary to a tensor\n",
        "class_weights_tensor = torch.tensor(list(class_weights_dict.values()), dtype=torch.float32)\n",
        "\n",
        "# Move class weights tensor to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights_tensor = class_weights_tensor.to(device)\n",
        "\n",
        "print(\"‚úÖ Calculated class weights:\")\n",
        "print(class_weights_dict)\n",
        "\n",
        "# We need a custom trainer to pass class weights to the model's forward pass\n",
        "class WeightedLossTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Initialize the weighted loss trainer\n",
        "# Check if model, training_args, data_collator are defined, if not, recreate them\n",
        "if 'model' not in globals():\n",
        "    print('üîß RECREATING MODEL')\n",
        "    print('=' * 40)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        specialized_model_name,\n",
        "        num_labels=12,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    )\n",
        "    # Assuming 'emotions' is defined\n",
        "    if 'emotions' in globals():\n",
        "        model.config.id2label = {i: emotion for i, emotion in enumerate(emotions)}\n",
        "        model.config.label2id = {emotion: i for i, emotion in enumerate(emotions)}\n",
        "    else:\n",
        "         print(\"‚ö†Ô∏è WARNING: 'emotions' list not defined, cannot set id2label/label2id in model config.\")\n",
        "\n",
        "    print('‚úÖ Model recreated successfully')\n",
        "\n",
        "if 'training_args' not in globals():\n",
        "    print('‚öôÔ∏è  RECONFIGURING TRAINING ARGUMENTS')\n",
        "    print('=' * 40)\n",
        "    # Assuming best_run is defined and has hyperparameters\n",
        "    if 'best_run' in globals() and hasattr(best_run, 'hyperparameters'):\n",
        "        best_hyperparameters = best_run.hyperparameters\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir='./corrected_emotion_model',\n",
        "            learning_rate=best_hyperparameters.get('learning_rate', 2e-5), # Use default if not found\n",
        "            per_device_train_batch_size=best_hyperparameters.get('per_device_train_batch_size', 16),\n",
        "            per_device_eval_batch_size=best_hyperparameters.get('per_device_eval_batch_size', 16),\n",
        "            num_train_epochs=best_hyperparameters.get('num_train_epochs', 5),\n",
        "            weight_decay=best_hyperparameters.get('weight_decay', 0.01),\n",
        "            logging_dir='./logs',\n",
        "            logging_steps=10,\n",
        "            eval_strategy=\"steps\",\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=50,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model='f1',\n",
        "            warmup_steps=100,\n",
        "            dataloader_num_workers=0,\n",
        "            save_total_limit=3\n",
        "        )\n",
        "        print('‚úÖ Training arguments reconfigured with best_run hyperparameters')\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è WARNING: 'best_run' not defined or has no hyperparameters, using default TrainingArguments.\")\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir='./corrected_emotion_model',\n",
        "            learning_rate=2e-5,\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=16,\n",
        "            num_train_epochs=5,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir='./logs',\n",
        "            logging_steps=10,\n",
        "            eval_strategy=\"steps\",\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=50,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model='f1',\n",
        "            warmup_steps=100,\n",
        "            dataloader_num_workers=0,\n",
        "            save_total_limit=3\n",
        "        )\n",
        "        print('‚úÖ Default Training arguments reconfigured')\n",
        "\n",
        "\n",
        "if 'data_collator' not in globals():\n",
        "    print('üì¶ RECREATING DATA COLLATOR')\n",
        "    print('=' * 40)\n",
        "    # Assuming tokenizer is defined\n",
        "    if 'tokenizer' in globals():\n",
        "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "        print('‚úÖ Data collator recreated successfully')\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è WARNING: 'tokenizer' not defined, cannot recreate data collator.\")\n",
        "\n",
        "\n",
        "# Check if compute_metrics is defined, if not, recreate it\n",
        "if 'compute_metrics' not in globals():\n",
        "    print('üìä RECREATING COMPUTE_METRICS FUNCTION')\n",
        "    print('=' * 40)\n",
        "    # Assuming emotions is defined\n",
        "    if 'emotions' in globals():\n",
        "        def compute_metrics(eval_pred):\n",
        "            predictions, labels = eval_pred\n",
        "            predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "            # Calculate metrics\n",
        "            report = classification_report(labels, predictions, target_names=emotions, output_dict=True)\n",
        "\n",
        "            return {\n",
        "                'f1': report['weighted avg']['f1-score'],\n",
        "                'accuracy': report['accuracy'],\n",
        "                'precision': report['weighted avg']['precision'],\n",
        "                'recall': report['weighted avg']['recall']\n",
        "            }\n",
        "        print('‚úÖ compute_metrics function recreated successfully')\n",
        "    else:\n",
        "         print(\"‚ö†Ô∏è WARNING: 'emotions' list not defined, cannot recreate compute_metrics function.\")\n",
        "\n",
        "\n",
        "final_trainer = WeightedLossTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print('‚úÖ WeightedLossTrainer initialized successfully with class weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a9c1e41"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `compute_metrics` was not defined when initializing the `WeightedLossTrainer`. Although I included code to recreate it, the condition `'emotions' in globals()` was false, preventing its re-definition. I need to ensure that the `emotions` list is defined before attempting to recreate `compute_metrics`. I will add a check for `emotions` and recreate it if necessary, then ensure `compute_metrics` is properly defined before initializing the trainer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzC2ZY_pLam3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import Trainer, AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, DataCollatorWithPadding # Import all necessary transformers classes again\n",
        "from datasets import Dataset # Re-import Dataset\n",
        "from sklearn.metrics import classification_report # Import classification_report\n",
        "\n",
        "# Check if specialized_model_name is defined, if not, define it\n",
        "if 'specialized_model_name' not in globals():\n",
        "    specialized_model_name = 'j-hartmann/emotion-english-distilroberta-base'\n",
        "    print(f'üîß Defined specialized_model_name: {specialized_model_name}')\n",
        "\n",
        "# Check if emotions is defined, if not, define it\n",
        "if 'emotions' not in globals():\n",
        "    emotions = ['anxious', 'calm', 'content', 'excited', 'frustrated', 'grateful', 'happy', 'hopeful', 'overwhelmed', 'proud', 'sad', 'tired']\n",
        "    print(f'üéØ Defined emotions list: {emotions}')\n",
        "\n",
        "\n",
        "# Recreate train_data and val_data\n",
        "# Check if balanced_data is defined, if not, recreate it\n",
        "if 'balanced_data' not in globals():\n",
        "     # Create balanced training dataset\n",
        "    print('üìä RECREATING BALANCED DATASET')\n",
        "    print('=' * 40)\n",
        "\n",
        "    balanced_data = [\n",
        "        # anxious (12 samples)\n",
        "        {'text': 'I feel anxious about the presentation.', 'label': 0},\n",
        "        {'text': 'I am anxious about the future.', 'label': 0},\n",
        "        {'text': 'This makes me feel anxious.', 'label': 0},\n",
        "        {'text': 'I am feeling anxious today.', 'label': 0},\n",
        "        {'text': 'The uncertainty makes me anxious.', 'label': 0},\n",
        "        {'text': 'I feel anxious about the results.', 'label': 0},\n",
        "        {'text': 'This situation is making me anxious.', 'label': 0},\n",
        "        {'text': 'I am anxious about the meeting.', 'label': 0},\n",
        "        {'text': 'The pressure is making me anxious.', 'label': 0},\n",
        "        {'text': 'I feel anxious about the decision.', 'label': 0},\n",
        "        {'text': 'This is causing me anxiety.', 'label': 0},\n",
        "        {'text': 'I am anxious about the changes.', 'label': 0},\n",
        "\n",
        "        # calm (12 samples)\n",
        "        {'text': 'I feel calm and peaceful.', 'label': 1},\n",
        "        {'text': 'I am feeling calm today.', 'label': 1},\n",
        "        {'text': 'This makes me feel calm.', 'label': 1},\n",
        "        {'text': 'I am calm about the situation.', 'label': 1},\n",
        "        {'text': 'I feel calm and relaxed.', 'label': 1},\n",
        "        {'text': 'This gives me a sense of calm.', 'label': 1},\n",
        "        {'text': 'I am feeling calm and centered.', 'label': 1},\n",
        "        {'text': 'This brings me calm.', 'label': 1},\n",
        "        {'text': 'I feel calm and at peace.', 'label': 1},\n",
        "        {'text': 'I am calm about the outcome.', 'label': 1},\n",
        "        {'text': 'This creates a feeling of calm.', 'label': 1},\n",
        "        {'text': 'I feel calm and collected.', 'label': 1},\n",
        "\n",
        "        # content (12 samples)\n",
        "        {'text': 'I feel content with my life.', 'label': 2},\n",
        "        {'text': 'I am content with the results.', 'label': 2},\n",
        "        {'text': 'This makes me feel content.', 'label': 2},\n",
        "        {'text': 'I am feeling content today.', 'label': 2},\n",
        "        {'text': 'I feel content and satisfied.', 'label': 2},\n",
        "        {'text': 'This gives me contentment.', 'label': 2},\n",
        "        {'text': 'I am content with my choices.', 'label': 2},\n",
        "        {'text': 'I feel content and fulfilled.', 'label': 2},\n",
        "        {'text': 'This brings me contentment.', 'label': 2},\n",
        "        {'text': 'I am content with the situation.', 'label': 2},\n",
        "        {'text': 'I feel content and at ease.', 'label': 2},\n",
        "        {'text': 'This creates contentment in me.', 'label': 2},\n",
        "\n",
        "        # excited (12 samples)\n",
        "        {'text': 'I am excited about the new opportunity.', 'label': 3},\n",
        "        {'text': 'I feel excited about the future.', 'label': 3},\n",
        "        {'text': 'This makes me feel excited.', 'label': 3},\n",
        "        {'text': 'I am feeling excited today.', 'label': 3},\n",
        "        {'text': 'I feel excited and enthusiastic.', 'label': 3},\n",
        "        {'text': 'This gives me excitement.', 'label': 3},\n",
        "        {'text': 'I am excited about the project.', 'label': 3},\n",
        "        {'text': 'I feel excited and motivated.', 'label': 3},\n",
        "        {'text': 'This brings me excitement.', 'label': 3},\n",
        "        {'text': 'I am excited about the possibilities.', 'label': 3},\n",
        "        {'text': 'I feel excited and energized.', 'label': 3},\n",
        "        {'text': 'This creates excitement in me.', 'label': 3},\n",
        "\n",
        "        # frustrated (12 samples)\n",
        "        {'text': 'I am so frustrated with this project.', 'label': 4},\n",
        "        {'text': 'I feel frustrated about the situation.', 'label': 4},\n",
        "        {'text': 'This makes me feel frustrated.', 'label': 4},\n",
        "        {'text': 'I am feeling frustrated today.', 'label': 4},\n",
        "        {'text': 'I feel frustrated and annoyed.', 'label': 4},\n",
        "        {'text': 'This gives me frustration.', 'label': 4},\n",
        "        {'text': 'I am frustrated with the results.', 'label': 4},\n",
        "        {'text': 'I feel frustrated and irritated.', 'label': 4},\n",
        "        {'text': 'This brings me frustration.', 'label': 4},\n",
        "        {'text': 'I am frustrated with the process.', 'label': 4},\n",
        "        {'text': 'I feel frustrated and upset.', 'label': 4},\n",
        "        {'text': 'This creates frustration in me.', 'label': 4},\n",
        "\n",
        "        # grateful (12 samples)\n",
        "        {'text': 'I am grateful for all the support.', 'label': 5},\n",
        "        {'text': 'I feel grateful for the opportunity.', 'label': 5},\n",
        "        {'text': 'This makes me feel grateful.', 'label': 5},\n",
        "        {'text': 'I am feeling grateful today.', 'label': 5},\n",
        "        {'text': 'I feel grateful and thankful.', 'label': 5},\n",
        "        {'text': 'This gives me gratitude.', 'label': 5},\n",
        "        {'text': 'I am grateful for the help.', 'label': 5},\n",
        "        {'text': 'I feel grateful and appreciative.', 'label': 5},\n",
        "        {'text': 'This brings me gratitude.', 'label': 5},\n",
        "        {'text': 'I am grateful for the kindness.', 'label': 5},\n",
        "        {'text': 'I feel grateful and blessed.', 'label': 5},\n",
        "        {'text': 'This creates gratitude in me.', 'label': 5},\n",
        "\n",
        "        # happy (12 samples)\n",
        "        {'text': 'I am feeling really happy today!', 'label': 6},\n",
        "        {'text': 'I feel happy about the news.', 'label': 6},\n",
        "        {'text': 'This makes me feel happy.', 'label': 6},\n",
        "        {'text': 'I am feeling happy today.', 'label': 6},\n",
        "        {'text': 'I feel happy and joyful.', 'label': 6},\n",
        "        {'text': 'This gives me happiness.', 'label': 6},\n",
        "        {'text': 'I am happy with the results.', 'label': 6},\n",
        "        {'text': 'I feel happy and delighted.', 'label': 6},\n",
        "        {'text': 'This brings me happiness.', 'label': 6},\n",
        "        {'text': 'I am happy about the success.', 'label': 6},\n",
        "        {'text': 'I feel happy and cheerful.', 'label': 6},\n",
        "        {'text': 'This creates happiness in me.', 'label': 6},\n",
        "\n",
        "        # hopeful (12 samples)\n",
        "        {'text': 'I am hopeful for the future.', 'label': 7},\n",
        "        {'text': 'I feel hopeful about the outcome.', 'label': 7},\n",
        "        {'text': 'This makes me feel hopeful.', 'label': 7},\n",
        "        {'text': 'I am feeling hopeful today.', 'label': 7},\n",
        "        {'text': 'I feel hopeful and optimistic.', 'label': 7},\n",
        "        {'text': 'This gives me hope.', 'label': 7},\n",
        "        {'text': 'I am hopeful about the changes.', 'label': 7},\n",
        "        {'text': 'I feel hopeful and positive.', 'label': 7},\n",
        "        {'text': 'This brings me hope.', 'label': 7},\n",
        "        {'text': 'I am hopeful about the possibilities.', 'label': 7},\n",
        "        {'text': 'I feel hopeful and confident.', 'label': 7},\n",
        "        {'text': 'This creates hope in me.', 'label': 7},\n",
        "\n",
        "        # overwhelmed (12 samples)\n",
        "        {'text': 'I am feeling overwhelmed with tasks.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed by the workload.', 'label': 8},\n",
        "        {'text': 'This makes me feel overwhelmed.', 'label': 8},\n",
        "        {'text': 'I am feeling overwhelmed today.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed and stressed.', 'label': 8},\n",
        "        {'text': 'This gives me overwhelm.', 'label': 8},\n",
        "        {'text': 'I am overwhelmed by the situation.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed and exhausted.', 'label': 8},\n",
        "        {'text': 'This brings me overwhelm.', 'label': 8},\n",
        "        {'text': 'I am overwhelmed by the pressure.', 'label': 8},\n",
        "        {'text': 'I feel overwhelmed and drained.', 'label': 8},\n",
        "        {'text': 'This creates overwhelm in me.', 'label': 8},\n",
        "\n",
        "        # proud (12 samples)\n",
        "        {'text': 'I am proud of my accomplishments.', 'label': 9},\n",
        "        {'text': 'I feel proud of the results.', 'label': 9},\n",
        "        {'text': 'This makes me feel proud.', 'label': 9},\n",
        "        {'text': 'I am feeling proud today.', 'label': 9},\n",
        "        {'text': 'I feel proud and accomplished.', 'label': 9},\n",
        "        {'text': 'This gives me pride.', 'label': 9},\n",
        "        {'text': 'I am proud of the achievement.', 'label': 9},\n",
        "        {'text': 'I feel proud and satisfied.', 'label': 9},\n",
        "        {'text': 'This brings me pride.', 'label': 9},\n",
        "        {'text': 'I am proud of the success.', 'label': 9},\n",
        "        {'text': 'I feel proud and confident.', 'label': 9},\n",
        "        {'text': 'This creates pride in me.', 'label': 9},\n",
        "\n",
        "        # sad (12 samples)\n",
        "        {'text': 'I feel sad about the loss.', 'label': 10},\n",
        "        {'text': 'I am sad about the situation.', 'label': 10},\n",
        "        {'text': 'This makes me feel sad.', 'label': 10},\n",
        "        {'text': 'I am feeling sad today.', 'label': 10},\n",
        "        {'text': 'I feel sad and down.', 'label': 10},\n",
        "        {'text': 'This gives me sadness.', 'label': 10},\n",
        "        {'text': 'I am sad about the outcome.', 'label': 10},\n",
        "        {'text': 'I feel sad and depressed.', 'label': 10},\n",
        "        {'text': 'This brings me sadness.', 'label': 10},\n",
        "        {'text': 'I am sad about the news.', 'label': 10},\n",
        "        {'text': 'I feel sad and heartbroken.', 'label': 10},\n",
        "        {'text': 'This creates sadness in me.', 'label': 10},\n",
        "\n",
        "        # tired (12 samples)\n",
        "        {'text': 'I am tired from working all day.', 'label': 11},\n",
        "        {'text': 'I feel tired of the routine.', 'label': 11},\n",
        "        {'text': 'This makes me feel tired.', 'label': 11},\n",
        "        {'text': 'I am feeling tired today.', 'label': 11},\n",
        "        {'text': 'I feel tired and exhausted.', 'label': 11},\n",
        "        {'text': 'This gives me tiredness.', 'label': 11},\n",
        "        {'text': 'I am tired of the situation.', 'label': 11},\n",
        "        {'text': 'I feel tired and worn out.', 'label': 11},\n",
        "        {'text': 'This brings me tiredness.', 'label': 11},\n",
        "        {'text': 'I am tired of the stress.', 'label': 11},\n",
        "        {'text': 'I feel tired and fatigued.', 'label': 11},\n",
        "        {'text': 'This creates tiredness in me.', 'label': 11}\n",
        "    ]\n",
        "\n",
        "    print(f'‚úÖ Recreated balanced dataset with {len(balanced_data)} samples')\n",
        "\n",
        "\n",
        "# Check if tokenizer is defined, if not, recreate it\n",
        "if 'tokenizer' not in globals():\n",
        "    print('üîß RECREATING TOKENIZER')\n",
        "    print('=' * 40)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(specialized_model_name)\n",
        "    print('‚úÖ Tokenizer recreated successfully')\n",
        "\n",
        "\n",
        "train_data, val_data = train_test_split(balanced_data, test_size=0.2, random_state=42, stratify=[d['label'] for d in balanced_data])\n",
        "\n",
        "# Convert to datasets\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "# Calculate class weights based on the training dataset\n",
        "train_labels = [d['label'] for d in train_data]\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "\n",
        "# Convert class weights to a dictionary mapping class index to weight\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "# Convert class weights dictionary to a tensor\n",
        "class_weights_tensor = torch.tensor(list(class_weights_dict.values()), dtype=torch.float32)\n",
        "\n",
        "# Move class weights tensor to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights_tensor = class_weights_tensor.to(device)\n",
        "\n",
        "print(\"‚úÖ Calculated class weights:\")\n",
        "print(class_weights_dict)\n",
        "\n",
        "# Define compute_metrics function if not defined\n",
        "if 'compute_metrics' not in globals():\n",
        "    print('üìä DEFINING COMPUTE_METRICS FUNCTION')\n",
        "    print('=' * 40)\n",
        "    # Assuming emotions is defined now\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        # Check if emotions is defined before using it\n",
        "        target_names = emotions if 'emotions' in globals() else None\n",
        "        report = classification_report(labels, predictions, target_names=target_names, output_dict=True)\n",
        "\n",
        "        return {\n",
        "            'f1': report['weighted avg']['f1-score'],\n",
        "            'accuracy': report['accuracy'],\n",
        "            'precision': report['weighted avg']['precision'],\n",
        "            'recall': report['weighted avg']['recall']\n",
        "        }\n",
        "    print('‚úÖ compute_metrics function defined')\n",
        "\n",
        "\n",
        "# We need a custom trainer to pass class weights to the model's forward pass\n",
        "class WeightedLossTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Initialize the weighted loss trainer\n",
        "# Check if model, training_args, data_collator are defined, if not, recreate them\n",
        "if 'model' not in globals():\n",
        "    print('üîß RECREATING MODEL')\n",
        "    print('=' * 40)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        specialized_model_name,\n",
        "        num_labels=12,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    )\n",
        "    # Assuming 'emotions' is defined\n",
        "    if 'emotions' in globals():\n",
        "        model.config.id2label = {i: emotion for i, emotion in enumerate(emotions)}\n",
        "        model.config.label2id = {emotion: i for i, emotion in enumerate(emotions)}\n",
        "    else:\n",
        "         print(\"‚ö†Ô∏è WARNING: 'emotions' list not defined, cannot set id2label/label2id in model config.\")\n",
        "\n",
        "    print('‚úÖ Model recreated successfully')\n",
        "\n",
        "if 'training_args' not in globals():\n",
        "    print('‚öôÔ∏è  RECONFIGURING TRAINING ARGUMENTS')\n",
        "    print('=' * 40)\n",
        "    # Assuming best_run is defined and has hyperparameters\n",
        "    if 'best_run' in globals() and hasattr(best_run, 'hyperparameters'):\n",
        "        best_hyperparameters = best_run.hyperparameters\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir='./corrected_emotion_model',\n",
        "            learning_rate=best_hyperparameters.get('learning_rate', 2e-5), # Use default if not found\n",
        "            per_device_train_batch_size=best_hyperparameters.get('per_device_train_batch_size', 16),\n",
        "            per_device_eval_batch_size=best_hyperparameters.get('per_device_eval_batch_size', 16),\n",
        "            num_train_epochs=best_hyperparameters.get('num_train_epochs', 5),\n",
        "            weight_decay=best_hyperparameters.get('weight_decay', 0.01),\n",
        "            logging_dir='./logs',\n",
        "            logging_steps=10,\n",
        "            eval_strategy=\"steps\",\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=50,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model='f1',\n",
        "            warmup_steps=100,\n",
        "            dataloader_num_workers=0,\n",
        "            save_total_limit=3\n",
        "        )\n",
        "        print('‚úÖ Training arguments reconfigured with best_run hyperparameters')\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è WARNING: 'best_run' not defined or has no hyperparameters, using default TrainingArguments.\")\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir='./corrected_emotion_model',\n",
        "            learning_rate=2e-5,\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=16,\n",
        "            num_train_epochs=5,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir='./logs',\n",
        "            logging_steps=10,\n",
        "            eval_strategy=\"steps\",\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=50,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model='f1',\n",
        "            warmup_steps=100,\n",
        "            dataloader_num_workers=0,\n",
        "            save_total_limit=3\n",
        "        )\n",
        "        print('‚úÖ Default Training arguments reconfigured')\n",
        "\n",
        "\n",
        "if 'data_collator' not in globals():\n",
        "    print('üì¶ RECREATING DATA COLLATOR')\n",
        "    print('=' * 40)\n",
        "    # Assuming tokenizer is defined\n",
        "    if 'tokenizer' in globals():\n",
        "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "        print('‚úÖ Data collator recreated successfully')\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è WARNING: 'tokenizer' not defined, cannot recreate data collator.\")\n",
        "\n",
        "\n",
        "final_trainer = WeightedLossTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print('‚úÖ WeightedLossTrainer initialized successfully with class weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "616b9f11"
      },
      "source": [
        "**Reasoning**:\n",
        "The `WeightedLossTrainer` has been successfully initialized with the calculated class weights. The next step according to the subtask is to train the model using this modified training process to incorporate the chosen technique (class weighting). I will now train the final model using the `final_trainer` instance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7c48282"
      },
      "source": [
        "# Train the final model with weighted loss\n",
        "print(\"üöÄ STARTING FINAL MODEL TRAINING WITH CLASS WEIGHTS\")\n",
        "print('=' * 40)\n",
        "final_trainer.train()\n",
        "print(\"‚úÖ Final model training completed successfully with class weights\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b84ce084"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `compute_loss` method in my custom `WeightedLossTrainer` does not accept the `num_items_in_batch` argument, which is now being passed by the updated `Trainer` class in the `transformers` library. I need to modify the `compute_loss` method signature in `WeightedLossTrainer` to accept this argument, even if I don't use it. I will redefine the `WeightedLossTrainer` with the updated `compute_loss` signature and then re-initialize and train the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKeXs0KRN0sY"
      },
      "source": [
        "# Redefine the WeightedLossTrainer with the updated compute_loss signature\n",
        "class WeightedLossTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor.to(logits.device)) # Ensure weights are on the same device as logits\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Initialize the weighted loss trainer again with the corrected class\n",
        "final_trainer = WeightedLossTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the final model with weighted loss\n",
        "print(\"üöÄ STARTING FINAL MODEL TRAINING WITH CLASS WEIGHTS (Corrected Trainer)\")\n",
        "print('=' * 40)\n",
        "final_trainer.train()\n",
        "print(\"‚úÖ Final model training completed successfully with class weights\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "029f951d"
      },
      "source": [
        "## Retrain and evaluate\n",
        "\n",
        "### Subtask:\n",
        "Retrain and evaluate the model with the chosen technique (class weighting).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50843288"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to evaluate the performance of the final model after training with class weights on the validation set. This will show the impact of the class weighting technique. I will use the `evaluate` method of the `final_trainer` and print the relevant metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "354eff71"
      },
      "source": [
        "# Evaluate the final model with class weights\n",
        "print('üìä EVALUATING FINAL MODEL WITH CLASS WEIGHTS')\n",
        "print('=' * 40)\n",
        "\n",
        "final_results_weighted = final_trainer.evaluate()\n",
        "print(f'Final Weighted F1 Score: {final_results_weighted[\"eval_f1\"]:.3f}')\n",
        "print(f'Final Weighted Accuracy: {final_results_weighted[\"eval_accuracy\"]:.3f}')\n",
        "print(f'Final Weighted Precision: {final_results_weighted[\"eval_precision\"]:.3f}')\n",
        "print(f'Final Weighted Recall: {final_results_weighted[\"eval_recall\"]:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d81829cc"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial attempt to implement class weighting faced issues due to undefined variables and missing imports, which were resolved by recreating necessary data and ensuring all dependencies were imported.\n",
        "*   A `SyntaxError` in the manually created balanced dataset was identified and corrected.\n",
        "*   The custom `WeightedLossTrainer` initially failed due to an incorrect method signature for `compute_loss`; this was fixed by adding the `num_items_in_batch` parameter and ensuring class weights were on the correct device.\n",
        "*   After resolving these issues, the model was successfully trained using the `WeightedLossTrainer` with calculated class weights.\n",
        "*   Evaluation of the model trained with class weights on the validation set yielded the following metrics: F1 Score: 0.396, Accuracy: 0.483, Precision: 0.405, and Recall: 0.483.\n",
        "*   `UndefinedMetricWarning` messages were observed during evaluation, indicating that the model did not predict any samples for certain emotion classes in the validation set.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The observed `UndefinedMetricWarning` suggests that even with class weighting, the model may still struggle to correctly classify all emotion categories, particularly those that might still be under-represented or have subtle distinctions in the data. Further analysis into the specific classes causing the warnings is needed.\n",
        "*   Consider exploring data augmentation techniques for the under-represented classes in conjunction with or as an alternative to class weighting to provide more diverse training examples and potentially improve performance on these classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7bd8d2d"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "history_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb69f44615dd49b0980390c62423afc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b67cb71868274fe095be2fad8501b8c1",
              "IPY_MODEL_151c06fd676742fa92e5ecd7df4e72ae",
              "IPY_MODEL_bc27d89f217442c0b479f241a0ec511a"
            ],
            "layout": "IPY_MODEL_e67e7fc4984f4abdb3c595aeceafa2c4"
          }
        },
        "7b78a0d818614ca4860a14351c2236a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e65d5b15ff5e472c9471b19345c49b8f",
              "IPY_MODEL_35c9f9d4928349b09709cfd3a940a844",
              "IPY_MODEL_f03bced8119f47c0a3da22392f7b23c8"
            ],
            "layout": "IPY_MODEL_adf163c85a344cbd8dcf68f96bc0b543"
          }
        },
        "b67cb71868274fe095be2fad8501b8c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_959f149366b54d8e9b855fdb19990827",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1fa5ee18a18a42928a2303200814adfc",
            "value": "Map:‚Äá100%"
          }
        },
        "151c06fd676742fa92e5ecd7df4e72ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ceda2e5dc99415dbb54389309a855e4",
            "max": 115,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb1a72a58c944ad5a1e536354cc0f200",
            "value": 115
          }
        },
        "bc27d89f217442c0b479f241a0ec511a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c749483038847f98b28d8f040095aa9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a37b944dd1c54b5ba750f57ca91e92ba",
            "value": "‚Äá115/115‚Äá[00:00&lt;00:00,‚Äá5775.20‚Äáexamples/s]"
          }
        },
        "e67e7fc4984f4abdb3c595aeceafa2c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "959f149366b54d8e9b855fdb19990827": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fa5ee18a18a42928a2303200814adfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ceda2e5dc99415dbb54389309a855e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb1a72a58c944ad5a1e536354cc0f200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c749483038847f98b28d8f040095aa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a37b944dd1c54b5ba750f57ca91e92ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e65d5b15ff5e472c9471b19345c49b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bba41e8415b41b7997b9299bbfcca5e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1157c02bacba464e9a7ed02c3c5122d6",
            "value": "Map:‚Äá100%"
          }
        },
        "35c9f9d4928349b09709cfd3a940a844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_472fed8652d343508be19f5e20d6975f",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92fc96efd0ea4fcbb04d995846c1b1cd",
            "value": 29
          }
        },
        "f03bced8119f47c0a3da22392f7b23c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d573ba5366d4f0ab20ff7751e5232f6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a7eb62fb89084f4bbdefd08698e85364",
            "value": "‚Äá29/29‚Äá[00:00&lt;00:00,‚Äá1747.20‚Äáexamples/s]"
          }
        },
        "adf163c85a344cbd8dcf68f96bc0b543": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bba41e8415b41b7997b9299bbfcca5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1157c02bacba464e9a7ed02c3c5122d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "472fed8652d343508be19f5e20d6975f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92fc96efd0ea4fcbb04d995846c1b1cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d573ba5366d4f0ab20ff7751e5232f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7eb62fb89084f4bbdefd08698e85364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
