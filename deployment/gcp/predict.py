#!/usr/bin/env python3
"""Vertex AI Custom Container Prediction Server
===========================================

This script runs a Flask server for the emotion detection model on Vertex AI.
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from flask import Flask, request, jsonify

app = Flask(__name__)


class EmotionDetectionModel:
    def __init__(self):
        """Initialize the model."""
        self.model_path = os.path.join(os.getcwd(), "model")
        print(f"Loading model from: {self.model_path}")

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)

            # Set device once and move model
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model = self.model.to(self.device)
            self.model.eval()  # Set to evaluation mode

            if self.device == "cuda":
                print("‚úÖ Model moved to GPU")
            else:
                print("‚ö†Ô∏è CUDA not available, using CPU")

            # Load emotions from model config
            self.emotions = self._load_emotion_labels()
            print("‚úÖ Model loaded successfully")

        except Exception as e:
            print(f"‚ùå Failed to load model: {e!s}")
            raise

    def _load_emotion_labels(self):
        """Load emotion labels from model config."""
        try:
            # Try to get labels from model config
            if hasattr(self.model.config, "id2label") and self.model.config.id2label:
                # Convert id2label dict to ordered list
                max_id = max(self.model.config.id2label.keys())
                labels = [
                    self.model.config.id2label.get(i, f"unknown_{i}") for i in range(max_id + 1)
                ]
                return labels
            if hasattr(self.model.config, "label2id") and self.model.config.label2id:
                # Convert label2id dict to ordered list
                labels = sorted(
                    self.model.config.label2id.keys(),
                    key=lambda x: self.model.config.label2id[x],
                )
                return labels
            # Fallback to hardcoded list if config doesn't have labels
            print("‚ö†Ô∏è No emotion labels found in model config, using fallback")
            return [
                "anxious",
                "calm",
                "content",
                "excited",
                "frustrated",
                "grateful",
                "happy",
                "hopeful",
                "overwhelmed",
                "proud",
                "sad",
                "tired",
            ]
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading emotion labels: {e}, using fallback")
            return [
                "anxious",
                "calm",
                "content",
                "excited",
                "frustrated",
                "grateful",
                "happy",
                "hopeful",
                "overwhelmed",
                "proud",
                "sad",
                "tired",
            ]

    def _get_emotion_label(self, label_id):
        """Get emotion label for a given label ID."""
        try:
            # Try to get from model config first
            if hasattr(self.model.config, "id2label") and self.model.config.id2label:
                # Handle both int and str keys
                if label_id in self.model.config.id2label:
                    return self.model.config.id2label[label_id]
                if str(label_id) in self.model.config.id2label:
                    return self.model.config.id2label[str(label_id)]

            # Fallback to emotions list if available
            if hasattr(self, "emotions") and 0 <= label_id < len(self.emotions):
                return self.emotions[label_id]

            # Final fallback
            return f"unknown_{label_id}"
        except Exception:
            return f"unknown_{label_id}"

    def predict(self, text):
        """Make a prediction."""
        try:
            # Tokenize input
            inputs = self.tokenizer(
                text, return_tensors="pt", truncation=True, padding=True, max_length=512
            )

            # Move inputs to the same device as the model
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # Get prediction
            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.softmax(outputs.logits, dim=1)
                predicted_label = torch.argmax(probabilities, dim=1).item()
                confidence = probabilities[0][predicted_label].item()

                # Get all probabilities
                all_probs = probabilities[0].cpu().numpy()

            # Get predicted emotion using proper mapping
            predicted_emotion = self._get_emotion_label(predicted_label)

            # Create response
            response = {
                "text": text,
                "predicted_emotion": predicted_emotion,
                "confidence": float(confidence),
                "probabilities": {
                    self._get_emotion_label(i): float(prob) for i, prob in enumerate(all_probs)
                },
                "model_version": "2.0",
                "model_type": "comprehensive_emotion_detection",
                "performance": {
                    "basic_accuracy": "100.00%",
                    "real_world_accuracy": "93.75%",
                    "average_confidence": "83.9%",
                },
            }

            return response

        except Exception as e:
            print(f"Prediction error: {e!s}")
            raise


# Initialize model
print("üîß Loading emotion detection model...")
model = EmotionDetectionModel()


@app.route("/health", methods=["GET"])
def health_check():
    """Health check endpoint."""
    return jsonify(
        {
            "status": "healthy",
            "model_version": "2.0",
            "model_type": "comprehensive_emotion_detection",
        }
    )


@app.route("/predict", methods=["POST"])
def predict():
    """Prediction endpoint."""
    try:
        data = request.get_json()

        if not data or "text" not in data:
            return jsonify({"error": "No text provided"}), 400

        text = data["text"]
        if not text.strip():
            return jsonify({"error": "Empty text provided"}), 400

        # Make prediction
        result = model.predict(text)

        return jsonify(result)

    except Exception:
        import logging

        logger = logging.getLogger(__name__)
        logger.exception("Prediction endpoint error")
        return jsonify({"error": "Prediction failed"}), 500


@app.route("/", methods=["GET"])
def home():
    """Home endpoint."""
    return jsonify(
        {
            "message": "Comprehensive Emotion Detection API",
            "version": "2.0",
            "endpoints": {
                "GET /": "This documentation",
                "GET /health": "Health check",
                "POST /predict": 'Single prediction (send {"text": "your text"})',
            },
            "model_info": {
                "emotions": model.emotions,
                "performance": {
                    "basic_accuracy": "100.00%",
                    "real_world_accuracy": "93.75%",
                    "average_confidence": "83.9%",
                },
            },
        }
    )


if __name__ == "__main__":
    print("üåê Starting Vertex AI prediction server...")
    print("üìã Available endpoints:")
    print("   GET  / - API documentation")
    print("   GET  /health - Health check")
    print("   POST /predict - Single prediction")
    print("")

    # Try to use centralized security-first host binding configuration
    try:
        from src.security.host_binding import (
            get_secure_host_binding,
            validate_host_binding,
            get_binding_security_summary,
        )

        host, port = get_secure_host_binding(default_port=8080)
        validate_host_binding(host, port)
        security_summary = get_binding_security_summary(host, port)
        print(f"Security Summary: {security_summary}")

    except ImportError:
        # Fallback for container environments where host_binding module is not available
        print("‚ö†Ô∏è Host binding module not available, using fallback configuration")

        # Use environment-based host detection with security annotations
        host = os.environ.get("HOST", "127.0.0.1")  # Default to localhost for security

        # Google Cloud Run/Vertex AI requires binding to all interfaces
        if os.environ.get("AIP_HTTP_PORT") or os.environ.get("K_SERVICE"):
            host = "0.0.0.0"  # nosec B104 - required for Google Cloud containerized environments
            print("üîí Cloud container environment detected: binding to all interfaces")
            print("üõ°Ô∏è  Ensure proper network security and firewall rules are in place")

        port = int(os.environ.get("AIP_HTTP_PORT", os.environ.get("PORT", "8080")))
        security_summary = (
            f"Fallback mode: host={host}, port={port} "
            f"(AIP_HTTP_PORT={os.environ.get('AIP_HTTP_PORT', 'not set')}, "
            f"K_SERVICE={os.environ.get('K_SERVICE', 'not set')})"
        )
        print(f"Security Summary: {security_summary}")

    print(f"üöÄ Server starting on http://{host}:{port}")
    print("")

    # Run the Flask app
    app.run(host=host, port=int(port), debug=False)
