#!/usr/bin/env python3
"""
Vertex AI Custom Container Prediction Server
===========================================

This script runs a Flask server for the emotion detection model on Vertex AI.
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from flask import Flask, request, jsonify

app = Flask(__name__)

class EmotionDetectionModel:
    def __init__(self):
        """Initialize the model."""
        self.model_path = os.path.join(os.getcwd(), "model")
        print(f"Loading model from: {self.model_path}")

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)

            # Set device once and move model
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self.model = self.model.to(self.device)
            self.model.eval()  # Set to evaluation mode
            
            if self.device == 'cuda':
                print("‚úÖ Model moved to GPU")
            else:
                print("‚ö†Ô∏è CUDA not available, using CPU")

            # Load emotions from model config
            self.emotions = self._load_emotion_labels()
            print("‚úÖ Model loaded successfully")

        except Exception as e:
            print(f"‚ùå Failed to load model: {str(e)}")
            raise

    def _load_emotion_labels(self):
        """Load emotion labels from model config."""
        try:
            # Try to get labels from model config
            if hasattr(self.model.config, 'id2label') and self.model.config.id2label:
                # Convert id2label dict to ordered list
                max_id = max(self.model.config.id2label.keys())
                labels = [self.model.config.id2label.get(i, f"unknown_{i}") for i in range(max_id + 1)]
                return labels
            elif hasattr(self.model.config, 'label2id') and self.model.config.label2id:
                # Convert label2id dict to ordered list
                labels = sorted(self.model.config.label2id.keys(), key=lambda x: self.model.config.label2id[x])
                return labels
            else:
                # Fallback to hardcoded list if config doesn't have labels
                print("‚ö†Ô∏è No emotion labels found in model config, using fallback")
                return ['anxious', 'calm', 'content', 'excited', 'frustrated', 'grateful', 'happy', 'hopeful', 'overwhelmed', 'proud', 'sad', 'tired']
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading emotion labels: {e}, using fallback")
            return ['anxious', 'calm', 'content', 'excited', 'frustrated', 'grateful', 'happy', 'hopeful', 'overwhelmed', 'proud', 'sad', 'tired']

    def predict(self, text):
        """Make a prediction."""
        try:
            # Tokenize input
            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)

            # Move inputs to the same device as the model
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # Get prediction
            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.softmax(outputs.logits, dim=1)
                predicted_label = torch.argmax(probabilities, dim=1).item()
                confidence = probabilities[0][predicted_label].item()

                # Get all probabilities
                all_probs = probabilities[0].cpu().numpy()

            # Get predicted emotion
            if predicted_label in self.model.config.id2label:
                predicted_emotion = self.model.config.id2label[predicted_label]
            elif str(predicted_label) in self.model.config.id2label:
                predicted_emotion = self.model.config.id2label[str(predicted_label)]
            else:
                predicted_emotion = f"unknown_{predicted_label}"

            # Create response
            response = {
                'text': text,
                'predicted_emotion': predicted_emotion,
                'confidence': float(confidence),
                'probabilities': {
                    emotion: float(prob) for emotion, prob in zip(self.emotions, all_probs)
                },
                'model_version': '2.0',
                'model_type': 'comprehensive_emotion_detection',
                'performance': {
                    'basic_accuracy': '100.00%',
                    'real_world_accuracy': '93.75%',
                    'average_confidence': '83.9%'
                }
            }

            return response

        except Exception as e:
            print(f"Prediction error: {str(e)}")
            raise

# Initialize model
print("üîß Loading emotion detection model...")
model = EmotionDetectionModel()

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint."""
    return jsonify({
        'status': 'healthy',
        'model_version': '2.0',
        'model_type': 'comprehensive_emotion_detection'
    })

@app.route('/predict', methods=['POST'])
def predict():
    """Prediction endpoint."""
    try:
        data = request.get_json()

        if not data or 'text' not in data:
            return jsonify({'error': 'No text provided'}), 400

        text = data['text']
        if not text.strip():
            return jsonify({'error': 'Empty text provided'}), 400

        # Make prediction
        result = model.predict(text)

        return jsonify(result)

    except Exception:
        import logging
        logger = logging.getLogger(__name__)
        logger.exception("Prediction endpoint error")
        return jsonify({'error': 'Prediction failed'}), 500

@app.route('/', methods=['GET'])
def home():
    """Home endpoint."""
    return jsonify({
        'message': 'Comprehensive Emotion Detection API',
        'version': '2.0',
        'endpoints': {
            'GET /': 'This documentation',
            'GET /health': 'Health check',
            'POST /predict': 'Single prediction (send {"text": "your text"})'
        },
        'model_info': {
            'emotions': model.emotions,
            'performance': {
                'basic_accuracy': '100.00%',
                'real_world_accuracy': '93.75%',
                'average_confidence': '83.9%'
            }
        }
    })

if __name__ == '__main__':
    print("üåê Starting Vertex AI prediction server...")
    print("üìã Available endpoints:")
    print("   GET  / - API documentation")
    print("   GET  /health - Health check")
    print("   POST /predict - Single prediction")
    print("")
    # Use centralized security-first host binding configuration
    from src.security.host_binding import (
        get_secure_host_binding,
        validate_host_binding,
        get_binding_security_summary,
    )

    host, port = get_secure_host_binding(default_port=8080)
    validate_host_binding(host, port)

    security_summary = get_binding_security_summary(host, port)
    print(f"Security Summary: {security_summary}")
    print(f"üöÄ Server starting on http://{host}:{port}")
    print("")

    # Run the Flask app
    app.run(host=host, port=port, debug=False)
