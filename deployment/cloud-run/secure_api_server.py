#!/usr/bin/env python3
"""
üöÄ SECURE EMOTION DETECTION API FOR CLOUD RUN
============================================
Production-ready Flask API with comprehensive security features and Swagger documentation.
"""

import os
import time
import logging
import uuid
import threading
import hmac
from flask import Flask, request, jsonify, g
from flask_restx import Api, Resource, fields, Namespace
from werkzeug.datastructures import FileStorage
from functools import wraps

# Import security modules
from security_headers import add_security_headers
from rate_limiter import rate_limit

# Import shared model utilities
from model_utils import (
    ensure_model_loaded, predict_emotions, get_model_status,
    validate_text_input,
)

# Import T5 and Whisper models
T5_AVAILABLE = False
WHISPER_AVAILABLE = False

try:
    from src.models.summarization.t5_summarizer import create_t5_summarizer
    T5_AVAILABLE = True
except ImportError as e:
    import_logger.warning(f"T5 summarization not available: {e}")
    T5_AVAILABLE = False

try:
    from src.models.voice_processing.whisper_transcriber import create_whisper_transcriber
    WHISPER_AVAILABLE = True
except ImportError as e:
    import_logger.warning(f"Whisper transcription not available: {e}")
    WHISPER_AVAILABLE = False

# Temporary file cleanup utility
def cleanup_temp_file(file_path):
    """Safely delete temporary file with error logging"""
    try:
        if file_path and os.path.exists(file_path):
            os.remove(file_path)
            logger.debug(f"Successfully deleted temporary file: {file_path}")
    except Exception as exc:
        logger.error(f"Failed to delete temporary file {file_path}: {exc}")

# Configure logging for Cloud Run
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Set up logger for import error handling
import_logger = logging.getLogger(__name__)

app = Flask(__name__)

# Add security headers
add_security_headers(app)

# Global model instances for T5 and Whisper
t5_summarizer = None
whisper_transcriber = None

def initialize_advanced_models():
    """Initialize T5 and Whisper models if available (only if not already loaded)"""
    global t5_summarizer, whisper_transcriber, T5_AVAILABLE, WHISPER_AVAILABLE

    # Initialize T5 model
    if T5_AVAILABLE and t5_summarizer is None:
        try:
            logger.info("Loading T5 summarization model (fallback)...")
            t5_summarizer = create_t5_summarizer("t5-small")
            logger.info("‚úÖ T5 summarization model loaded")
        except Exception as e:
            logger.error(f"‚ùå Failed to load T5 summarizer: {e}")
            T5_AVAILABLE = False

    # Initialize Whisper model
    if WHISPER_AVAILABLE and whisper_transcriber is None:
        try:
            logger.info("Loading Whisper transcription model (fallback)...")
            whisper_transcriber = create_whisper_transcriber("base")
            logger.info("‚úÖ Whisper transcription model loaded")
        except Exception as e:
            logger.error(f"‚ùå Failed to load Whisper transcriber: {e}")
            WHISPER_AVAILABLE = False

def load_all_models():
    """Consolidated model loading function for all AI models"""
    global t5_summarizer, whisper_transcriber, T5_AVAILABLE, WHISPER_AVAILABLE
    
    logger.info("üîÑ Loading all AI models...")
    
    # Load emotion detection model
    try:
        load_model()
        logger.info("‚úÖ Emotion detection model loaded")
    except Exception as e:
        logger.error(f"‚ùå Failed to load emotion detection model: {e}")
        raise

    # Load T5 summarization model
    if T5_AVAILABLE and t5_summarizer is None:
        try:
            logger.info("üîÑ Loading T5 summarization model...")
            t5_summarizer = create_t5_summarizer("t5-small")
            logger.info("‚úÖ T5 summarization model loaded")
        except Exception as e:
            logger.error(f"‚ùå Failed to load T5 summarizer: {e}")
            T5_AVAILABLE = False

    # Load Whisper transcription model
    if WHISPER_AVAILABLE and whisper_transcriber is None:
        try:
            logger.info("üîÑ Loading Whisper transcription model...")
            whisper_transcriber = create_whisper_transcriber("base")
            logger.info("‚úÖ Whisper transcription model loaded")
        except Exception as e:
            logger.error(f"‚ùå Failed to load Whisper transcriber: {e}")
            WHISPER_AVAILABLE = False

    logger.info("‚úÖ All available models loaded successfully")

# Initialize advanced models at startup
initialize_advanced_models()

# Register root endpoint BEFORE Flask-RESTX initialization to avoid conflicts
@app.route('/')
def home():  # Changed from api_root to home to avoid conflict with Flask-RESTX's root
    """Get API status and information"""
    try:
        logger.info(f"Root endpoint accessed from {request.remote_addr}")
        return jsonify({
            'service': 'SAMO Emotion Detection API',
            'status': 'operational',
            'version': '2.0.0-secure',
            'security': 'enabled',
            'rate_limit': RATE_LIMIT_PER_MINUTE,
            'timestamp': time.time()
        })
    except Exception as e:
        logger.error(f"Root endpoint error for {request.remote_addr}: {str(e)}")
        return create_error_response('Internal server error', 500)

# Initialize Flask-RESTX API without Swagger to avoid 500 errors
api = Api(
    app,
    version='2.0.0',
    title='SAMO Emotion Detection API',
    description='Secure, production-ready emotion detection API with comprehensive security features',
    # Temporarily disable Swagger docs to avoid 500 errors
    # doc='/docs',
    authorizations={
        'apikey': {
            'type': 'apiKey',
            'in': 'header',
            'name': 'X-API-Key'
        }
    },
    security='apikey'
)

# Create namespaces for better organization
main_ns = Namespace('api', description='Main API operations')  # Removed leading slash to avoid double slashes
admin_ns = Namespace('/admin', description='Admin operations', authorizations={
    'apikey': {
        'type': 'apiKey',
        'in': 'header',
        'name': 'X-API-Key'
    }
})

# Add namespaces to API
api.add_namespace(main_ns)
api.add_namespace(admin_ns)

# Define request/response models for Swagger
text_input_model = api.model('TextInput', {
    'text': fields.String(required=True, description='Text to analyze for emotion', example='I am feeling happy today!')
})

emotion_response_model = api.model('EmotionResponse', {
    'text': fields.String(description='Input text'),
    'emotions': fields.List(fields.Nested(api.model('Emotion', {
        'emotion': fields.String(description='Emotion label'),
        'confidence': fields.Float(description='Confidence score')
    }))),
    'confidence': fields.Float(description='Overall confidence'),
    'request_id': fields.String(description='Unique request identifier'),
    'timestamp': fields.Float(description='Unix timestamp')
})

batch_input_model = api.model('BatchInput', {
    'texts': fields.List(fields.String, required=True, description='List of texts to analyze', example=['I am happy', 'I am sad'])
})

batch_response_model = api.model('BatchResponse', {
    'results': fields.List(fields.Nested(emotion_response_model))
})

error_model = api.model('Error', {
    'error': fields.String(description='Error message'),
    'status_code': fields.Integer(description='HTTP status code'),
    'request_id': fields.String(description='Unique request identifier'),
    'timestamp': fields.Float(description='Unix timestamp')
})

# Security configuration from environment variables
ADMIN_API_KEY = os.environ.get("ADMIN_API_KEY")
if not ADMIN_API_KEY:
    raise ValueError("ADMIN_API_KEY environment variable must be set")
MAX_INPUT_LENGTH = int(os.environ.get("MAX_INPUT_LENGTH", "512"))
MAX_TEXT_LENGTH = int(os.environ.get("MAX_TEXT_LENGTH", "5000"))
MAX_AUDIO_FILE_SIZE_MB = int(os.environ.get("MAX_AUDIO_FILE_SIZE_MB", "45"))
RATE_LIMIT_PER_MINUTE = int(os.environ.get("RATE_LIMIT_PER_MINUTE", "100"))
MODEL_PATH = os.environ.get("MODEL_PATH", "/app/model")
PORT = int(os.environ.get("PORT", "8080"))

# Global variables for model state (thread-safe with locks)
model = None
tokenizer = None
emotion_mapping = None
model_loading = False
model_loaded = False
model_lock = threading.Lock()

# Emotion mapping based on training order
EMOTION_MAPPING = ['anxious', 'calm', 'content', 'excited', 'frustrated', 'grateful', 'happy', 'hopeful', 'overwhelmed', 'proud', 'sad', 'tired']

def require_api_key(f):
    """Decorator to require API key via X-API-Key header"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        api_key = request.headers.get('X-API-Key')
        if not verify_api_key(api_key):
            logger.warning(f"Invalid API key attempt from {request.remote_addr}")
            return create_error_response('Unauthorized - Invalid API key', 401)
        return f(*args, **kwargs)
    return decorated_function

def verify_api_key(api_key: str) -> bool:
    """Verify API key using constant-time comparison"""
    if not api_key:
        return False
    return hmac.compare_digest(api_key, ADMIN_API_KEY)

def sanitize_input(text: str) -> str:
    """Sanitize input text"""
    if not isinstance(text, str):
        raise ValueError("Input must be a string")

    # Remove potentially dangerous characters
    dangerous_chars = ['<', '>', '"', "'", '&', ';', '|', '`', '$', '(', ')', '{{', '}}']
    for char in dangerous_chars:
        text = text.replace(char, '')

    # Limit length
    if len(text) > MAX_INPUT_LENGTH:
        text = text[:MAX_INPUT_LENGTH]

    return text.strip()

def load_model():
    """Load the emotion detection model using shared utilities"""
    # Use the shared model loading function
    success = ensure_model_loaded()
    if not success:
        logger.error("‚ùå Model loading failed")
        raise RuntimeError("Model loading failed - check logs for details")

def predict_emotion(text: str) -> dict:
    """Predict emotion for given text using shared utilities"""
    # Use shared prediction function
    result = predict_emotions(text)

    # Add request ID for tracking
    result['request_id'] = str(uuid.uuid4())

    return result

def check_model_loaded():
    """Ensure model is loaded before processing requests"""
    # Use shared model loading function
    return ensure_model_loaded()

def create_error_response(error_message: str, status_code: int):
    """Create a properly formatted error response for Flask-RESTX"""
    error_response = {
        'error': error_message,
        'status_code': status_code,
        'request_id': str(uuid.uuid4()),
        'timestamp': time.time()
    }
    return error_response, status_code

def handle_rate_limit_exceeded():
    """Handle rate limit exceeded - return proper error response"""
    logger.warning(f"Rate limit exceeded for {request.remote_addr}")
    return create_error_response('Rate limit exceeded - too many requests', 429)

def log_rate_limit_info():
    """Log rate limiting information for debugging"""
    logger.debug(f"Rate limiting configured: {RATE_LIMIT_PER_MINUTE} requests per minute")
    logger.debug(f"Current request from: {request.remote_addr}")

@app.before_request
def before_request():
    """Add request ID and timing to all requests"""
    g.start_time = time.time()
    g.request_id = str(uuid.uuid4())
    
    # Lazy model initialization on first request
    if not check_model_loaded():
        logger.info("üîÑ Lazy initializing model on first request...")
        initialize_model()
    
    # Log incoming requests for debugging
    logger.info(f"üì• Request: {request.method} {request.path} from {request.remote_addr} (ID: {g.request_id})")
    
    # Log request headers for debugging (excluding sensitive ones)
    headers_to_log = {k: v for k, v in request.headers.items() 
                      if k.lower() not in ['authorization', 'x-api-key', 'cookie']}
    logger.debug(f"üìã Request headers: {headers_to_log}")

@app.after_request
def after_request(response):
    """Add request tracking headers"""
    if hasattr(g, 'start_time'):
        duration = time.time() - g.start_time
        response.headers['X-Request-Duration'] = str(duration)
    if hasattr(g, 'request_id'):
        response.headers['X-Request-ID'] = g.request_id
    
    # Log response for debugging
    logger.info(f"üì§ Response: {response.status_code} for {request.method} {request.path} "
                f"from {request.remote_addr} (ID: {g.request_id}, Duration: {duration:.3f}s)")
    
    return response



@main_ns.route('/health')
class Health(Resource):
    @api.doc('get_health')
    @api.response(200, 'Success')
    @api.response(503, 'Service Unavailable', error_model)
    @api.response(500, 'Internal Server Error', error_model)
    def get(self):
        """Get API health status"""
        try:
            logger.info(f"Health check from {request.remote_addr}")
            model_status = check_model_loaded()
            
            if model_status:
                logger.info("Health check passed - model is ready")
                return {
                    'status': 'healthy',
                    'model_loaded': model_status,
                    'model_loading': False,
                    'port': PORT,
                    'timestamp': time.time()
                }
            else:
                logger.warning("Health check failed - model not ready")
                return create_error_response('Service unavailable - model not ready', 503)
                
        except Exception as e:
            logger.error(f"Health check error for {request.remote_addr}: {str(e)}")
            return create_error_response('Internal server error', 500)

@main_ns.route('/predict')
class Predict(Resource):
    @api.doc('post_predict', security='apikey')
    @api.expect(text_input_model, validate=True)
    @api.response(200, 'Success', emotion_response_model)
    @api.response(400, 'Bad Request', error_model)
    @api.response(401, 'Unauthorized', error_model)
    @api.response(429, 'Too Many Requests', error_model)
    @api.response(503, 'Service Unavailable', error_model)
    @rate_limit(RATE_LIMIT_PER_MINUTE)
    @require_api_key
    def post(self):
        """Predict emotion for a single text input"""
        try:
            # Log rate limiting info for debugging
            log_rate_limit_info()
            
            # Get and validate input
            data = request.get_json()
            if not data or 'text' not in data:
                logger.warning(f"Missing text field in request from {request.remote_addr}")
                return create_error_response('Missing text field', 400)

            text = data['text']
            if not text or not isinstance(text, str):
                logger.warning(f"Invalid text input from {request.remote_addr}: {type(text)}")
                return create_error_response('Text must be a non-empty string', 400)

            # Sanitize input
            try:
                text = sanitize_input(text)
            except ValueError as e:
                logger.warning(f"Input sanitization failed for {request.remote_addr}: {str(e)}")
                return create_error_response(str(e), 400)

            # Ensure model is loaded
            if not check_model_loaded():
                logger.error("Model not ready for prediction request")
                return create_error_response('Model not ready', 503)

            # Predict emotion
            logger.info(f"Processing prediction request for {request.remote_addr}")
            result = predict_emotion(text)
            return result

        except Exception as e:
            logger.error(f"Prediction error for {request.remote_addr}: {str(e)}")
            return create_error_response('Internal server error', 500)

@main_ns.route('/predict_batch')
class PredictBatch(Resource):
    @api.doc('post_predict_batch', security='apikey')
    @api.expect(batch_input_model, validate=True)
    @api.response(200, 'Success', batch_response_model)
    @api.response(400, 'Bad Request', error_model)
    @api.response(401, 'Unauthorized', error_model)
    @api.response(429, 'Too Many Requests', error_model)
    @api.response(503, 'Service Unavailable', error_model)
    @rate_limit(RATE_LIMIT_PER_MINUTE)
    @require_api_key
    def post(self):
        """Predict emotions for multiple text inputs"""
        try:
            # Log rate limiting info for debugging
            log_rate_limit_info()
            
            # Get and validate input
            data = request.get_json()
            if not data or 'texts' not in data:
                logger.warning(f"Missing texts field in batch request from {request.remote_addr}")
                return create_error_response('Missing texts field', 400)

            texts = data['texts']
            if not isinstance(texts, list) or len(texts) == 0:
                logger.warning(f"Invalid texts input from {request.remote_addr}: {type(texts)}")
                return create_error_response('Texts must be a non-empty list', 400)

            if len(texts) > 100:  # Limit batch size
                logger.warning(f"Batch size too large from {request.remote_addr}: {len(texts)}")
                return create_error_response('Batch size too large (max 100)', 400)

            # Ensure model is loaded
            if not check_model_loaded():
                logger.error("Model not ready for batch prediction request")
                return create_error_response('Model not ready', 503)

            # Process each text
            logger.info(f"Processing batch prediction request for {request.remote_addr} with {len(texts)} texts")
            results = []
            for text in texts:
                if not text or not isinstance(text, str):
                    continue
                
                try:
                    text = sanitize_input(text)
                    result = predict_emotion(text)
                    results.append(result)
                except Exception as e:
                    logger.warning(f"Failed to process text in batch from {request.remote_addr}: {str(e)}")
                    continue

            return {'results': results}

        except Exception as e:
            logger.error(f"Batch prediction error for {request.remote_addr}: {str(e)}")
            return create_error_response('Internal server error', 500)

@main_ns.route('/emotions')
class Emotions(Resource):
    @api.doc('get_emotions')
    @api.response(200, 'Success')
    @api.response(500, 'Internal Server Error', error_model)
    def get(self):
        """Get list of supported emotions"""
        try:
            logger.info(f"Emotions list requested from {request.remote_addr}")
            return {
                'emotions': EMOTION_MAPPING,
                'count': len(EMOTION_MAPPING),
                'timestamp': time.time()
            }
        except Exception as e:
            logger.error(f"Emotions endpoint error for {request.remote_addr}: {str(e)}")
            return create_error_response('Internal server error', 500)

# Admin endpoints
@admin_ns.route('/model_status')
class ModelStatus(Resource):
    @api.doc('get_model_status', security='apikey')
    @api.response(200, 'Success')
    @api.response(401, 'Unauthorized', error_model)
    @api.response(500, 'Internal Server Error', error_model)
    @require_api_key
    def get(self):
        """Get detailed model status (admin only)"""
        try:
            # Get model status from shared utilities
            logger.info(f"Admin model status request from {request.remote_addr}")
            status = get_model_status()
            return status
        except Exception as e:
            logger.error(f"Model status error for {request.remote_addr}: {str(e)}")
            return create_error_response('Internal server error', 500)

@admin_ns.route('/security_status')
class SecurityStatus(Resource):
    @api.doc('get_security_status', security='apikey')
    @api.response(200, 'Success')
    @api.response(401, 'Unauthorized', error_model)
    @api.response(500, 'Internal Server Error', error_model)
    @require_api_key
    def get(self):
        """Get security configuration status (admin only)"""
        try:
            logger.info(f"Admin security status request from {request.remote_addr}")
            return {
                'api_key_protection': True,
                'input_sanitization': True,
                'rate_limiting': True,
                'request_tracking': True,
                'security_headers': True,
                'timestamp': time.time()
            }
        except Exception as e:
            logger.error(f"Security status error for {request.remote_addr}: {str(e)}")
            return create_error_response('Internal server error', 500)

# Error handlers for Flask-RESTX - using direct registration due to decorator compatibility issue
def rate_limit_exceeded(error):
    """Handle rate limit exceeded errors"""
    logger.warning(f"Rate limit exceeded for {request.remote_addr}")
    return create_error_response('Rate limit exceeded - too many requests', 429)

def internal_error(error):
    """Handle internal server errors"""
    logger.error(f"Internal server error for {request.remote_addr}: {str(error)}")
    return create_error_response('Internal server error', 500)

def not_found(error):
    """Handle not found errors"""
    logger.warning(f"Endpoint not found for {request.remote_addr}: {request.url}")
    return create_error_response('Endpoint not found', 404)

def method_not_allowed(error):
    """Handle method not allowed errors"""
    logger.warning(f"Method not allowed for {request.remote_addr}: {request.method} {request.url}")
    return create_error_response('Method not allowed', 405)

def handle_unexpected_error(error):
    """Handle any unexpected errors"""
    logger.error(f"Unexpected error for {request.remote_addr}: {str(error)}")
    return create_error_response('An unexpected error occurred', 500)

# Register error handlers directly
api.error_handlers[429] = rate_limit_exceeded
api.error_handlers[500] = internal_error
api.error_handlers[404] = not_found
api.error_handlers[405] = method_not_allowed
api.error_handlers[Exception] = handle_unexpected_error

# ===== ADVANCED ENDPOINTS: Summarization and Transcription =====

# Simple functional endpoint for testing
@app.route('/summarize', methods=['POST'])
@rate_limit()
@require_api_key
def summarize_text():
    """Simple functional endpoint for T5 summarization"""
    logger.info("üì• Functional summarization endpoint called")

    if not T5_AVAILABLE or t5_summarizer is None:
        logger.error("T5 summarization service unavailable")
        return jsonify({"error": "Text summarization service unavailable"}), 503

    start_time = time.time()
    data = request.get_json()
    logger.info(f"Request data: {data}")

    if not data or 'text' not in data:
        return jsonify({"error": "Text field is required"}), 400

    text = data['text'].strip()
    max_length = data.get('max_length', 150)
    min_length = data.get('min_length', 30)
    logger.info(f"Processing text: {len(text)} chars, max_length: {max_length}")

    if not text:
        return jsonify({"error": "Text cannot be empty"}), 400

    if len(text) > MAX_TEXT_LENGTH:
        return jsonify({"error": f"Text too long (max {MAX_TEXT_LENGTH} characters)"}), 400

    try:
        logger.info("üîÑ Starting T5 summarization...")
        summary = t5_summarizer.generate_summary(
            text, max_length=max_length, min_length=min_length
        )
        logger.info(f"‚úÖ T5 summarization completed: {summary[:100] if summary else 'None'}...")

        original_length = len(text.split())
        summary_length = len(summary.split()) if summary else 0
        compression_ratio = 1 - (summary_length / original_length) if original_length > 0 else 0

        result = {
            'summary': summary,
            'original_length': original_length,
            'summary_length': summary_length,
            'compression_ratio': compression_ratio,
            'processing_time': time.time() - start_time
        }
        logger.info(f"üì§ Summarization result: {result}")
        return jsonify(result)

    except Exception as e:
        logger.error(f"‚ùå Summarization failed: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({"error": f"Summarization failed: {str(e)}"}), 500


# Simple functional endpoint for Whisper transcription
@app.route('/transcribe', methods=['POST'])
@rate_limit()
@require_api_key
def transcribe_audio():
    """Simple functional endpoint for Whisper transcription"""
    logger.info("üì• Functional transcription endpoint called")

    if not WHISPER_AVAILABLE or whisper_transcriber is None:
        logger.error("Whisper transcription service unavailable")
        return jsonify({"error": "Voice transcription service unavailable"}), 503

    start_time = time.time()

    # Check if audio file is provided
    if 'audio' not in request.files:
        return jsonify({"error": "Audio file is required"}), 400

    audio_file = request.files['audio']
    if audio_file.filename == '':
        return jsonify({"error": "No audio file selected"}), 400

    # Get optional parameters
    language = request.form.get('language', None)
    model_size = request.form.get('model_size', 'base')

    logger.info(f"Processing audio file: {audio_file.filename}, language: {language}")

    try:
        # Save uploaded file temporarily
        import tempfile
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(audio_file.filename)[1]) as tmp_file:
            audio_file.save(tmp_file.name)
            temp_path = tmp_file.name

        logger.info("üîÑ Starting Whisper transcription...")

        # Transcribe the audio
        result = whisper_transcriber.transcribe(temp_path, language=language)

        # Clean up temporary file
        cleanup_temp_file(temp_path)
        logger.info(f"‚úÖ Whisper transcription completed: {result.text[:100] if result and result.text else 'None'}...")

        response_data = {
            'transcription': result.text if result else '',
            'language': result.language if result else 'unknown',
            'confidence': result.confidence if result else 0.0,
            'duration': result.duration if result else 0.0,
            'word_count': result.word_count if result else 0,
            'speaking_rate': result.speaking_rate if result else 0.0,
            'audio_quality': result.audio_quality if result else 'unknown',
            'processing_time': result.processing_time if result else 0.0
        }

        logger.info(f"üì§ Transcription result: {response_data}")
        return jsonify(response_data)

    except Exception as e:
        logger.error(f"‚ùå Transcription failed: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")

        # Clean up temporary file if it exists
        if 'temp_path' in locals():
            cleanup_temp_file(temp_path)
        return jsonify({"error": f"Transcription failed: {str(e)}"}), 500


@api.route('/summarize')
class Summarize(Resource):
    """Text summarization endpoint"""

    @api.doc('summarize_text')
    @api.expect(api.model('SummarizeRequest', {
        'text': fields.String(required=True, description='Text to summarize', example='This is a long text that needs to be summarized...'),
        'max_length': fields.Integer(default=150, description='Maximum summary length'),
        'min_length': fields.Integer(default=30, description='Minimum summary length')
    }))
    # Temporarily removed @api.marshal_with to debug
    # @api.marshal_with(api.model('SummarizeResponse', {
    #     'summary': fields.String(description='Generated summary'),
    #     'original_length': fields.Integer(description='Original text length'),
    #     'summary_length': fields.Integer(description='Summary length'),
    #     'compression_ratio': fields.Float(description='Compression ratio'),
    #     'processing_time': fields.Float(description='Processing time in seconds')
    # }))
    @rate_limit
    @require_api_key
    def post(self):
        """Summarize text using T5 model"""
        logger.info("üì• Summarization request received")
        logger.info(f"T5_AVAILABLE: {T5_AVAILABLE}, t5_summarizer: {t5_summarizer is not None}")

        if not T5_AVAILABLE or t5_summarizer is None:
            logger.error("T5 summarization service unavailable")
            api.abort(503, "Text summarization service unavailable")

        start_time = time.time()
        data = request.get_json()
        logger.info(f"Request data: {data}")

        if not data or 'text' not in data:
            api.abort(400, "Text field is required")

        text = data['text'].strip()
        max_length = data.get('max_length', 150)
        min_length = data.get('min_length', 30)
        logger.info(f"Text length: {len(text)}, max_length: {max_length}, min_length: {min_length}")

        if not text:
            api.abort(400, "Text cannot be empty")

        if len(text) > MAX_TEXT_LENGTH:
            api.abort(400, f"Text too long (max {MAX_TEXT_LENGTH} characters)")

        try:
            logger.info("üîÑ Starting T5 summarization...")
            summary = t5_summarizer.generate_summary(
                text, max_length=max_length, min_length=min_length
            )
            logger.info(f"‚úÖ T5 summarization completed: {summary[:100]}...")

            original_length = len(text.split())
            summary_length = len(summary.split()) if summary else 0
            compression_ratio = 1 - (summary_length / original_length) if original_length > 0 else 0

            result = {
                'summary': summary,
                'original_length': original_length,
                'summary_length': summary_length,
                'compression_ratio': compression_ratio,
                'processing_time': time.time() - start_time
            }
            logger.info(f"üì§ Summarization result: {result}")
            return result

        except Exception as e:
            logger.error(f"‚ùå Summarization failed: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            api.abort(500, f"Summarization failed: {str(e)}")


@api.route('/transcribe')
class Transcribe(Resource):
    """Voice transcription endpoint"""

    @api.doc('transcribe_audio')
    @api.expect(api.parser()
        .add_argument('audio', type=FileStorage, location='files', required=True,
                     help='Audio file to transcribe (MP3, WAV, M4A)')
        .add_argument('language', type=str, location='form', help='Language code (optional)')
        .add_argument('model_size', type=str, location='form', default='base',
                     help='Whisper model size (tiny, base, small, medium, large)'))
    @api.marshal_with(api.model('TranscriptionResponse', {
        'text': fields.String(description='Transcribed text'),
        'language': fields.String(description='Detected language'),
        'confidence': fields.Float(description='Transcription confidence'),
        'duration': fields.Float(description='Audio duration in seconds'),
        'processing_time': fields.Float(description='Processing time in seconds'),
        'word_count': fields.Integer(description='Number of words'),
        'speaking_rate': fields.Float(description='Words per minute')
    }))
    @rate_limit
    @require_api_key
    def post(self):
        """Transcribe audio file to text using Whisper"""
        if not WHISPER_AVAILABLE or whisper_transcriber is None:
            api.abort(503, "Voice transcription service unavailable")

        start_time = time.time()

        # Parse form data
        if 'audio' not in request.files:
            api.abort(400, "Audio file is required")

        audio_file = request.files['audio']
        if not audio_file.filename:
            api.abort(400, "No audio file selected")

        # Validate file type
        allowed_extensions = {'mp3', 'wav', 'm4a', 'aac', 'ogg', 'flac'}
        if '.' not in audio_file.filename:
            api.abort(400, "File must have an extension")
        ext = audio_file.filename.rsplit('.', 1)[1].lower()
        if ext not in allowed_extensions:
            api.abort(400, f"Unsupported file type. Allowed: {', '.join(allowed_extensions)}")

        # Check file size (max 45MB)
        audio_file.seek(0, 2)  # Seek to end
        file_size = audio_file.tell()
        audio_file.seek(0)  # Reset to beginning
        if file_size > MAX_AUDIO_FILE_SIZE_MB * 1024 * 1024:
            api.abort(400, f"File too large (max {MAX_AUDIO_FILE_SIZE_MB}MB)")

        try:
            # Save uploaded file temporarily
            import tempfile
            with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{ext}') as temp_file:
                audio_file.save(temp_file.name)
                temp_path = temp_file.name

            try:
                # Transcribe
                language = request.form.get('language')
                result = whisper_transcriber.transcribe(temp_path, language=language)

                # Extract result data
                transcription_text = result.text if hasattr(result, 'text') else str(result)
                language_detected = getattr(result, 'language', 'unknown')
                confidence = getattr(result, 'confidence', 0.0)
                duration = getattr(result, 'duration', 0.0)
                word_count = len(transcription_text.split())
                speaking_rate = word_count / (duration / 60) if duration > 0 else 0

                return {
                    'text': transcription_text,
                    'language': language_detected,
                    'confidence': confidence,
                    'duration': duration,
                    'processing_time': time.time() - start_time,
                    'word_count': word_count,
                    'speaking_rate': speaking_rate
                }

            finally:
                # Cleanup temporary file
                cleanup_temp_file(temp_path)

        except Exception as e:
            logger.error(f"Transcription failed: {e}")
            api.abort(500, "Transcription failed")


@api.route('/analyze/complete')
class CompleteAnalysis(Resource):
    """Complete analysis endpoint combining all AI models"""

    @api.doc('analyze_complete')
    @api.expect(api.parser()
        .add_argument('text', type=str, location='form', help='Text to analyze (optional if audio provided)')
        .add_argument('audio', type=FileStorage, location='files', help='Audio file to transcribe (optional if text provided)')
        .add_argument('language', type=str, location='form', help='Language code for transcription')
        .add_argument('generate_summary', type=bool, location='form', default=True, help='Whether to generate summary')
        .add_argument('emotion_threshold', type=float, location='form', default=0.1, help='Emotion detection threshold'))
    @api.marshal_with(api.model('CompleteAnalysisResponse', {
        'transcription': fields.Nested(api.model('TranscriptionData', {
            'text': fields.String(),
            'language': fields.String(),
            'confidence': fields.Float(),
            'duration': fields.Float()
        })),
        'emotion_analysis': fields.Nested(api.model('EmotionData', {
            'emotions': fields.Raw(),
            'primary_emotion': fields.String(),
            'confidence': fields.Float(),
            'emotional_intensity': fields.String()
        })),
        'summary': fields.Nested(api.model('SummaryData', {
            'summary': fields.String(),
            'compression_ratio': fields.Float(),
            'emotional_tone': fields.String()
        })),
        'processing_time': fields.Float(),
        'pipeline_status': fields.Raw()
    }))
    @rate_limit
    @require_api_key
    def post(self):
        """Complete analysis pipeline: transcription + emotion + summarization"""
        start_time = time.time()
        pipeline_status = {
            'emotion_detection': True,
            'text_summarization': T5_AVAILABLE and t5_summarizer is not None,
            'voice_processing': WHISPER_AVAILABLE and whisper_transcriber is not None
        }

        text_to_analyze = request.form.get('text', '').strip()
        generate_summary = request.form.get('generate_summary', 'true').lower() == 'true'
        emotion_threshold = float(request.form.get('emotion_threshold', 0.1))

        # Handle transcription if audio provided
        if 'audio' in request.files:
            audio_file = request.files['audio']
            if audio_file.filename:
                # Use transcription endpoint logic
                import tempfile

                ext = audio_file.filename.rsplit('.', 1)[1].lower()
                with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{ext}') as temp_file:
                    audio_file.save(temp_file.name)
                    temp_path = temp_file.name

                try:
                    language = request.form.get('language')
                    transcription_result = whisper_transcriber.transcribe(temp_path, language=language)
                    text_to_analyze = transcription_result.text if hasattr(transcription_result, 'text') else str(transcription_result)
                finally:
                    cleanup_temp_file(temp_path)

        if not text_to_analyze:
            api.abort(400, "Either text or audio file must be provided")

        # Emotion Analysis
        emotion_result = {}
        try:
            raw_emotion = predict_emotions(text_to_analyze, threshold=emotion_threshold)
            emotion_result = normalize_emotion_results(raw_emotion)
        except Exception as e:
            logger.warning(f"Emotion analysis failed: {e}")
            emotion_result = {
                'emotions': {'neutral': 1.0},
                'primary_emotion': 'neutral',
                'confidence': 1.0,
                'emotional_intensity': 'neutral'
            }

        # Text Summarization
        summary_result = {}
        if generate_summary and T5_AVAILABLE and t5_summarizer is not None:
            try:
                summary_text = t5_summarizer.generate_summary(text_to_analyze)
                original_length = len(text_to_analyze.split())
                summary_length = len(summary_text.split())
                compression_ratio = 1 - (summary_length / original_length) if original_length > 0 else 0

                # Determine emotional tone
                tone = "neutral"
                if emotion_result.get('primary_emotion') in ['joy', 'gratitude', 'excitement']:
                    tone = "positive"
                elif emotion_result.get('primary_emotion') in ['sadness', 'anger', 'fear']:
                    tone = "negative"

                summary_result = {
                    'summary': summary_text,
                    'compression_ratio': compression_ratio,
                    'emotional_tone': tone
                }
            except Exception as e:
                logger.warning(f"Summarization failed: {e}")

        return {
            'transcription': {
                'text': text_to_analyze,
                'language': 'en',  # Default assumption
                'confidence': 1.0 if 'audio' not in request.files else 0.95,
                'duration': 0.0  # Would need audio metadata
            } if 'audio' in request.files else None,
            'emotion_analysis': emotion_result,
            'summary': summary_result,
            'processing_time': time.time() - start_time,
            'pipeline_status': pipeline_status
        }


def initialize_model():
    """Initialize the emotion detection model"""
    try:
        logger.info("üöÄ Initializing emotion detection API server...")
        logger.info(f"üìä Configuration: MAX_INPUT_LENGTH={MAX_INPUT_LENGTH}, RATE_LIMIT={RATE_LIMIT_PER_MINUTE}/min")
        logger.info(f"üîê Security: API key protection enabled, Admin API key configured")
        logger.info(f"üåê Server: Port {PORT}, Model path: {MODEL_PATH}")
        logger.info(f"üîÑ Rate limiting: {RATE_LIMIT_PER_MINUTE} requests per minute")

        # Load all models using consolidated function
        load_all_models()

        logger.info("‚úÖ Model initialization completed successfully")
        logger.info("üöÄ API server ready to handle requests")

    except Exception as e:
        logger.error(f"‚ùå Failed to initialize API server: {str(e)}")
        raise

# Initialize models immediately when module is imported
logger.info("üöÄ Initializing models during module import...")
try:
    initialize_model()
    logger.info("‚úÖ Models loaded successfully during module import")
    MODELS_LOADED_AT_STARTUP = True
except Exception as e:
    logger.error(f"‚ùå Failed to load models during module import: {e}")
    # Continue anyway - models will be loaded on first request if startup fails
    logger.info("‚ö†Ô∏è Continuing without pre-loaded models - will load on first request")
    MODELS_LOADED_AT_STARTUP = False

if __name__ == '__main__':
    logger.info(f"üåê Starting Flask development server on port {PORT}")
    app.run(host='0.0.0.0', port=PORT, debug=False)

# Root endpoint is now registered BEFORE Flask-RESTX initialization to avoid conflicts

# Make Flask app available to Gunicorn
