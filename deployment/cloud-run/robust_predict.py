#!/usr/bin/env python3
"""
üöÄ EMOTION DETECTION API FOR CLOUD RUN
======================================
Robust Flask API optimized for Cloud Run deployment.
"""

import os
import time
import logging
from flask import Flask, request, jsonify
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from pathlib import Path

# Configure logging for Cloud Run
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# Global variables for model state
model = None
tokenizer = None
emotion_mapping = None
model_loading = False
model_loaded = False

# Emotion mapping based on training order
EMOTION_MAPPING = ['anxious', 'calm', 'content', 'excited', 'frustrated', 'grateful', 'happy', 'hopeful', 'overwhelmed', 'proud', 'sad', 'tired']

def load_model():
    """Load the emotion detection model"""
    global model, tokenizer, emotion_mapping, model_loading, model_loaded
    
    if model_loading or model_loaded:
        return
    
    model_loading = True
    logger.info("üîÑ Starting model loading...")
    
    try:
        # Get model path
        model_path = Path("/app/model")
        logger.info(f"üìÅ Loading model from: {model_path}")
        
        # Check if model files exist
        if not model_path.exists():
            raise FileNotFoundError(f"Model directory not found: {model_path}")
        
        # Load tokenizer and model
        logger.info("üì• Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained("roberta-base")
        
        logger.info("üì• Loading model...")
        model = AutoModelForSequenceClassification.from_pretrained(str(model_path))
        
        # Set device (CPU for Cloud Run)
        device = torch.device('cpu')
        model.to(device)
        model.eval()
        
        emotion_mapping = EMOTION_MAPPING
        model_loaded = True
        model_loading = False
        
        logger.info(f"‚úÖ Model loaded successfully on {device}")
        logger.info(f"üéØ Supported emotions: {emotion_mapping}")
        
    except Exception as e:
        model_loading = False
        logger.error(f"‚ùå Failed to load model: {e}")
        raise

def predict_emotion(text):
    """Predict emotion for given text"""
    global model, tokenizer, emotion_mapping
    
    if not model_loaded:
        raise RuntimeError("Model not loaded")
    
    # Tokenize
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
    
    # Predict
    with torch.no_grad():
        outputs = model(**inputs)
        probabilities = torch.softmax(outputs.logits, dim=1)
        predicted_class = torch.argmax(probabilities, dim=1).item()
        confidence = probabilities[0][predicted_class].item()
    
    # Map to emotion name
    emotion = emotion_mapping[predicted_class]
    
    return {
        "emotion": emotion,
        "confidence": confidence,
        "text": text
    }

@app.route('/', methods=['GET'])
def root():
    """Root endpoint"""
    return jsonify({
        "message": "Hello from SAMO Emotion Detection API!",
        "status": "running",
        "timestamp": time.time()
    })

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model_loaded,
        'model_loading': model_loading,
        'port': os.environ.get('PORT', '8080'),
        'timestamp': time.time()
    })

@app.route('/predict', methods=['POST'])
def predict():
    """Predict emotion for given text"""
    try:
        # Load model if not loaded
        if not model_loaded and not model_loading:
            load_model()
        
        if not model_loaded:
            return jsonify({'error': 'Model not loaded'}), 503
        
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400
        
        text = data.get('text', '')
        if not text:
            return jsonify({'error': 'No text provided'}), 400
        
        # Make prediction
        result = predict_emotion(text)
        return jsonify(result)
    
    except Exception:
        logger.exception("Prediction error")
        return jsonify({'error': 'Prediction processing failed. Please try again later.'}), 500

@app.route('/predict_batch', methods=['POST'])
def predict_batch():
    """Predict emotions for multiple texts"""
    try:
        # Load model if not loaded
        if not model_loaded and not model_loading:
            load_model()
        
        if not model_loaded:
            return jsonify({'error': 'Model not loaded'}), 503
        
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400
        
        texts = data.get('texts', [])
        if not texts:
            return jsonify({'error': 'No texts provided'}), 400
        
        # Make predictions
        results = []
        for text in texts:
            result = predict_emotion(text)
            results.append(result)
        
        return jsonify({'results': results})
    
    except Exception:
        logger.exception("Batch prediction error")
        return jsonify({'error': 'Batch prediction processing failed. Please try again later.'}), 500

@app.route('/emotions', methods=['GET'])
def get_emotions():
    """Get list of supported emotions"""
    return jsonify({
        'emotions': EMOTION_MAPPING,
        'count': len(EMOTION_MAPPING)
    })

@app.route('/model_status', methods=['GET'])
def model_status():
    """Get detailed model status"""
    return jsonify({
        'model_loaded': model_loaded,
        'model_loading': model_loading,
        'emotions': EMOTION_MAPPING if model_loaded else [],
        'device': 'cpu',
        'timestamp': time.time()
    })

# Load model on startup
def initialize_model():
    """Initialize model before first request"""
    try:
        load_model()
    except Exception:
        logger.exception("Failed to initialize model")

# Initialize model when module is imported
initialize_model()

if __name__ == '__main__':
    logger.info("üöÄ Starting SAMO Emotion Detection API")
    logger.info("=" * 50)
    logger.info("üìä Model Performance: 99.48% F1 Score")
    logger.info("üéØ Supported Emotions: %s", EMOTION_MAPPING)
    logger.info("üåê API Endpoints:")
    logger.info("  - GET  / - Root endpoint")
    logger.info("  - GET  /health - Health check")
    logger.info("  - POST /predict - Single text prediction")
    logger.info("  - POST /predict_batch - Batch prediction")
    logger.info("  - GET  /emotions - List emotions")
    logger.info("  - GET  /model_status - Model status")
    logger.info("=" * 50)
    
    # Load model immediately
    try:
        load_model()
    except Exception:
        logger.exception("Failed to load model on startup")
    
    # Get port from environment (Cloud Run requirement)
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port, debug=False) 