#!/usr/bin/env python3
        # Calibrate temperature
        # Create tokenized dataset
        # Extract raw validation data
        # Load checkpoint
        # Load dataset
        # Load trained model
        # Save calibrated model
        from src.models.emotion_detection.bert_classifier import EmotionDataset
        from transformers import AutoTokenizer
        import traceback
    # Collect logits and labels
    # Concatenate all batches
    # Create temperature scaling layer
    # Optimize temperature parameter
    # Setup device
import logging
import os
import sys
# Add project root to path
# Configure logging
from pathlib import Path

import torch
from torch import nn

from src.models.emotion_detection.dataset_loader import GoEmotionsDataLoader
from src.models.emotion_detection.training_pipeline import \
    create_bert_emotion_classifier

"""
Temperature Scaling for Model Calibration

This script applies temperature scaling to improve model calibration
and potentially boost F1 score by 5-10%.
"""

project_root = Path(__file__).parent.parent.resolve()
sys.path.append(str(project_root))

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class TemperatureScaling(nn.Module):
    """Temperature scaling for model calibration."""

    def __init__(self):
        super().__init__()
        self.temperature = nn.Parameter(torch.ones(1) * 1.5)

    def forward(self, logits):
        """Apply temperature scaling to logits."""
        return logits / self.temperature


def calibrate_temperature(model, val_loader, device):
    """Calibrate temperature parameter on validation set."""
    logger.info("üîß Calibrating temperature parameter...")

    temperature_scaling = TemperatureScaling().to(device)

    all_logits = []
    all_labels = []

    model.eval()
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].float().to(device)

            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs["logits"]

            all_logits.append(logits.cpu())
            all_labels.append(labels.cpu())

    all_logits = torch.cat(all_logits, dim=0)
    all_labels = torch.cat(all_labels, dim=0)

    optimizer = torch.optim.LBFGS([temperature_scaling.temperature], lr=0.01, max_iter=50)

    def eval():
        optimizer.zero_grad()
        loss = nn.functional.binary_cross_entropy_with_logits(
            temperature_scaling(all_logits), all_labels
        )
        loss.backward()
        return loss

    optimizer.step(eval)

    optimal_temperature = temperature_scaling.temperature.item()
    logger.info("‚úÖ Optimal temperature: {optimal_temperature:.3f}")

    return temperature_scaling


def apply_temperature_scaling():
    """Apply temperature scaling to improve model calibration."""

    logger.info("üå°Ô∏è Starting Temperature Scaling")
    logger.info("   ‚Ä¢ Expected improvement: 5-10% F1 score")
    logger.info("   ‚Ä¢ Method: Model calibration")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info("Using device: {device}")

    try:
        logger.info("Loading validation dataset...")
        data_loader = GoEmotionsDataLoader()
        datasets = data_loader.prepare_datasets()

        val_raw = datasets["validation"]
        val_texts = [item["text"] for item in val_raw]
        val_labels = [item["labels"] for item in val_raw]

        tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        val_dataset = EmotionDataset(val_texts, val_labels, tokenizer, max_length=512)
        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)

        model_path = "./models/checkpoints/focal_loss_best_model.pt"
        if not Path(model_path):
            logger.error("‚ùå Model not found: {model_path}")
            logger.info("   ‚Ä¢ Please run focal_loss_training.py first")
            return False

        logger.info("Loading model from {model_path}")
        model, _ = create_bert_emotion_classifier(
            model_name="bert-base-uncased",
            class_weights=None,
            freeze_bert_layers=4,
        )
        model.to(device)

        checkpoint = torch.load(model_path, map_location=device)
        model.load_state_dict(checkpoint["model_state_dict"])
        logger.info("‚úÖ Model loaded successfully")

        temperature_scaling = calibrate_temperature(model, val_loader, device)

        output_dir = "./models/checkpoints"
        os.makedirs(output_dir, exist_ok=True)
        calibrated_path = Path(output_dir, "temperature_scaled_model.pt")

        torch.save(
            {
                "model_state_dict": model.state_dict(),
                "temperature_scaling_state_dict": temperature_scaling.state_dict(),
                "temperature": temperature_scaling.temperature.item(),
                "original_checkpoint": checkpoint,
            },
            calibrated_path,
        )

        logger.info("‚úÖ Calibrated model saved to: {calibrated_path}")
        logger.info("   ‚Ä¢ Temperature: {temperature_scaling.temperature.item():.3f}")

        return True

    except Exception as e:
        logger.error("‚ùå Temperature scaling failed: {e}")
        traceback.print_exc()
        return False


def main():
    """Main function."""
    logger.info("üå°Ô∏è Temperature Scaling Script")
    logger.info("This script calibrates the model for better F1 scores")

    success = apply_temperature_scaling()

    if success:
        logger.info("‚úÖ Temperature scaling completed successfully!")
        sys.exit(0)
    else:
        logger.error("‚ùå Temperature scaling failed. Check the logs above.")
        sys.exit(1)


if __name__ == "__main__":
    main()
