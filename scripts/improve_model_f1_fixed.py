                # Test loading the checkpoint
        # Additional training with focal loss
        # Apply class weights if provided
        # Calculate binary cross entropy loss
        # Calculate class weights
        # Calculate focal loss
        # Calculate focal weight
        # Check if target achieved
        # Check if target achieved
        # Convert logits to probabilities
        # Create focal loss
        # Create or load model
        # Create trainer for focal loss fine-tuning
        # Evaluate final model
        # For now, save the best individual model
        # IMPORTANT: Disable dev mode to use full dataset
        # Load dataset
        # Model 1: Standard configuration
        # Model 2: Different learning rate
        # Model 3: With focal loss
        # Note: This will be handled in the trainer initialization
        # Save model
        # Save model
        # Simple ensemble prediction (average of predictions)
        # Train fresh model with extended epochs and full dataset
        # Train multiple models with different configurations
    # Apply selected technique
    # Create data loader
    # Create model with optimal settings
    # Create trainer with development mode disabled for better results
    # Evaluate
    # Find valid checkpoint (if any)
    # Report results
    # Set device
    # Train model on full dataset
    # Update output path
# Add src to path
# Configure logging
# Constants
#!/usr/bin/env python3
from pathlib import Path
from src.models.emotion_detection.bert_classifier import create_bert_emotion_classifier
from src.models.emotion_detection.dataset_loader import GoEmotionsDataLoader
from src.models.emotion_detection.training_pipeline import EmotionDetectionTrainer
from torch import nn
from typing import Optional
import argparse
import logging
import sys
import time
import torch
import torch.nn.functional as F




"""
Fixed F1 Score Improvement Script

This script fixes the checkpoint loading issues and implements F1 score improvement
techniques that can work with or without existing checkpoints.

Usage:
    python scripts/improve_model_f1_fixed.py [--technique TECHNIQUE] [--output_model PATH]

Arguments:
    --technique: Improvement technique to apply (ensemble, focal_loss, full_training)
    --output_model: Path to save improved model
"""

sys.path.append(str(Path(__file__).parent.parent.resolve()))
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

DEFAULT_OUTPUT_MODEL = "models/checkpoints/bert_emotion_classifier_improved_fixed.pt"
CHECKPOINT_PATHS = [
    "models/checkpoints/bert_emotion_classifier_final.pt",
    "test_checkpoints/best_model.pt",
    "test_checkpoints_dev/best_model.pt",
]
OPTIMAL_TEMPERATURE = 1.0
OPTIMAL_THRESHOLD = 0.6


class FocalLoss(nn.Module):
    """Focal Loss for handling class imbalance."""

    def __init__(self, gamma: float = 2.0, alpha: Optional[torch.Tensor] = None):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        probs = torch.sigmoid(inputs)

        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction="none")

        p_t = probs * targets + (1 - probs) * (1 - targets)
        focal_weight = (1 - p_t) ** self.gamma

        if self.alpha is not None:
            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
            focal_weight = alpha_t * focal_weight

        focal_loss = focal_weight * bce_loss
        return focal_loss.mean()


def find_valid_checkpoint() -> Optional[str]:
    """Find a valid checkpoint file that can be loaded."""
    for checkpoint_path in CHECKPOINT_PATHS:
        path = Path(checkpoint_path)
        if path.exists():
            try:
                checkpoint = torch.load(path, map_location="cpu", weights_only=False)
                if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
                    logger.info("‚úÖ Found valid checkpoint: {checkpoint_path}")
                    return str(path)
                else:
                    logger.warning("‚ö†Ô∏è Checkpoint {checkpoint_path} has unexpected format")
            except Exception:
                logger.warning("‚ö†Ô∏è Checkpoint {checkpoint_path} is corrupted: {e}")

    logger.warning("No valid checkpoint found. Will train from scratch.")
    return None


def train_fresh_model(epochs: int = 3, batch_size: int = 16) -> tuple[nn.Module, dict]:
    """Train a fresh model from scratch with optimal settings."""
    logger.info("üöÄ Training fresh model from scratch...")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info("Using device: {device}")

    data_loader = GoEmotionsDataLoader()
    data_loader.download_dataset()
    datasets = data_loader.prepare_datasets()

    model, loss_fn = create_bert_emotion_classifier(
        freeze_bert_layers=4  # Less freezing for better learning
    )

    trainer = EmotionDetectionTrainer(
        model=model,
        loss_fn=loss_fn,
        learning_rate=2e-5,
        batch_size=batch_size,
        num_epochs=epochs,
        device=device,
        checkpoint_dir=Path("models/checkpoints"),
        early_stopping_patience=3,
    )

    logger.info("Training model for {epochs} epochs with batch_size={batch_size}")
    trainer.train(datasets["train"], datasets["validation"])

    metrics = trainer.evaluate(datasets["test"])

    logger.info(
        "Fresh model results - Micro F1: {metrics['micro_f1']:.4f}, Macro F1: {metrics['macro_f1']:.4f}"
    )

    return model, metrics


def improve_with_focal_loss(checkpoint_path: Optional[str] = None) -> bool:
    """Improve model F1 score using Focal Loss."""
    try:
        logger.info("üéØ Improving model with Focal Loss...")

        data_loader = GoEmotionsDataLoader()
        data_loader.download_dataset()
        datasets = data_loader.prepare_datasets()

        class_weights = data_loader.compute_class_weights()
        class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)

        if checkpoint_path and Path(checkpoint_path).exists():
            logger.info("Loading model from checkpoint: {checkpoint_path}")
            model, _ = create_bert_emotion_classifier()
            checkpoint = torch.load(checkpoint_path, map_location="cpu", weights_only=False)
            model.load_state_dict(checkpoint["model_state_dict"])
        else:
            logger.info("Training fresh model with Focal Loss...")
            model, initial_metrics = train_fresh_model(epochs=5, batch_size=32)
            logger.info("Fresh model baseline - F1: {initial_metrics.get('micro_f1', 0):.4f}")

        focal_loss = FocalLoss(gamma=2.0, alpha=class_weights_tensor)

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)

        trainer = EmotionDetectionTrainer(
            model=model,
            loss_fn=focal_loss,
            learning_rate=1e-5,  # Lower learning rate for fine-tuning
            batch_size=32,
            num_epochs=3,
            device=device,
            checkpoint_dir=Path("models/checkpoints"),
            early_stopping_patience=2,
        )

        logger.info("Fine-tuning with Focal Loss...")
        trainer.train(datasets["train"], datasets["validation"])

        metrics = trainer.evaluate(datasets["test"])

        logger.info(
            "Focal Loss results - Micro F1: {metrics['micro_f1']:.4f}, Macro F1: {metrics['macro_f1']:.4f}"
        )

        output_path = Path(DEFAULT_OUTPUT_MODEL)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        torch.save(
            {
                "model_state_dict": model.state_dict(),
                "technique": "focal_loss",
                "metrics": metrics,
                "temperature": OPTIMAL_TEMPERATURE,
                "threshold": OPTIMAL_THRESHOLD,
            },
            output_path,
        )

        logger.info("‚úÖ Focal Loss model saved to {output_path}")

        if metrics["micro_f1"] >= 0.75:
            logger.info("üéâ Target F1 score of 75% achieved!")
        else:
            logger.info("üìä Current F1: {metrics['micro_f1']:.1%}, Target: 75%")

        return True

    except Exception:
        logger.error("‚ùå Error improving model with Focal Loss: {e}")
        return False


def improve_with_full_training() -> bool:
    """Improve model with full dataset training and optimal settings."""
    try:
        logger.info("üöÄ Training model with full dataset and optimal settings...")

        model, metrics = train_fresh_model(epochs=8, batch_size=32)

        output_path = Path(DEFAULT_OUTPUT_MODEL)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        torch.save(
            {
                "model_state_dict": model.state_dict(),
                "technique": "full_training",
                "metrics": metrics,
                "temperature": OPTIMAL_TEMPERATURE,
                "threshold": OPTIMAL_THRESHOLD,
            },
            output_path,
        )

        logger.info("‚úÖ Full training model saved to {output_path}")

        if metrics["micro_f1"] >= 0.75:
            logger.info("üéâ Target F1 score of 75% achieved!")
        else:
            logger.info("üìä Current F1: {metrics['micro_f1']:.1%}, Target: 75%")

        return True

    except Exception:
        logger.error("‚ùå Error with full training: {e}")
        return False


def create_simple_ensemble(checkpoint_path: Optional[str] = None) -> bool:
    """Create a simple ensemble without requiring multiple pre-trained models."""
    try:
        logger.info("üé≠ Creating simple ensemble approach...")

        models = []

        logger.info("Training ensemble model 1/3 (standard config)...")
        model1, metrics1 = train_fresh_model(epochs=4, batch_size=32)
        models.append((model1, metrics1))

        logger.info("Training ensemble model 2/3 (different learning rate)...")
        model2, metrics2 = train_fresh_model(epochs=4, batch_size=16)
        models.append((model2, metrics2))

        logger.info("Training ensemble model 3/3 (focal loss)...")
        model3, _ = improve_with_focal_loss()

        best_model = max(models, key=lambda x: x[1].get("micro_f1", 0))

        output_path = Path(DEFAULT_OUTPUT_MODEL)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        torch.save(
            {
                "model_state_dict": best_model[0].state_dict(),
                "technique": "simple_ensemble_best",
                "metrics": best_model[1],
                "temperature": OPTIMAL_TEMPERATURE,
                "threshold": OPTIMAL_THRESHOLD,
            },
            output_path,
        )

        logger.info("‚úÖ Best ensemble model saved to {output_path}")
        logger.info("Best F1 score: {best_model[1].get('micro_f1', 0):.4f}")

        return True

    except Exception:
        logger.error("‚ùå Error creating ensemble: {e}")
        return False


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Fixed F1 improvement script")
    parser.add_argument(
        "--technique",
        type=str,
        choices=["focal_loss", "full_training", "ensemble"],
        default="focal_loss",
        help="Improvement technique to apply",
    )
    parser.add_argument(
        "--output_model",
        type=str,
        default=DEFAULT_OUTPUT_MODEL,
        help="Path to save improved model (default: {DEFAULT_OUTPUT_MODEL})",
    )

    args = parser.parse_args()

    DEFAULT_OUTPUT_MODEL = args.output_model

    logger.info("üéØ Starting F1 improvement with technique: {args.technique}")

    checkpoint_path = find_valid_checkpoint()

    start_time = time.time()

    if args.technique == "focal_loss":
        success = improve_with_focal_loss(checkpoint_path)
    elif args.technique == "full_training":
        success = improve_with_full_training()
    elif args.technique == "ensemble":
        success = create_simple_ensemble(checkpoint_path)
    else:
        logger.error("Unknown technique: {args.technique}")
        success = False

    duration = time.time() - start_time
    if success:
        logger.info("‚úÖ F1 improvement completed successfully in {duration:.1f}s")
        logger.info("Model saved to: {args.output_model}")
    else:
        logger.error("‚ùå F1 improvement failed after {duration:.1f}s")

    sys.exit(0 if success else 1)
