#!/usr/bin/env python3
"""
Test Secure Model Loader
=======================

This script tests the secure model loader to ensure it can load the model
without security vulnerabilities.
"""

import os
import sys
from pathlib import Path

# Add the scripts directory to the path so we can import our modules
sys.path.insert(0, str(Path(__file__).parent))

def test_secure_model_loader():
    """Test the secure model loader functionality."""
    
    print("üîí Testing Secure Model Loader...")
    print("=" * 50)
    
    # Test 1: Check if model directory exists
    print("\n1. Checking model directory...")
    model_path = "deployment/models/default"
    if not os.path.exists(model_path):
        print(f"‚ùå Model directory not found: {model_path}")
        return False
    
    print(f"‚úÖ Model directory found: {model_path}")
    
    # Test 2: Check required files
    print("\n2. Checking required model files...")
    required_files = ['config.json', 'model.safetensors', 'tokenizer.json', 'vocab.json']
    missing_files = []
    
    for file in required_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            print(f"‚úÖ Found: {file}")
        else:
            print(f"‚ùå Missing: {file}")
            missing_files.append(file)
    
    if missing_files:
        print(f"‚ùå Missing required files: {missing_files}")
        return False
    
    # Test 3: Test secure model loader
    print("\n3. Testing secure model loader...")
    try:
        from secure_model_loader import load_emotion_model_securely
        
        print("   Loading model securely...")
        tokenizer, model = load_emotion_model_securely(model_path)
        
        print("‚úÖ Model loaded successfully!")
        print(f"   Tokenizer type: {type(tokenizer).__name__}")
        print(f"   Model type: {type(model).__name__}")
        print(f"   Model config: {model.config.model_type}")
        print(f"   Number of labels: {model.config.num_labels}")
        
        # Test 4: Test prediction
        print("\n4. Testing prediction...")
        test_text = "I am feeling happy today!"
        
        import torch
        inputs = tokenizer(test_text, return_tensors='pt', truncation=True, padding=True)
        with torch.no_grad():
            outputs = model(**inputs)
            probabilities = torch.softmax(outputs.logits, dim=1)
            predicted_label = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0][predicted_label].item()
        
        if predicted_label in model.config.id2label:
            predicted_emotion = model.config.id2label[predicted_label]
        else:
            predicted_emotion = f"unknown_{predicted_label}"
        
        print("‚úÖ Prediction successful!")
        print(f"   Input text: '{test_text}'")
        print(f"   Predicted emotion: {predicted_emotion}")
        print(f"   Confidence: {confidence:.3f}")
        
        return True
        
    except ImportError as e:
        print(f"‚ùå Import error: {e}")
        return False
    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        return False

def main():
    """Main function to run the tests."""
    print("üîí SAMO-DL Secure Model Loader Test")
    print("=" * 50)
    
    success = test_secure_model_loader()
    
    if success:
        print("\n" + "=" * 50)
        print("üéâ Secure model loader test completed successfully!")
        print("‚úÖ Model can be loaded securely")
        print("‚úÖ Predictions are working")
        print("‚úÖ Security measures are active")
        print("\nüìã Next Steps:")
        print("1. ‚úÖ Security dependencies installed")
        print("2. ‚úÖ Secure model loader tested")
        print("3. üîÑ Test secure API server")
        print("4. üîÑ Address remaining security vulnerabilities")
        print("5. üöÄ Deploy secure model to GCP/Vertex AI")
        sys.exit(0)
    else:
        print("\n‚ùå Tests failed. Please check the errors above.")
        sys.exit(1)

if __name__ == "__main__":
    main() 