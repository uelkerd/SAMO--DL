# HuggingFace Inference Endpoints Configuration
# Best for: Production, consistent latency, high throughput

HF_TOKEN=your_hf_token_here
MODEL_NAME=$REPO_NAME
DEPLOYMENT_TYPE=endpoint
INFERENCE_ENDPOINT_URL=https://your-endpoint-id.us-east-1.aws.endpoints.huggingface.cloud

# Optional settings
MAX_RETRIES=3
TIMEOUT_SECONDS=10