#!/usr/bin/env python3
"""
üîç Test Code Review Fixes V3
============================
Comprehensive validation of the latest code review fixes:
1. Regex-based model_utils.py updates (replacing brittle string replacement)
2. DataParallel checkpoint handling ('module.' prefix stripping)
3. Non-contiguous id2label keys handling
4. Unused imports and legacy code cleanup
5. Test path expansion with actual directory creation
6. Timeout handling in API server
7. PII exposure prevention in error responses
"""

import os
import sys
import json
import tempfile
import ast
import re

def test_regex_based_model_replacement():
    """Test that the model replacement now uses robust regex patterns."""
    print("üîß Testing regex-based model replacement functionality...")
    
    # Simulate the regex patterns from our fixed code
    tokenizer_pattern = r'AutoTokenizer\.from_pretrained\s*\(\s*[\'"][^\'\"]+[\'"]\s*\)'
    model_pattern = r'AutoModelForSequenceClassification\.from_pretrained\s*\(\s*[\'"][^\'\"]+[\'"]'
    
    # Test various formatting scenarios
    test_cases = [
        # Standard formatting
        "AutoTokenizer.from_pretrained('distilroberta-base')",
        "AutoTokenizer.from_pretrained(\"distilroberta-base\")",
        
        # Extra whitespace
        "AutoTokenizer.from_pretrained(  'distilroberta-base'  )",
        "AutoTokenizer.from_pretrained(\n    'distilroberta-base'\n)",
        
        # Model cases
        "AutoModelForSequenceClassification.from_pretrained('distilroberta-base'",
        "AutoModelForSequenceClassification.from_pretrained(  \"distilroberta-base\"",
    ]
    
    repo_name = "user/test-model"
    tokenizer_replacement = f"AutoTokenizer.from_pretrained('{repo_name}')"
    model_replacement = f"AutoModelForSequenceClassification.from_pretrained('{repo_name}'"
    
    successes = 0
    for i, test_case in enumerate(test_cases):
        print(f"  Test case {i+1}: {test_case[:50]}...")
        
        if "AutoTokenizer" in test_case:
            result = re.sub(tokenizer_pattern, tokenizer_replacement, test_case)
            expected = tokenizer_replacement
        else:
            result = re.sub(model_pattern, model_replacement, test_case)
            expected = model_replacement
        
        if expected in result:
            print("    ‚úÖ Regex replacement successful")
            successes += 1
        else:
            print(f"    ‚ùå Regex replacement failed: {result}")
    
    print(f"  Regex patterns successful: {successes}/{len(test_cases)}")
    return successes == len(test_cases)

def test_config_file_creation():
    """Test that config file creation works properly."""
    print("üîß Testing configuration file creation...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        config_path = os.path.join(temp_dir, "deployment", "custom_model_config.json")
        
        # Simulate the config creation logic
        deployment_dir = os.path.dirname(config_path)
        os.makedirs(deployment_dir, exist_ok=True)
        
        config_data = {
            "model_repository": "test-user/test-model",
            "deployment_type": "huggingface_hub",
            "updated_at": "test-timestamp"
        }
        
        with open(config_path, 'w') as f:
            json.dump(config_data, f, indent=2)
        
        # Validate
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                loaded_config = json.load(f)
            
            if loaded_config["model_repository"] == "test-user/test-model":
                print("  ‚úÖ Config file creation successful")
                return True
            print("  ‚ùå Config file content incorrect")
            return False
        else:
            print("  ‚ùå Config file not created")
            return False

def test_dataparallel_checkpoint_handling():
    """Test DataParallel checkpoint key stripping."""
    print("üîß Testing DataParallel checkpoint handling...")
    
    # Simulate a DataParallel checkpoint
    dataparallel_state_dict = {
        "module.classifier.weight": "tensor_data_1",
        "module.classifier.bias": "tensor_data_2", 
        "module.roberta.embeddings.word_embeddings.weight": "tensor_data_3",
        "regular_key": "tensor_data_4"  # Non-module key
    }
    
    # Simulate the cleaning logic from our fix
    if any(key.startswith('module.') for key in dataparallel_state_dict):
        print("  üîß Detected DataParallel checkpoint - testing key cleaning...")
        clean_state_dict = {}
        for key, value in dataparallel_state_dict.items():
            new_key = key[7:] if key.startswith('module.') else key
            clean_state_dict[new_key] = value
        
        expected_keys = {
            "classifier.weight",
            "classifier.bias", 
            "roberta.embeddings.word_embeddings.weight",
            "regular_key"
        }
        
        if set(clean_state_dict.keys()) == expected_keys:
            print(f"  ‚úÖ DataParallel key cleaning successful: {len(clean_state_dict)} keys cleaned")
            return True
        print(f"  ‚ùå Key cleaning failed. Got: {set(clean_state_dict.keys())}")
        return False
    else:
        print("  ‚ùå DataParallel detection failed")
        return False

def test_non_contiguous_id2label_handling():
    """Test robust id2label key handling."""
    print("üîß Testing non-contiguous id2label handling...")
    
    test_cases = [
        # Non-contiguous integer keys
        {"0": "happy", "2": "sad", "5": "angry"},
        
        # String integer keys
        {"0": "joy", "1": "sadness", "2": "fear"},
        
        # Mixed/problematic keys that need fallback
        {"label_a": "happy", "label_b": "sad", "label_c": "angry"}
    ]
    
    successes = 0
    for i, id2label in enumerate(test_cases):
        print(f"  Test case {i+1}: {id2label}")
        
        try:
            # Simulate the robust handling logic from our fix
            int_keys = []
            for key in id2label.keys():
                if isinstance(key, str):
                    int_keys.append(int(key))
                else:
                    int_keys.append(key)
            
            int_keys.sort()
            sorted_labels = [id2label[str(key)] for key in int_keys]
            print(f"    ‚úÖ Numeric sorting successful: {sorted_labels}")
            successes += 1
            
        except (ValueError, TypeError):
            # Fallback to alphabetical sorting
            print("    üîÑ Falling back to alphabetical sorting...")
            sorted_keys = sorted(id2label.keys())
            sorted_labels = [id2label[key] for key in sorted_keys]
            print(f"    ‚úÖ Alphabetical sorting successful: {sorted_labels}")
            successes += 1
        
        except Exception as e:
            print(f"    ‚ùå Both sorting methods failed: {e}")
    
    print(f"  id2label handling successful: {successes}/{len(test_cases)}")
    return successes == len(test_cases)

def test_unused_imports_cleanup():
    """Test that unused imports were properly removed."""
    print("üîß Testing unused imports cleanup...")
    
    upload_script = "scripts/deployment/upload_model_to_huggingface.py"
    if not os.path.exists(upload_script):
        print(f"  ‚ùå Script not found: {upload_script}")
        return False
    
    with open(upload_script, 'r') as f:
        content = f.read()
    
    # Check for improvements
    improvements = []
    
    # Check that duplicate sys import is removed
    sys_import_count = content.count("import sys")
    if sys_import_count <= 1:  # Should be 0 now, but allow 1 for safety
        improvements.append("Duplicate sys import removed")
    
    # Check that legacy version check is removed
    if "sys.version_info" not in content:
        improvements.append("Legacy version check removed")
    
    # Check that Any import is present
    if "from typing import" in content and "Any" in content:
        improvements.append("Any import added properly")
    
    # Check syntax validity
    try:
        ast.parse(content)
        improvements.append("File syntax remains valid")
    except SyntaxError as e:
        print(f"  ‚ùå Syntax error after cleanup: {e}")
        return False
    
    print(f"  ‚úÖ Import cleanup improvements: {len(improvements)}")
    for improvement in improvements:
        print(f"    ‚Ä¢ {improvement}")
    
    return len(improvements) >= 3

def test_path_expansion_fix():
    """Test that path expansion now properly handles directory creation."""
    print("üîß Testing path expansion fix with temporary directories...")
    
    test_script = "scripts/deployment/test_model_path_detection.py"
    if not os.path.exists(test_script):
        print(f"  ‚ùå Test script not found: {test_script}")
        return False
    
    with open(test_script, 'r') as f:
        content = f.read()
    
    # Check that the fix uses TemporaryDirectory
    if "tempfile.TemporaryDirectory" in content:
        print("  ‚úÖ Uses TemporaryDirectory for isolated testing")
        
        # Check that it creates actual directory structure
        if "os.makedirs(test_projects_dir, exist_ok=True)" in content:
            print("  ‚úÖ Creates actual directory structure before testing")
            
            # Check that it validates directory existence
            if "os.path.exists" in content:
                print("  ‚úÖ Validates directory existence in output")
                return True
    
    print("  ‚ùå Path expansion fix not properly implemented")
    return False

def test_api_timeout_handling():
    """Test that API server now has proper timeout handling."""
    print("üîß Testing API server timeout handling...")
    
    api_server = "deployment/flexible_api_server.py"
    if not os.path.exists(api_server):
        print(f"  ‚ùå API server not found: {api_server}")
        return False
    
    with open(api_server, 'r') as f:
        content = f.read()
    
    # Check for explicit timeout handling
    timeout_patterns = [
        "except requests.exceptions.Timeout:",
        "Request timeout (endpoint may be starting up)",
        "Try again in a few seconds"
    ]
    
    timeout_checks = []
    for pattern in timeout_patterns:
        if pattern in content:
            timeout_checks.append(f"Contains: {pattern}")
    
    print(f"  ‚úÖ Timeout handling patterns found: {len(timeout_checks)}/{len(timeout_patterns)}")
    for check in timeout_checks:
        print(f"    ‚Ä¢ {check}")
    
    return len(timeout_checks) == len(timeout_patterns)

def test_pii_exposure_prevention():
    """Test that PII exposure has been prevented in error responses."""
    print("üîß Testing PII exposure prevention...")
    
    api_server = "deployment/flexible_api_server.py"
    if not os.path.exists(api_server):
        print(f"  ‚ùå API server not found: {api_server}")
        return False
    
    with open(api_server, 'r') as f:
        content = f.read()
    
    # Find all error response patterns
    error_patterns = [
        r'"error":\s*[^}]+\}',  # Error responses
        r'"error":[^,}]+,',     # Error fields in responses
    ]
    
    pii_exposures = []
    
    # Look for "text": text in error contexts
    lines = content.split('\n')
    for i, line in enumerate(lines):
        if '"text": text' in line:
            # Check surrounding context for error indicators
            context_start = max(0, i-5)
            context_end = min(len(lines), i+5)
            context = ' '.join(lines[context_start:context_end])
            
            if any(error_indicator in context.lower() for error_indicator in ['error', 'exception', 'failed', 'timeout']):
                # Check if this is actually a successful response (should contain emotion)
                if '"emotion"' not in context:
                    pii_exposures.append(f"Line {i+1}: {line.strip()}")
    
    # Check for redacted logging
    redacted_logging = content.count("text_preview")
    
    print(f"  PII exposures found: {len(pii_exposures)}")
    print(f"  Redacted logging instances: {redacted_logging}")
    
    for exposure in pii_exposures:
        print(f"    ‚ùå {exposure}")
    
    if len(pii_exposures) == 0 and redacted_logging >= 2:
        print("  ‚úÖ PII exposure prevention successful")
        return True
    print("  ‚ö†Ô∏è  PII exposure issues may remain")
    return False

def test_syntax_validation():
    """Test that all modified files still have valid syntax."""
    print("üîß Testing syntax validation of all modified files...")
    
    files_to_check = [
        "scripts/deployment/upload_model_to_huggingface.py",
        "scripts/deployment/test_model_path_detection.py",
        "deployment/flexible_api_server.py"
    ]
    
    valid_files = 0
    for file_path in files_to_check:
        print(f"  Checking {file_path}...")
        
        if not os.path.exists(file_path):
            print("    ‚ùå File not found")
            continue
        
        try:
            with open(file_path, 'r') as f:
                content = f.read()
            
            ast.parse(content)
            print("    ‚úÖ Valid Python syntax")
            valid_files += 1
            
        except SyntaxError as e:
            print(f"    ‚ùå Syntax error: {e}")
        except Exception as e:
            print(f"    ‚ùå Error reading file: {e}")
    
    print(f"  Valid files: {valid_files}/{len(files_to_check)}")
    return valid_files == len(files_to_check)

def main():
    """Run all code review fix validation tests."""
    print("üîç TESTING CODE REVIEW FIXES V3")
    print("=" * 60)
    print("Comprehensive validation of latest code review improvements:")
    print("1. Regex-based model replacement (replacing brittle string replacement)")
    print("2. DataParallel checkpoint handling ('module.' prefix stripping)")
    print("3. Non-contiguous id2label keys handling")
    print("4. Unused imports and legacy code cleanup")
    print("5. Test path expansion with actual directory creation")
    print("6. Timeout handling in API server")
    print("7. PII exposure prevention in error responses")
    print("=" * 60)
    
    tests = [
        ("Regex-based Model Replacement", test_regex_based_model_replacement),
        ("Config File Creation", test_config_file_creation),
        ("DataParallel Checkpoint Handling", test_dataparallel_checkpoint_handling),
        ("Non-contiguous id2label Handling", test_non_contiguous_id2label_handling),
        ("Unused Imports Cleanup", test_unused_imports_cleanup),
        ("Path Expansion Fix", test_path_expansion_fix),
        ("API Timeout Handling", test_api_timeout_handling),
        ("PII Exposure Prevention", test_pii_exposure_prevention),
        ("Syntax Validation", test_syntax_validation),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå {test_name} failed with exception: {e}")
            results.append((test_name, False))
        print()  # Add spacing between tests
    
    print("üéØ CODE REVIEW FIXES V3 VALIDATION SUMMARY")
    print("=" * 60)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"  {status}: {test_name}")
    
    print(f"\nTests passed: {passed}/{total}")
    
    if passed == total:
        print("\nüéâ ALL CODE REVIEW FIXES V3 SUCCESSFULLY IMPLEMENTED!")
        print("üìã Summary of improvements:")
        print("  ‚úÖ Robust regex-based model replacement (no more brittle string matching)")
        print("  ‚úÖ DataParallel checkpoint compatibility ('module.' prefix handling)")
        print("  ‚úÖ Robust id2label handling (non-contiguous & string keys)")
        print("  ‚úÖ Clean imports (removed duplicates & legacy version checks)")  
        print("  ‚úÖ Reliable path expansion testing (actual directory creation)")
        print("  ‚úÖ Comprehensive API timeout handling (parity with serverless)")
        print("  ‚úÖ PII exposure prevention (no user input in error responses)")
        print("  ‚úÖ All syntax remains valid and functional")
        print("\nüõ°Ô∏è Security, robustness, and maintainability significantly enhanced!")
        return True
    print(f"\n‚ö†Ô∏è {total - passed} test(s) failed - review implementation")
    return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)