import os
import sys
import traceback

#!/usr/bin/env python3
"""
Minimal Working Training Script
Uses only working modules to avoid environment issues
"""

import logging
import torch
from torch import nn

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class SimpleBERTClassifier(nn.Module):
    """Simple BERT classifier for emotion detection."""

    def __init__(self, num_classes=28, model_name="bert-base-uncased"):
        super().__init__()
        from transformers import AutoModel, AutoTokenizer

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return {"logits": logits}


class FocalLoss(nn.Module):
    """Focal Loss for handling class imbalance."""

    def __init__(self, alpha=0.25, gamma=2.0, reduction="mean"):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction="none")
        pt = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss

        if self.reduction == "mean":
            return focal_loss.mean()
        elif self.reduction == "sum":
            return focal_loss.sum()
        else:
            return focal_loss


def create_synthetic_data(num_samples=1000, seq_length=128):
    """Create synthetic training data to avoid dataset loading issues."""
    logger.info("Creating synthetic data: {num_samples} samples")

    # Create random input data
    input_ids = torch.randint(0, 30522, (num_samples, seq_length))  # BERT vocab size
    attention_mask = torch.ones(num_samples, seq_length)
    labels = torch.randint(0, 2, (num_samples, 28)).float()  # 28 emotion classes

    return input_ids, attention_mask, labels


def train_minimal_model():
    """Train a minimal BERT model with synthetic data."""

    logger.info("ðŸš€ Starting Minimal Working Training")
    logger.info("   â€¢ Using only working modules (PyTorch, NumPy, Transformers)")
    logger.info("   â€¢ Synthetic data to avoid dataset loading issues")
    logger.info("   â€¢ Focal Loss for class imbalance")

    # Setup device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info("Using device: {device}")

    try:
        # Create model
        logger.info("Creating BERT model...")
        model = SimpleBERTClassifier(num_classes=28)
        model.to(device)

        # Create focal loss
        focal_loss = FocalLoss(alpha=0.25, gamma=2.0)

        # Setup optimizer
        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

        # Create synthetic data
        train_input_ids, train_attention_mask, train_labels = create_synthetic_data(1000)
        val_input_ids, val_attention_mask, val_labels = create_synthetic_data(200)

        # Training loop
        best_val_loss = float("in")
        training_history = []

        for epoch in range(3):  # Quick 3 epochs
            logger.info("\nEpoch {epoch + 1}/3")

            # Training phase
            model.train()
            train_loss = 0.0
            num_batches = 0

            # Create mini-batches
            batch_size = 16
            for i in range(0, len(train_input_ids), batch_size):
                batch_input_ids = train_input_ids[i : i + batch_size].to(device)
                batch_attention_mask = train_attention_mask[i : i + batch_size].to(device)
                batch_labels = train_labels[i : i + batch_size].to(device)

                optimizer.zero_grad()

                # Forward pass
                outputs = model(batch_input_ids, attention_mask=batch_attention_mask)
                loss = focal_loss(outputs["logits"], batch_labels)

                # Backward pass
                loss.backward()
                optimizer.step()

                train_loss += loss.item()
                num_batches += 1

                # Log progress every 10 batches
                if num_batches % 10 == 0:
                    logger.info("   â€¢ Batch {num_batches}: Loss = {loss.item():.4f}")

            avg_train_loss = train_loss / num_batches

            # Validation phase
            model.eval()
            val_loss = 0.0
            val_batches = 0

            with torch.no_grad():
                for i in range(0, len(val_input_ids), batch_size):
                    batch_input_ids = val_input_ids[i : i + batch_size].to(device)
                    batch_attention_mask = val_attention_mask[i : i + batch_size].to(device)
                    batch_labels = val_labels[i : i + batch_size].to(device)

                    outputs = model(batch_input_ids, attention_mask=batch_attention_mask)
                    loss = focal_loss(outputs["logits"], batch_labels)

                    val_loss += loss.item()
                    val_batches += 1

            avg_val_loss = val_loss / val_batches

            # Log progress
            logger.info("   â€¢ Train Loss: {avg_train_loss:.4f}")
            logger.info("   â€¢ Val Loss: {avg_val_loss:.4f}")

            training_history.append(
                {"epoch": epoch + 1, "train_loss": avg_train_loss, "val_loss": avg_val_loss}
            )

            # Save best model
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                logger.info("   â€¢ New best validation loss: {best_val_loss:.4f}")

                # Save model
                output_dir = "./models/checkpoints"
                os.makedirs(output_dir, exist_ok=True)
                model_path = os.path.join(output_dir, "minimal_working_model.pt")

                torch.save(
                    {
                        "model_state_dict": model.state_dict(),
                        "optimizer_state_dict": optimizer.state_dict(),
                        "epoch": epoch + 1,
                        "val_loss": best_val_loss,
                        "training_history": training_history,
                    },
                    model_path,
                )

                logger.info("   â€¢ Model saved to: {model_path}")

        logger.info("ðŸŽ‰ Training completed successfully!")
        logger.info("   â€¢ Best validation loss: {best_val_loss:.4f}")
        logger.info("   â€¢ Model saved to: ./models/checkpoints/minimal_working_model.pt")

        return True

    except Exception as e:
        logger.error("âŒ Training failed: {e}")
        import traceback

        traceback.print_exc()
        return False


def main():
    """Main function."""
    logger.info("ðŸ§ª Minimal Working Training Script")
    logger.info("This script uses only working modules to avoid environment issues")

    success = train_minimal_model()

    if success:
        logger.info("âœ… Training completed successfully!")
        sys.exit(0)
    else:
        logger.error("âŒ Training failed. Check the logs above.")
        sys.exit(1)


if __name__ == "__main__":
    main()
