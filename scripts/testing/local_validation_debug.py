            # Check per-class distribution
            # Count positive labels
        # Analyze first few examples
        # Calculate statistics
        # Check CUDA
        # Check for critical issues
        # Check for issues
        # Check for issues
        # Check if we have the expected keys
        # Check statistics
        # Compare with manual BCE
        # Create loader without dev_mode parameter
        # Create model
        # Ensure some positive labels
        # Get training data
        # Load data
        # Log class distribution
        # Prepare datasets
        # Scenario 1: Mixed labels
        # Test different scenarios
        # Test forward pass
        from src.models.emotion_detection.bert_classifier import WeightedBCELoss
        from src.models.emotion_detection.bert_classifier import create_bert_emotion_classifier
        from src.models.emotion_detection.dataset_loader import create_goemotions_loader
        from src.models.emotion_detection.dataset_loader import create_goemotions_loader
        import pandas as pd
        import torch
        import torch
        import torch
        import torch.nn.functional as F
        import transformers
    # Run all validations
    # Run validations
    # Summary
# Add src to path
# Configure logging
#!/usr/bin/env python3
from pathlib import Path
import logging
import numpy as np
import sys

"""
Local Validation and Debug Script for SAMO Deep Learning.

This script performs targeted validation to identify the root cause of the 0.0000 loss issue.
It can be run locally to diagnose problems before deploying to GCP.
"""

sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


def check_environment():
    """Check basic environment setup."""
    logger.info("üîç Checking environment...")

    try:
        logger.info("‚úÖ PyTorch: {torch.__version__}")
        logger.info("‚úÖ Transformers: {transformers.__version__}")
        logger.info("‚úÖ NumPy: {np.__version__}")
        logger.info("‚úÖ Pandas: {pd.__version__}")

        if torch.cuda.is_available():
            logger.info("‚úÖ CUDA: {torch.cuda.get_device_name(0)}")
        else:
            logger.info("‚úÖ CPU mode available")

        return True

    except Exception as e:
        logger.error("‚ùå Environment check failed: {e}")
        return False


def check_data_loading():
    """Check data loading functionality."""
    logger.info("üîç Checking data loading...")

    try:
        logger.info("   Loading dataset...")
        loader = create_goemotions_loader()

        datasets = loader.prepare_datasets()

        expected_keys = ["train", "validation", "test", "statistics", "class_weights"]
        for key in expected_keys:
            if key not in datasets:
                logger.error("‚ùå Missing key in datasets: {key}")
                return False

        logger.info("‚úÖ Train set: {len(datasets['train'])} examples")
        logger.info("‚úÖ Validation set: {len(datasets['validation'])} examples")
        logger.info("‚úÖ Test set: {len(datasets['test'])} examples")

        stats = datasets["statistics"]
        logger.info("‚úÖ Total examples: {stats.get('total_examples', 'N/A')}")
        logger.info("‚úÖ Emotion distribution: {len(stats.get('emotion_counts', {}))} emotions")

        return True

    except Exception as e:
        logger.error("‚ùå Data loading failed: {e}")
        return False


def check_model_creation():
    """Check model creation and forward pass."""
    logger.info("üîç Checking model creation...")

    try:
        model, loss_fn = create_bert_emotion_classifier(
            model_name="bert-base-uncased",
            class_weights=None,
            freeze_bert_layers=6,
        )

        logger.info("‚úÖ Model created: {model.count_parameters():,} parameters")
        logger.info("‚úÖ Loss function: {type(loss_fn).__name__}")

        batch_size = 2
        seq_length = 64
        num_classes = 28

        dummy_input_ids = torch.randint(0, 1000, (batch_size, seq_length))
        dummy_attention_mask = torch.ones(batch_size, seq_length)
        dummy_labels = torch.randint(0, 2, (batch_size, num_classes)).float()

        dummy_labels[:, 0] = 1.0

        model.eval()
        with torch.no_grad():
            logits = model(dummy_input_ids, dummy_attention_mask)
            loss = loss_fn(logits, dummy_labels)

        logger.info("‚úÖ Forward pass successful")
        logger.info("   Logits shape: {logits.shape}")
        logger.info("   Loss value: {loss.item():.8f}")

        if loss.item() <= 0:
            logger.error("‚ùå CRITICAL: Loss is zero or negative: {loss.item()}")
            return False

        if torch.isnan(loss).any():
            logger.error("‚ùå CRITICAL: NaN loss!")
            return False

        return True

    except Exception as e:
        logger.error("‚ùå Model creation failed: {e}")
        return False


def check_loss_function():
    """Check loss function implementation."""
    logger.info("üîç Checking loss function...")

    try:
        batch_size = 4
        num_classes = 28

        logits = torch.randn(batch_size, num_classes)
        labels = torch.randint(0, 2, (batch_size, num_classes)).float()
        labels[:, 0] = 1.0  # Ensure some positive labels

        loss_fn = WeightedBCELoss()
        loss1 = loss_fn(logits, labels)

        bce_manual = F.binary_cross_entropy_with_logits(logits, labels, reduction="mean")

        logger.info("‚úÖ Mixed labels loss: {loss1.item():.8f}")
        logger.info("‚úÖ All positive loss: {loss_fn(logits, torch.ones(batch_size, num_classes)).item():.8f}")
        logger.info("‚úÖ All negative loss: {loss_fn(logits, torch.zeros(batch_size, num_classes)).item():.8f}")
        logger.info("‚úÖ Manual BCE loss: {bce_manual.item():.8f}")

        if loss1.item() <= 0:
            logger.error("‚ùå CRITICAL: Loss function producing zero/negative values!")
            return False

        return True

    except Exception as e:
        logger.error("‚ùå Loss function check failed: {e}")
        return False


def check_data_distribution():
    """Check data distribution to identify 0.0000 loss causes."""
    logger.info("üîç Checking data distribution...")

    try:
        loader = create_goemotions_loader()
        datasets = loader.prepare_datasets()

        train_dataset = datasets["train"]

        total_samples = min(100, len(train_dataset))
        total_positive_labels = 0
        label_distribution = {}

        for i in range(total_samples):
            example = train_dataset[i]
            labels = example["labels"]

            positive_count = sum(labels)
            total_positive_labels += positive_count

            for _class_idx, label in enumerate(labels):
                if class_idx not in label_distribution:
                    label_distribution[class_idx] = 0
                if label == 1:
                    label_distribution[class_idx] += 1

        total_possible_labels = total_samples * 28  # 28 emotion classes
        positive_rate = total_positive_labels / total_possible_labels

        logger.info("‚úÖ Total samples analyzed: {total_samples}")
        logger.info("‚úÖ Total positive labels: {total_positive_labels}")
        logger.info("‚úÖ Positive label rate: {positive_rate:.6f}")

        if positive_rate == 0:
            logger.error("‚ùå CRITICAL: No positive labels found!")
            logger.error("   This will cause 0.0000 loss with BCE")
            return False
        elif positive_rate == 1:
            logger.error("‚ùå CRITICAL: All labels are positive!")
            logger.error("   This will cause 0.0000 loss with BCE")
            return False
        elif positive_rate < 0.01:
            logger.warning("‚ö†Ô∏è  Very low positive label rate")
            logger.warning("   Consider using focal loss or class weights")

        logger.info("üìä Class distribution (first 10 classes):")
        for class_idx in range(min(10, len(label_distribution))):
            count = label_distribution.get(class_idx, 0)
            if count > 0:
                logger.info("   Class {class_idx}: {count} positive samples")

        return True

    except Exception as e:
        logger.error("‚ùå Data distribution check failed: {e}")
        return False


def main():
    """Main function to run all validations."""
    logger.info("üöÄ SAMO-DL Local Validation and Debug")
    logger.info("=" * 50)

    validations = [
        ("Environment", check_environment),
        ("Data Loading", check_data_loading),
        ("Model Creation", check_model_creation),
        ("Loss Function", check_loss_function),
        ("Data Distribution", check_data_distribution),
    ]

    results = {}
    for name, validation_func in validations:
        logger.info("\n{'='*40}")
        logger.info("Running: {name}")
        logger.info("{'='*40}")

        try:
            success = validation_func()
            results[name] = success

            if success:
                logger.info("‚úÖ {name} PASSED")
            else:
                logger.error("‚ùå {name} FAILED")

        except Exception as e:
            logger.error("‚ùå {name} ERROR: {e}")
            results[name] = False

    passed = sum(results.values())
    total = len(results)

    logger.info("\n{'='*50}")
    logger.info("üìä VALIDATION SUMMARY")
    logger.info("{'='*50}")
    logger.info("Total checks: {total}")
    logger.info("Passed: {passed}")
    logger.info("Failed: {total - passed}")

    if passed == total:
        logger.info("\n‚úÖ ALL VALIDATIONS PASSED!")
        logger.info("   The 0.0000 loss issue is likely due to:")
        logger.info("   1. Learning rate too high (try 2e-6 instead of 2e-5)")
        logger.info("   2. Need focal loss for class imbalance")
        logger.info("   3. Need class weights for imbalanced data")
        logger.info("   Ready for training with optimized configuration!")
    else:
        logger.error("\n‚ùå SOME CHECKS FAILED!")
        logger.error("   Fix the issues above before proceeding")
        logger.error("   This will prevent the 0.0000 loss problem")

    return passed == total


if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1)
