#!/usr/bin/env python3
"""
Create the Colab domain adaptation notebook with proper JSON format.
"""

import json

def create_colab_notebook():
    """Create the domain adaptation GPU training notebook."""

    notebook = {
        "cells": [
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "# SAMO Deep Learning - Domain Adaptation GPU Training\n",
                    "\n",
                    "## üéØ REQ-DL-012: Domain-Adapted Emotion Detection\n",
                    "\n",
                    "**Target**: Achieve 70% F1 score on journal entries through domain adaptation from GoEmotions (Reddit comments) to personal journal writing style.\n",
                    "\n",
                    "### Key Objectives:\n",
                    "- Bridge domain gap between Reddit comments and journal entries\n",
                    "- Implement focal loss for class imbalance\n",
                    "- Use domain adaptation techniques for better transfer learning\n",
                    "- Optimize for GPU training on Colab"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## üöÄ Environment Setup & GPU Configuration"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Verify GPU availability\n",
                    "import torch\n",
                    "import gc\n",
                    "\n",
                    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                    "if torch.cuda.is_available():\n",
                    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                    "    \n",
                    "    # Clear GPU cache\n",
                    "    torch.cuda.empty_cache()\n",
                    "    gc.collect()\n",
                    "else:\n",
                    "    print(\"‚ö†Ô∏è No GPU available. Training will be slow on CPU.\")\n",
                    "\n",
                    "# Enable cudnn benchmarking for faster training\n",
                    "torch.backends.cudnn.benchmark = True"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## üì¶ Install Dependencies"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Install required packages\n",
                    "!pip install torch>=2.1.0 torchvision>=0.16.0 torchaudio>=2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
                    "!pip install transformers>=4.30.0 datasets>=2.13.0 evaluate scikit-learn pandas numpy matplotlib seaborn\n",
                    "!pip install accelerate wandb pydub openai-whisper jiwer\n",
                    "\n",
                    "# Clone repository if not already done\n",
                    "!git clone https://github.com/uelkerd/SAMO--DL.git\n",
                    "%cd SAMO--DL"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## üîç Domain Gap Analysis"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "import json\n",
                    "import pandas as pd\n",
                    "import numpy as np\n",
                    "from datasets import load_dataset\n",
                    "import matplotlib.pyplot as plt\n",
                    "import seaborn as sns\n",
                    "\n",
                    "def analyze_writing_style(texts, domain_name):\n",
                    "    \"\"\"Analyze writing style characteristics of a domain.\"\"\"\n",
                    "    avg_length = np.mean([len(text.split()) for text in texts])\n",
                    "    personal_pronouns = sum(['I ' in text or 'my ' in text or 'me ' in text for text in texts]) / len(texts)\n",
                    "    reflection_words = sum(['think' in text.lower() or 'feel' in text.lower() or 'believe' in text.lower() \n",
                    "                           for text in texts]) / len(texts)\n",
                    "    \n",
                    "    print(f\"{domain_name} Style Analysis:\")\n",
                    "    print(f\"  Average length: {avg_length:.1f} words\")\n",
                    "    print(f\"  Personal pronouns: {personal_pronouns:.1%}\")\n",
                    "    print(f\"  Reflection words: {reflection_words:.1%}\")\n",
                    "    \n",
                    "    return {\n",
                    "        'avg_length': avg_length,\n",
                    "        'personal_pronouns': personal_pronouns,\n",
                    "        'reflection_words': reflection_words\n",
                    "    }\n",
                    "\n",
                    "# Load datasets\n",
                    "print(\"üìä Loading datasets...\")\n",
                    "\n",
                    "# Load GoEmotions dataset\n",
                    "go_emotions = load_dataset(\"go_emotions\", \"simplified\")\n",
                    "go_texts = go_emotions['train']['text'][:1000]  # Sample for analysis\n",
                    "\n",
                    "# Load journal dataset\n",
                    "with open('data/journal_test_dataset.json', 'r') as f:\n",
                    "    journal_entries = json.load(f)\n",
                    "\n",
                    "journal_df = pd.DataFrame(journal_entries)\n",
                    "journal_texts = journal_df['content'].tolist()\n",
                    "\n",
                    "# Analyze domains\n",
                    "print(\"\\nüîç Domain Gap Analysis:\")\n",
                    "go_analysis = analyze_writing_style(go_texts, \"GoEmotions (Reddit)\")\n",
                    "journal_analysis = analyze_writing_style(journal_texts, \"Journal Entries\")\n",
                    "\n",
                    "# Visualize differences\n",
                    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                    "\n",
                    "metrics = ['avg_length', 'personal_pronouns', 'reflection_words']\n",
                    "labels = ['Avg Length (words)', 'Personal Pronouns', 'Reflection Words']\n",
                    "\n",
                    "for i, (metric, label) in enumerate(zip(metrics, labels)):\n",
                    "    axes[i].bar(['GoEmotions', 'Journal'], \n",
                    "                [go_analysis[metric], journal_analysis[metric]])\n",
                    "    axes[i].set_title(label)\n",
                    "    axes[i].set_ylabel('Percentage' if 'pronouns' in metric or 'reflection' in metric else 'Count')\n",
                    "\n",
                    "plt.tight_layout()\n",
                    "plt.show()\n",
                    "\n",
                    "print(\"\\nüéØ Key Insights:\")\n",
                    "print(f\"- Journal entries are {journal_analysis['avg_length']/go_analysis['avg_length']:.1f}x longer\")\n",
                    "print(f\"- Journal entries use {journal_analysis['personal_pronouns']/go_analysis['personal_pronouns']:.1f}x more personal pronouns\")\n",
                    "print(f\"- Journal entries contain {journal_analysis['reflection_words']/go_analysis['reflection_words']:.1f}x more reflection words\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## üèóÔ∏è Model Architecture"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "import torch\n",
                    "import torch.nn as nn\n",
                    "import torch.nn.functional as F\n",
                    "from transformers import AutoModel, AutoTokenizer\n",
                    "\n",
                    "class FocalLoss(nn.Module):\n",
                    "    \"\"\"Focal Loss for addressing class imbalance in emotion detection.\"\"\"\n",
                    "    \n",
                    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
                    "        super(FocalLoss, self).__init__()\n",
                    "        self.alpha = alpha\n",
                    "        self.gamma = gamma\n",
                    "        self.reduction = reduction\n",
                    "    \n",
                    "    def forward(self, inputs, targets):\n",
                    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
                    "        pt = torch.exp(-ce_loss)\n",
                    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
                    "        \n",
                    "        if self.reduction == 'mean':\n",
                    "            return focal_loss.mean()\n",
                    "        elif self.reduction == 'sum':\n",
                    "            return focal_loss.sum()\n",
                    "        else:\n",
                    "            return focal_loss\n",
                    "\n",
                    "class DomainAdaptedEmotionClassifier(nn.Module):\n",
                    "    \"\"\"BERT-based emotion classifier with domain adaptation capabilities.\"\"\"\n",
                    "    \n",
                    "    def __init__(self, model_name=\"bert-base-uncased\", num_labels=12, dropout=0.3):\n",
                    "        super().__init__()\n",
                    "        self.bert = AutoModel.from_pretrained(model_name)\n",
                    "        self.dropout = nn.Dropout(dropout)\n",
                    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
                    "        \n",
                    "        # Domain adaptation layer\n",
                    "        self.domain_classifier = nn.Sequential(\n",
                    "            nn.Linear(self.bert.config.hidden_size, 512),\n",
                    "            nn.ReLU(),\n",
                    "            nn.Dropout(0.3),\n",
                    "            nn.Linear(512, 2)  # 2 domains: GoEmotions vs Journal\n",
                    "        )\n",
                    "    \n",
                    "    def forward(self, input_ids, attention_mask, domain_labels=None):\n",
                    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
                    "        pooled_output = outputs.pooler_output\n",
                    "        \n",
                    "        # Emotion classification\n",
                    "        emotion_logits = self.classifier(self.dropout(pooled_output))\n",
                    "        \n",
                    "        # Domain classification (for domain adaptation)\n",
                    "        domain_logits = self.domain_classifier(pooled_output)\n",
                    "        \n",
                    "        if domain_labels is not None:\n",
                    "            return emotion_logits, domain_logits\n",
                    "        return emotion_logits\n",
                    "\n",
                    "# Initialize model and tokenizer\n",
                    "print(\"üèóÔ∏è Initializing model...\")\n",
                    "model_name = \"bert-base-uncased\"\n",
                    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                    "model = DomainAdaptedEmotionClassifier(model_name=model_name, num_labels=12)\n",
                    "\n",
                    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                    "model = model.to(device)\n",
                    "\n",
                    "print(f\"‚úÖ Model loaded on {device}\")\n",
                    "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## üìä Data Preparation"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
                    "from sklearn.model_selection import train_test_split\n",
                    "from sklearn.preprocessing import LabelEncoder\n",
                    "\n",
                    "class EmotionDataset(Dataset):\n",
                    "    \"\"\"Custom dataset for emotion classification.\"\"\"\n",
                    "    \n",
                    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
                    "        self.texts = texts\n",
                    "        self.labels = labels\n",
                    "        self.tokenizer = tokenizer\n",
                    "        self.max_length = max_length\n",
                    "    \n",
                    "    def __len__(self):\n",
                    "        return len(self.texts)\n",
                    "    \n",
                    "    def __getitem__(self, idx):\n",
                    "        text = str(self.texts[idx])\n",
                    "        label = self.labels[idx]\n",
                    "        \n",
                    "        encoding = self.tokenizer(\n",
                    "            text,\n",
                    "            truncation=True,\n",
                    "            padding='max_length',\n",
                    "            max_length=self.max_length,\n",
                    "            return_tensors='pt'\n",
                    "        )\n",
                    "        \n",
                    "        return {\n",
                    "            'input_ids': encoding['input_ids'].flatten(),\n",
                    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
                    "            'labels': torch.tensor(label, dtype=torch.long)\n",
                    "        }\n",
                    "\n",
                    "# Prepare GoEmotions data\n",
                    "print(\"üìä Preparing GoEmotions data...\")\n",
                    "go_train = go_emotions['train']\n",
                    "go_texts = go_train['text'][:10000]  # Use subset for faster training\n",
                    "go_labels = go_train['labels'][:10000]\n",
                    "\n",
                    "# Convert multi-label to single label (take first emotion)\n",
                    "go_single_labels = [label[0] if label else 0 for label in go_labels]\n",
                    "\n",
                    "# Prepare journal data\n",
                    "print(\"üìä Preparing journal data...\")\n",
                    "journal_texts = journal_df['content'].tolist()\n",
                    "journal_emotions = journal_df['emotion'].tolist()\n",
                    "\n",
                    "# Create label encoder\n",
                    "label_encoder = LabelEncoder()\n",
                    "all_emotions = list(set(go_single_labels + journal_emotions))\n",
                    "label_encoder.fit(all_emotions)\n",
                    "\n",
                    "# Encode labels\n",
                    "go_encoded_labels = label_encoder.transform(go_single_labels)\n",
                    "journal_encoded_labels = label_encoder.transform(journal_emotions)\n",
                    "\n",
                    "# Split journal data\n",
                    "journal_train_texts, journal_val_texts, journal_train_labels, journal_val_labels = train_test_split(\n",
                    "    journal_texts, journal_encoded_labels, test_size=0.2, random_state=42, stratify=journal_encoded_labels\n",
                    ")\n",
                    "\n",
                    "# Create datasets\n",
                    "go_dataset = EmotionDataset(go_texts, go_encoded_labels, tokenizer)\n",
                    "journal_train_dataset = EmotionDataset(journal_train_texts, journal_train_labels, tokenizer)\n",
                    "journal_val_dataset = EmotionDataset(journal_val_texts, journal_val_labels, tokenizer)\n",
                    "\n",
                    "# Create dataloaders\n",
                    "batch_size = 16\n",
                    "go_loader = DataLoader(go_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
                    "journal_train_loader = DataLoader(journal_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
                    "journal_val_loader = DataLoader(journal_val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
                    "\n",
                    "print(f\"‚úÖ Data prepared:\")\n",
                    "print(f\"  GoEmotions: {len(go_dataset)} samples\")\n",
                    "print(f\"  Journal Train: {len(journal_train_dataset)} samples\")\n",
                    "print(f\"  Journal Val: {len(journal_val_dataset)} samples\")\n",
                    "print(f\"  Total classes: {len(label_encoder.classes_)}\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## üéØ Training Pipeline"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "from sklearn.metrics import classification_report, f1_score\n",
                    "import wandb\n",
                    "\n",
                    "class DomainAdaptationTrainer:\n",
                    "    \"\"\"Trainer for domain adaptation training.\"\"\"\n",
                    "    \n",
                    "    def __init__(self, model, tokenizer, device):\n",
                    "        self.model = model\n",
                    "        self.tokenizer = tokenizer\n",
                    "        self.device = device\n",
                    "        self.criterion = FocalLoss(alpha=1, gamma=2)\n",
                    "        self.domain_criterion = nn.CrossEntropyLoss()\n",
                    "        \n",
                    "    def train_step(self, batch, domain_labels, lambda_domain=0.1):\n",
                    "        \"\"\"Single training step with domain adaptation.\"\"\"\n",
                    "        self.model.train()\n",
                    "        \n",
                    "        input_ids = batch['input_ids'].to(self.device)\n",
                    "        attention_mask = batch['attention_mask'].to(self.device)\n",
                    "        labels = batch['labels'].to(self.device)\n",
                    "        domain_labels = domain_labels.to(self.device)\n",
                    "        \n",
                    "        # Forward pass\n",
                    "        emotion_logits, domain_logits = self.model(input_ids, attention_mask, domain_labels)\n",
                    "        \n",
                    "        # Calculate losses\n",
                    "        emotion_loss = self.criterion(emotion_logits, labels)\n",
                    "        domain_loss = self.domain_criterion(domain_logits, domain_labels)\n",
                    "        \n",
                    "        # Combined loss\n",
                    "        total_loss = emotion_loss + lambda_domain * domain_loss\n",
                    "        \n",
                    "        return {\n",
                    "            'total_loss': total_loss,\n",
                    "            'emotion_loss': emotion_loss,\n",
                    "            'domain_loss': domain_loss\n",
                    "        }\n",
                    "    \n",
                    "    def evaluate(self, dataloader):\n",
                    "        \"\"\"Evaluate model on validation set.\"\"\"\n",
                    "        self.model.eval()\n",
                    "        total_loss = 0\n",
                    "        all_predictions = []\n",
                    "        all_labels = []\n",
                    "        \n",
                    "        with torch.no_grad():\n",
                    "            for batch in dataloader:\n",
                    "                input_ids = batch['input_ids'].to(self.device)\n",
                    "                attention_mask = batch['attention_mask'].to(self.device)\n",
                    "                labels = batch['labels'].to(self.device)\n",
                    "                \n",
                    "                emotion_logits = self.model(input_ids, attention_mask)\n",
                    "                loss = self.criterion(emotion_logits, labels)\n",
                    "                \n",
                    "                total_loss += loss.item()\n",
                    "                predictions = torch.argmax(emotion_logits, dim=1)\n",
                    "                \n",
                    "                all_predictions.extend(predictions.cpu().numpy())\n",
                    "                all_labels.extend(labels.cpu().numpy())\n",
                    "        \n",
                    "        # Calculate metrics\n",
                    "        f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
                    "        f1_weighted = f1_score(all_labels, all_predictions, average='weighted')\n",
                    "        \n",
                    "        return {\n",
                    "            'loss': total_loss / len(dataloader),\n",
                    "            'f1_macro': f1_macro,\n",
                    "            'f1_weighted': f1_weighted\n",
                    "        }\n",
                    "\n",
                    "# Initialize trainer\n",
                    "trainer = DomainAdaptationTrainer(model, tokenizer, device)\n",
                    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
                    "\n",
                    "# Initialize wandb (optional)\n",
                    "try:\n",
                    "    wandb.init(project=\"samo-domain-adaptation\", name=\"journal-emotion-detection\")\n",
                    "    use_wandb = True\n",
                    "except:\n",
                    "    print(\"‚ö†Ô∏è Wandb not available, continuing without logging\")\n",
                    "    use_wandb = False\n",
                    "\n",
                    "print(\"üéØ Starting domain adaptation training...\")"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Training loop\n",
                    "num_epochs = 5\n",
                    "best_f1 = 0\n",
                    "training_history = []\n",
                    "\n",
                    "for epoch in range(num_epochs):\n",
                    "    print(f\"\\nüîÑ Epoch {epoch + 1}/{num_epochs}\")\n",
                    "    \n",
                    "    # Training phase\n",
                    "    model.train()\n",
                    "    total_loss = 0\n",
                    "    \n",
                    "    # Train on GoEmotions data\n",
                    "    print(\"  üìö Training on GoEmotions data...\")\n",
                    "    for i, batch in enumerate(go_loader):\n",
                    "        domain_labels = torch.zeros(batch['input_ids'].size(0), dtype=torch.long)\n",
                    "        losses = trainer.train_step(batch, domain_labels, lambda_domain=0.1)\n",
                    "        \n",
                    "        optimizer.zero_grad()\n",
                    "        losses['total_loss'].backward()\n",
                    "        optimizer.step()\n",
                    "        \n",
                    "        total_loss += losses['total_loss'].item()\n",
                    "        \n",
                    "        if i % 100 == 0:\n",
                    "            print(f\"    Batch {i}/{len(go_loader)}, Loss: {losses['total_loss'].item():.4f}\")\n",
                    "    \n",
                    "    # Train on journal data\n",
                    "    print(\"  üìù Training on journal data...\")\n",
                    "    for i, batch in enumerate(journal_train_loader):\n",
                    "        domain_labels = torch.ones(batch['input_ids'].size(0), dtype=torch.long)\n",
                    "        losses = trainer.train_step(batch, domain_labels, lambda_domain=0.1)\n",
                    "        \n",
                    "        optimizer.zero_grad()\n",
                    "        losses['total_loss'].backward()\n",
                    "        optimizer.step()\n",
                    "        \n",
                    "        total_loss += losses['total_loss'].item()\n",
                    "        \n",
                    "        if i % 10 == 0:\n",
                    "            print(f\"    Batch {i}/{len(journal_train_loader)}, Loss: {losses['total_loss'].item():.4f}\")\n",
                    "    \n",
                    "    # Validation\n",
                    "    print(\"  üéØ Validating on journal test set...\")\n",
                    "    val_results = trainer.evaluate(journal_val_loader)\n",
                    "    \n",
                    "    avg_loss = total_loss / (len(go_loader) + len(journal_train_loader))\n",
                    "    \n",
                    "    print(f\"  üìä Epoch {epoch + 1} Results:\")\n",
                    "    print(f\"    Average Loss: {avg_loss:.4f}\")\n",
                    "    print(f\"    Validation F1 (Macro): {val_results['f1_macro']:.4f}\")\n",
                    "    print(f\"    Validation F1 (Weighted): {val_results['f1_weighted']:.4f}\")\n",
                    "    \n",
                    "    # Log to wandb\n",
                    "    if use_wandb:\n",
                    "        wandb.log({\n",
                    "            'epoch': epoch,\n",
                    "            'train_loss': avg_loss,\n",
                    "            'val_loss': val_results['loss'],\n",
                    "            'val_f1_macro': val_results['f1_macro'],\n",
                    "            'val_f1_weighted': val_results['f1_weighted']\n",
                    "        })\n",
                    "    \n",
                    "    # Save best model\n",
                    "    if val_results['f1_macro'] > best_f1:\n",
                    "        best_f1 = val_results['f1_macro']\n",
                    "        torch.save(model.state_dict(), 'best_domain_adapted_model.pth')\n",
                    "        print(f\"    üíæ New best model saved! F1: {best_f1:.4f}\")\n",
                    "    \n",
                    "    training_history.append({\n",
                    "        'epoch': epoch,\n",
                    "        'train_loss': avg_loss,\n",
                    "        'val_f1_macro': val_results['f1_macro'],\n",
                    "        'val_f1_weighted': val_results['f1_weighted']\n",
                    "    })\n",
                    "    \n",
                    "    # Clear GPU cache\n",
                    "    if torch.cuda.is_available():\n",
                    "        torch.cuda.empty_cache()\n",
                    "\n",
                    "print(f\"\\nüéâ Training completed! Best F1 Score: {best_f1:.4f}\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## üìà Results Analysis & Visualization"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Plot training history\n",
                    "history_df = pd.DataFrame(training_history)\n",
                    "\n",
                    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                    "\n",
                    "# Loss plot\n",
                    "axes[0].plot(history_df['epoch'], history_df['train_loss'], 'b-', label='Training Loss')\n",
                    "axes[0].set_title('Training Loss Over Time')\n",
                    "axes[0].set_xlabel('Epoch')\n",
                    "axes[0].set_ylabel('Loss')\n",
                    "axes[0].legend()\n",
                    "axes[0].grid(True)\n",
                    "\n",
                    "# F1 Score plot\n",
                    "axes[1].plot(history_df['epoch'], history_df['val_f1_macro'], 'r-', label='F1 Macro')\n",
                    "axes[1].plot(history_df['epoch'], history_df['val_f1_weighted'], 'g-', label='F1 Weighted')\n",
                    "axes[1].axhline(y=0.7, color='orange', linestyle='--', label='Target (70%)')\n",
                    "axes[1].set_title('Validation F1 Score Over Time')\n",
                    "axes[1].set_xlabel('Epoch')\n",
                    "axes[1].set_ylabel('F1 Score')\n",
                    "axes[1].legend()\n",
                    "axes[1].grid(True)\n",
                    "\n",
                    "plt.tight_layout()\n",
                    "plt.show()\n",
                    "\n",
                    "# Final evaluation\n",
                    "print(\"\\nüéØ Final Model Evaluation:\")\n",
                    "model.load_state_dict(torch.load('best_domain_adapted_model.pth'))\n",
                    "final_results = trainer.evaluate(journal_val_loader)\n",
                    "\n",
                    "print(f\"üìä Final Results:\")\n",
                    "print(f\"  F1 Score (Macro): {final_results['f1_macro']:.4f}\")\n",
                    "print(f\"  F1 Score (Weighted): {final_results['f1_weighted']:.4f}\")\n",
                    "print(f\"  Target Met (70%): {'‚úÖ' if final_results['f1_macro'] >= 0.7 else '‚ùå'}\")\n",
                    "\n",
                    "# REQ-DL-012 Validation\n",
                    "print(f\"\\nüéØ REQ-DL-012 Validation:\")\n",
                    "print(f\"  Target: 70% F1 score on journal entries\")\n",
                    "print(f\"  Achieved: {final_results['f1_macro']:.1%} F1 score\")\n",
                    "print(f\"  Status: {'‚úÖ SUCCESS' if final_results['f1_macro'] >= 0.7 else '‚ùå NEEDS IMPROVEMENT'}\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## üíæ Model Export & Deployment"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Save model artifacts\n",
                    "import pickle\n",
                    "\n",
                    "# Save label encoder\n",
                    "with open('label_encoder.pkl', 'wb') as f:\n",
                    "    pickle.dump(label_encoder, f)\n",
                    "\n",
                    "# Save tokenizer\n",
                    "tokenizer.save_pretrained('./domain_adapted_model')\n",
                    "\n",
                    "# Save model config\n",
                    "model_config = {\n",
                    "    'model_name': model_name,\n",
                    "    'num_labels': 12,\n",
                    "    'max_length': 128,\n",
                    "    'label_encoder_path': 'label_encoder.pkl',\n",
                    "    'model_path': 'best_domain_adapted_model.pth'\n",
                    "}\n",
                    "\n",
                    "with open('model_config.json', 'w') as f:\n",
                    "    json.dump(model_config, f, indent=2)\n",
                    "\n",
                    "print(\"üíæ Model artifacts saved:\")\n",
                    "print(\"  - best_domain_adapted_model.pth (model weights)\")\n",
                    "print(\"  - label_encoder.pkl (label encoder)\")\n",
                    "print(\"  - domain_adapted_model/ (tokenizer)\")\n",
                    "print(\"  - model_config.json (configuration)\")\n",
                    "\n",
                    "# Download files (for Colab)\n",
                    "from google.colab import files\n",
                    "files.download('best_domain_adapted_model.pth')\n",
                    "files.download('label_encoder.pkl')\n",
                    "files.download('model_config.json')\n",
                    "\n",
                    "print(\"\\nüöÄ Model ready for deployment!\")\n",
                    "print(\"üìã Next steps:\")\n",
                    "print(\"  1. Integrate model into SAMO-DL pipeline\")\n",
                    "print(\"  2. Update emotion detection API\")\n",
                    "print(\"  3. Deploy to production environment\")\n",
                    "print(\"  4. Update PRD with achieved metrics\")"
                ]
            }
        ],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.8.5"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }

    # Write the notebook to file
    notebook_path = "notebooks/domain_adaptation_gpu_training.ipynb"
    with open(notebook_path, 'w') as f:
        json.dump(notebook, f, indent=1)

    print(f"‚úÖ Created Colab notebook: {notebook_path}")
    print("üìã Notebook includes:")
    print("  - GPU environment setup")
    print("  - Domain gap analysis")
    print("  - Focal loss implementation")
    print("  - Domain adaptation training")
    print("  - REQ-DL-012 validation")
    print("  - Model export for deployment")

if __name__ == "__main__":
    create_colab_notebook()