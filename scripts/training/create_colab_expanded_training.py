#!/usr/bin/env python3
"""
Create a Colab notebook for expanded dataset training.
"""

def create_colab_notebook():
    """Create a complete Colab notebook for expanded training."""

    notebook_content = '''{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ðŸš€ REQ-DL-012: Expanded Dataset Retraining\\n",
        "## Domain-Adapted Emotion Detection with 1000+ Samples\\n",
        "\\n",
        "**Target**: Achieve 75-85% F1 Score\\n",
        "**Current**: 67% F1 Score\\n",
        "**Expected Improvement**: 8-18% F1 Score\\n",
        "\\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## ðŸ”§ Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "# Clone repository\\n",
        "!git clone https://github.com/uelkerd/SAMO--DL.git\\n",
        "%cd SAMO--DL\\n",
        "print(\"âœ… Repository cloned and ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\\n",
        "!pip install torch transformers scikit-learn datasets\\n",
        "print(\"âœ… Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expand_dataset"
      },
      "source": [
        "## ðŸ“Š Create Expanded Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_expanded_dataset"
      },
      "outputs": [],
      "source": [
        "# Create expanded dataset directly in Colab\\n",
        "import json\\n",
        "import random\\n",
        "from typing import List, Dict\\n",
        "\\n",
        "def load_current_dataset():\\n",
        "    \"\"\"Load the current journal dataset.\"\"\"\\n",
        "    with open('data/journal_test_dataset.json', 'r') as f:\\n",
        "        return json.load(f)\\n",
        "\\n",
        "def create_variation(base_sample: Dict, emotion: str) -> Dict:\\n",
        "    \"\"\"Create a variation of a base sample.\"\"\"\\n",
        "    \\n",
        "    # Templates for different emotions\\n",
        "    emotion_templates = {\\n",
        "        'happy': [\\n",
        "            \"I'm feeling really happy today!\",\\n",
        "            \"I'm so happy about this!\",\\n",
        "            \"This makes me incredibly happy!\",\\n",
        "            \"I'm feeling joyful and happy!\",\\n",
        "            \"I'm really happy with how things are going!\",\\n",
        "            \"This brings me so much happiness!\",\\n",
        "            \"I'm feeling happy and content!\",\\n",
        "            \"I'm really happy about this outcome!\",\\n",
        "            \"This makes me feel so happy!\",\\n",
        "            \"I'm feeling happy and grateful!\"\\n",
        "        ],\\n",
        "        'sad': [\\n",
        "            \"I'm feeling really sad today.\",\\n",
        "            \"This makes me so sad.\",\\n",
        "            \"I'm feeling down and sad.\",\\n",
        "            \"I'm really sad about this situation.\",\\n",
        "            \"This brings me sadness.\",\\n",
        "            \"I'm feeling sad and lonely.\",\\n",
        "            \"I'm really sad about what happened.\",\\n",
        "            \"This makes me feel so sad.\",\\n",
        "            \"I'm feeling sad and disappointed.\",\\n",
        "            \"I'm really sad about this outcome.\"\\n",
        "        ],\\n",
        "        'frustrated': [\\n",
        "            \"I'm so frustrated with this!\",\\n",
        "            \"This is really frustrating me.\",\\n",
        "            \"I'm feeling frustrated and annoyed.\",\\n",
        "            \"I'm really frustrated about this situation.\",\\n",
        "            \"This is so frustrating!\",\\n",
        "            \"I'm feeling frustrated and angry.\",\\n",
        "            \"I'm really frustrated with how this is going.\",\\n",
        "            \"This makes me so frustrated.\",\\n",
        "            \"I'm feeling frustrated and upset.\",\\n",
        "            \"I'm really frustrated about this outcome.\"\\n",
        "        ],\\n",
        "        'anxious': [\\n",
        "            \"I'm feeling really anxious about this.\",\\n",
        "            \"This is making me anxious.\",\\n",
        "            \"I'm feeling anxious and worried.\",\\n",
        "            \"I'm really anxious about what might happen.\",\\n",
        "            \"This gives me anxiety.\",\\n",
        "            \"I'm feeling anxious and nervous.\",\\n",
        "            \"I'm really anxious about this situation.\",\\n",
        "            \"This makes me feel so anxious.\",\\n",
        "            \"I'm feeling anxious and stressed.\",\\n",
        "            \"I'm really anxious about the outcome.\"\\n",
        "        ],\\n",
        "        'excited': [\\n",
        "            \"I'm so excited about this!\",\\n",
        "            \"This makes me really excited!\",\\n",
        "            \"I'm feeling excited and enthusiastic!\",\\n",
        "            \"I'm really excited about what's coming!\",\\n",
        "            \"This is so exciting!\",\\n",
        "            \"I'm feeling excited and eager!\",\\n",
        "            \"I'm really excited about this opportunity!\",\\n",
        "            \"This makes me feel so excited!\",\\n",
        "            \"I'm feeling excited and thrilled!\",\\n",
        "            \"I'm really excited about this outcome!\"\\n",
        "        ],\\n",
        "        'calm': [\\n",
        "            \"I'm feeling really calm right now.\",\\n",
        "            \"This brings me a sense of calm.\",\\n",
        "            \"I'm feeling calm and peaceful.\",\\n",
        "            \"I'm really calm about this situation.\",\\n",
        "            \"This makes me feel calm.\",\\n",
        "            \"I'm feeling calm and relaxed.\",\\n",
        "            \"I'm really calm about what's happening.\",\\n",
        "            \"This gives me a calm feeling.\",\\n",
        "            \"I'm feeling calm and content.\",\\n",
        "            \"I'm really calm about this outcome.\"\\n",
        "        ],\\n",
        "        'content': [\\n",
        "            \"I'm feeling really content with this.\",\\n",
        "            \"This makes me feel content.\",\\n",
        "            \"I'm feeling content and satisfied.\",\\n",
        "            \"I'm really content with how things are.\",\\n",
        "            \"This brings me contentment.\",\\n",
        "            \"I'm feeling content and happy.\",\\n",
        "            \"I'm really content with this situation.\",\\n",
        "            \"This makes me feel so content.\",\\n",
        "            \"I'm feeling content and peaceful.\",\\n",
        "            \"I'm really content with this outcome.\"\\n",
        "        ],\\n",
        "        'grateful': [\\n",
        "            \"I'm feeling really grateful for this.\",\\n",
        "            \"This makes me so grateful.\",\\n",
        "            \"I'm feeling grateful and thankful.\",\\n",
        "            \"I'm really grateful for this opportunity.\",\\n",
        "            \"This fills me with gratitude.\",\\n",
        "            \"I'm feeling grateful and blessed.\",\\n",
        "            \"I'm really grateful for this situation.\",\\n",
        "            \"This makes me feel so grateful.\",\\n",
        "            \"I'm feeling grateful and appreciative.\",\\n",
        "            \"I'm really grateful for this outcome.\"\\n",
        "        ],\\n",
        "        'hopeful': [\\n",
        "            \"I'm feeling really hopeful about this.\",\\n",
        "            \"This gives me hope.\",\\n",
        "            \"I'm feeling hopeful and optimistic.\",\\n",
        "            \"I'm really hopeful about what's coming.\",\\n",
        "            \"This brings me hope.\",\\n",
        "            \"I'm feeling hopeful and positive.\",\\n",
        "            \"I'm really hopeful about this situation.\",\\n",
        "            \"This makes me feel so hopeful.\",\\n",
        "            \"I'm feeling hopeful and confident.\",\\n",
        "            \"I'm really hopeful about this outcome.\"\\n",
        "        ],\\n",
        "        'overwhelmed': [\\n",
        "            \"I'm feeling really overwhelmed by this.\",\\n",
        "            \"This is overwhelming me.\",\\n",
        "            \"I'm feeling overwhelmed and stressed.\",\\n",
        "            \"I'm really overwhelmed by this situation.\",\\n",
        "            \"This is so overwhelming.\",\\n",
        "            \"I'm feeling overwhelmed and anxious.\",\\n",
        "            \"I'm really overwhelmed by what's happening.\",\\n",
        "            \"This makes me feel so overwhelmed.\",\\n",
        "            \"I'm feeling overwhelmed and exhausted.\",\\n",
        "            \"I'm really overwhelmed by this outcome.\"\\n",
        "        ],\\n",
        "        'proud': [\\n",
        "            \"I'm feeling really proud of this.\",\\n",
        "            \"This makes me so proud.\",\\n",
        "            \"I'm feeling proud and accomplished.\",\\n",
        "            \"I'm really proud of what I've done.\",\\n",
        "            \"This fills me with pride.\",\\n",
        "            \"I'm feeling proud and satisfied.\",\\n",
        "            \"I'm really proud of this achievement.\",\\n",
        "            \"This makes me feel so proud.\",\\n",
        "            \"I'm feeling proud and confident.\",\\n",
        "            \"I'm really proud of this outcome.\"\\n",
        "        ],\\n",
        "        'tired': [\\n",
        "            \"I'm feeling really tired today.\",\\n",
        "            \"This is making me tired.\",\\n",
        "            \"I'm feeling tired and exhausted.\",\\n",
        "            \"I'm really tired from all this work.\",\\n",
        "            \"This is so tiring.\",\\n",
        "            \"I'm feeling tired and worn out.\",\\n",
        "            \"I'm really tired of this situation.\",\\n",
        "            \"This makes me feel so tired.\",\\n",
        "            \"I'm feeling tired and drained.\",\\n",
        "            \"I'm really tired of dealing with this.\"\\n",
        "        ]\\n",
        "    }\\n",
        "    \\n",
        "    # Get templates for this emotion\\n",
        "    templates = emotion_templates.get(emotion, [f\"I'm feeling {emotion}.\"])\\n",
        "    \\n",
        "    # Create variation\\n",
        "    template = random.choice(templates)\\n",
        "    \\n",
        "    # Add some variety to the content\\n",
        "    variations = [\\n",
        "        f\"{template} {random.choice(['It\\\\'s been a long day.', 'Things are going well.', 'I need to process this.', 'This is important to me.'])}\",\\n",
        "        f\"{template} {random.choice(['I hope this continues.', 'I wonder what\\\\'s next.', 'This feels right.', 'I\\\\'m processing this.'])}\",\\n",
        "        f\"{template} {random.choice(['I should reflect on this.', 'This is meaningful.', 'I appreciate this moment.', 'I\\\\'m learning from this.'])}\"\\n",
        "    ]\\n",
        "    \\n",
        "    content = random.choice(variations)\\n",
        "    \\n",
        "    return {\\n",
        "        'content': content,\\n",
        "        'emotion': emotion,\\n",
        "        'id': f\"expanded_{emotion}_{random.randint(1000, 9999)}\"\\n",
        "    }\\n",
        "\\n",
        "def create_balanced_dataset(target_size=1000):\\n",
        "    \"\"\"Create a balanced expanded dataset.\"\"\"\\n",
        "    print(\"ðŸ”§ Creating balanced expanded dataset...\")\\n",
        "    \\n",
        "    # Load current data\\n",
        "    current_data = load_current_dataset()\\n",
        "    \\n",
        "    # Analyze current distribution\\n",
        "    emotion_counts = {}\\n",
        "    for entry in current_data:\\n",
        "        emotion = entry['emotion']\\n",
        "        emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\\n",
        "    \\n",
        "    print(f\"ðŸ“Š Current emotion distribution:\")\\n",
        "    for emotion, count in sorted(emotion_counts.items()):\\n",
        "        print(f\"  {emotion}: {count} samples\")\\n",
        "    \\n",
        "    # Calculate target per emotion\\n",
        "    target_per_emotion = target_size // len(emotion_counts)\\n",
        "    print(f\"\\nðŸŽ¯ Target: {target_per_emotion} samples per emotion\")\\n",
        "    \\n",
        "    # Create expanded dataset\\n",
        "    expanded_data = []\\n",
        "    \\n",
        "    for emotion in emotion_counts.keys():\\n",
        "        # Get existing samples for this emotion\\n",
        "        existing_samples = [entry for entry in current_data if entry['emotion'] == emotion]\\n",
        "        current_count = len(existing_samples)\\n",
        "        \\n",
        "        print(f\"\\nðŸ“ Expanding '{emotion}' from {current_count} to {target_per_emotion} samples...\")\\n",
        "        \\n",
        "        # Add existing samples\\n",
        "        expanded_data.extend(existing_samples)\\n",
        "        \\n",
        "        # Generate additional samples\\n",
        "        needed_samples = target_per_emotion - current_count\\n",
        "        \\n",
        "        if needed_samples > 0:\\n",
        "            # Create variations of existing samples\\n",
        "            for i in range(needed_samples):\\n",
        "                # Pick a random existing sample to base variation on\\n",
        "                base_sample = random.choice(existing_samples)\\n",
        "                \\n",
        "                # Create variation\\n",
        "                variation = create_variation(base_sample, emotion)\\n",
        "                expanded_data.append(variation)\\n",
        "    \\n",
        "    print(f\"\\nâœ… Expanded dataset created:\")\\n",
        "    print(f\"  Original samples: {len(current_data)}\")\\n",
        "    print(f\"  Expanded samples: {len(expanded_data)}\")\\n",
        "    print(f\"  Target size: {target_size}\")\\n",
        "    \\n",
        "    return expanded_data\\n",
        "\\n",
        "# Create expanded dataset\\n",
        "expanded_data = create_balanced_dataset(target_size=1000)\\n",
        "\\n",
        "# Save expanded dataset\\n",
        "with open('data/expanded_journal_dataset.json', 'w') as f:\\n",
        "    json.dump(expanded_data, f, indent=2)\\n",
        "\\n",
        "print(\"âœ… Expanded dataset saved to data/expanded_journal_dataset.json\")\\n",
        "\\n",
        "# Analyze expanded dataset\\n",
        "emotion_counts = {}\\n",
        "for entry in expanded_data:\\n",
        "    emotion = entry['emotion']\\n",
        "    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\\n",
        "\\n",
        "print(\"\\nðŸ“Š Expanded Dataset Analysis:\")\\n",
        "print(\"=\" * 40)\\n",
        "print(\"Emotion distribution:\")\\n",
        "for emotion, count in sorted(emotion_counts.items()):\\n",
        "    print(f\"  {emotion}: {count} samples\")\\n",
        "\\n",
        "print(f\"\\nTotal samples: {len(expanded_data)}\")\\n",
        "print(f\"Unique emotions: {len(emotion_counts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## ðŸš€ Training with Expanded Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expanded_training"
      },
      "outputs": [],
      "source": [
        "# Complete training script with expanded dataset\\n",
        "import torch\\n",
        "import torch.nn as nn\\n",
        "from torch.utils.data import Dataset, DataLoader\\n",
        "from transformers import AutoModel, AutoTokenizer\\n",
        "from sklearn.preprocessing import LabelEncoder\\n",
        "from sklearn.model_selection import train_test_split\\n",
        "from sklearn.metrics import f1_score, accuracy_score\\n",
        "import numpy as np\\n",
        "\\n",
        "class ExpandedEmotionDataset(Dataset):\\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\\n",
        "        self.texts = texts\\n",
        "        self.labels = labels\\n",
        "        self.tokenizer = tokenizer\\n",
        "        self.max_length = max_length\\n",
        "    \\n",
        "    def __len__(self):\\n",
        "        return len(self.texts)\\n",
        "    \\n",
        "    def __getitem__(self, idx):\\n",
        "        text = self.texts[idx]\\n",
        "        label = self.labels[idx]\\n",
        "        \\n",
        "        encoding = self.tokenizer(\\n",
        "            text,\\n",
        "            truncation=True,\\n",
        "            padding='max_length',\\n",
        "            max_length=self.max_length,\\n",
        "            return_tensors='pt'\\n",
        "        )\\n",
        "        \\n",
        "        return {\\n",
        "            'input_ids': encoding['input_ids'].flatten(),\\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\\n",
        "        }\\n",
        "\\n",
        "class ExpandedEmotionClassifier(nn.Module):\\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", num_labels=12):\\n",
        "        super().__init__()\\n",
        "        self.num_labels = num_labels\\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\\n",
        "        self.dropout = nn.Dropout(0.3)\\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\\n",
        "    \\n",
        "    def forward(self, input_ids, attention_mask):\\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\\n",
        "        pooled_output = outputs.pooler_output\\n",
        "        logits = self.classifier(self.dropout(pooled_output))\\n",
        "        return logits\\n",
        "\\n",
        "def prepare_expanded_data(data, test_size=0.2, val_size=0.1):\\n",
        "    \"\"\"Prepare data for training with expanded dataset.\"\"\"\\n",
        "    print(\"ðŸ”§ Preparing expanded data...\")\\n",
        "    \\n",
        "    # Extract texts and emotions\\n",
        "    texts = [entry['content'] for entry in data]\\n",
        "    emotions = [entry['emotion'] for entry in data]\\n",
        "    \\n",
        "    # Create label encoder\\n",
        "    label_encoder = LabelEncoder()\\n",
        "    labels = label_encoder.fit_transform(emotions)\\n",
        "    \\n",
        "    print(f\"âœ… Label encoder created with {len(label_encoder.classes_)} classes\")\\n",
        "    print(f\"ðŸ“Š Classes: {list(label_encoder.classes_)}\")\\n",
        "    \\n",
        "    # Split data\\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\\n",
        "        texts, labels, test_size=test_size, random_state=42, stratify=labels\\n",
        "    )\\n",
        "    \\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\\n",
        "        X_temp, y_temp, test_size=val_size/(1-test_size), random_state=42, stratify=y_temp\\n",
        "    )\\n",
        "    \\n",
        "    print(f\"ðŸ“Š Data split:\")\\n",
        "    print(f\"  Training: {len(X_train)} samples\")\\n",
        "    print(f\"  Validation: {len(X_val)} samples\")\\n",
        "    print(f\"  Test: {len(X_test)} samples\")\\n",
        "    \\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), label_encoder\\n",
        "\\n",
        "def train_expanded_model(train_data, val_data, label_encoder, epochs=5, batch_size=16):\\n",
        "    \"\"\"Train the model with expanded dataset.\"\"\"\\n",
        "    print(\"ðŸš€ Training with expanded dataset...\")\\n",
        "    \\n",
        "    # Setup\\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n",
        "    print(f\"âœ… Using device: {device}\")\\n",
        "    \\n",
        "    # Load tokenizer\\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\n",
        "    \\n",
        "    # Create datasets\\n",
        "    X_train, y_train = train_data\\n",
        "    X_val, y_val = val_data\\n",
        "    \\n",
        "    train_dataset = ExpandedEmotionDataset(X_train, y_train, tokenizer)\\n",
        "    val_dataset = ExpandedEmotionDataset(X_val, y_val, tokenizer)\\n",
        "    \\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\\n",
        "    \\n",
        "    # Initialize model\\n",
        "    model = ExpandedEmotionClassifier(num_labels=len(label_encoder.classes_))\\n",
        "    model.to(device)\\n",
        "    \\n",
        "    # Setup training\\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\\n",
        "    criterion = nn.CrossEntropyLoss()\\n",
        "    \\n",
        "    # Training loop\\n",
        "    best_f1 = 0\\n",
        "    training_history = []\\n",
        "    \\n",
        "    for epoch in range(epochs):\\n",
        "        print(f\"\\nðŸ”„ Epoch {epoch + 1}/{epochs}\")\\n",
        "        \\n",
        "        # Training\\n",
        "        model.train()\\n",
        "        total_loss = 0\\n",
        "        \\n",
        "        for i, batch in enumerate(train_loader):\\n",
        "            input_ids = batch['input_ids'].to(device)\\n",
        "            attention_mask = batch['attention_mask'].to(device)\\n",
        "            labels = batch['labels'].to(device)\\n",
        "            \\n",
        "            optimizer.zero_grad()\\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\\n",
        "            loss = criterion(outputs, labels)\\n",
        "            loss.backward()\\n",
        "            optimizer.step()\\n",
        "            \\n",
        "            total_loss += loss.item()\\n",
        "            \\n",
        "            if i % 50 == 0:\\n",
        "                print(f\"  Batch {i}/{len(train_loader)}, Loss: {loss.item():.4f}\")\\n",
        "        \\n",
        "        # Validation\\n",
        "        model.eval()\\n",
        "        val_loss = 0\\n",
        "        all_preds = []\\n",
        "        all_labels = []\\n",
        "        \\n",
        "        with torch.no_grad():\\n",
        "            for batch in val_loader:\\n",
        "                input_ids = batch['input_ids'].to(device)\\n",
        "                attention_mask = batch['attention_mask'].to(device)\\n",
        "                labels = batch['labels'].to(device)\\n",
        "                \\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\\n",
        "                loss = criterion(outputs, labels)\\n",
        "                val_loss += loss.item()\\n",
        "                \\n",
        "                preds = torch.argmax(outputs, dim=1)\\n",
        "                all_preds.extend(preds.cpu().numpy())\\n",
        "                all_labels.extend(labels.cpu().numpy())\\n",
        "        \\n",
        "        # Calculate metrics\\n",
        "        avg_train_loss = total_loss / len(train_loader)\\n",
        "        avg_val_loss = val_loss / len(val_loader)\\n",
        "        f1_macro = f1_score(all_labels, all_preds, average='macro')\\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\\n",
        "        \\n",
        "        print(f\"ðŸ“Š Epoch {epoch + 1} Results:\")\\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\\n",
        "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\\n",
        "        print(f\"  Val F1 (Macro): {f1_macro:.4f}\")\\n",
        "        print(f\"  Val Accuracy: {accuracy:.4f}\")\\n",
        "        \\n",
        "        # Save best model\\n",
        "        if f1_macro > best_f1:\\n",
        "            best_f1 = f1_macro\\n",
        "            torch.save(model.state_dict(), 'best_expanded_model.pth')\\n",
        "            print(f\"ðŸ’¾ New best model saved! F1: {best_f1:.4f}\")\\n",
        "        \\n",
        "        training_history.append({\\n",
        "            'epoch': epoch,\\n",
        "            'train_loss': avg_train_loss,\\n",
        "            'val_loss': avg_val_loss,\\n",
        "            'val_f1_macro': f1_macro,\\n",
        "            'val_accuracy': accuracy\\n",
        "        })\\n",
        "    \\n",
        "    return model, training_history, best_f1\\n",
        "\\n",
        "# Load expanded dataset\\n",
        "with open('data/expanded_journal_dataset.json', 'r') as f:\\n",
        "    expanded_data = json.load(f)\\n",
        "\\n",
        "print(f\"ðŸ“Š Loaded {len(expanded_data)} expanded samples\")\\n",
        "\\n",
        "# Prepare data\\n",
        "train_data, val_data, test_data, label_encoder = prepare_expanded_data(expanded_data)\\n",
        "\\n",
        "# Train model\\n",
        "model, training_history, best_f1 = train_expanded_model(train_data, val_data, label_encoder)\\n",
        "\\n",
        "print(f\"\\nðŸŽ‰ Training completed!\")\\n",
        "print(f\"ðŸ“Š Best F1 Score: {best_f1:.4f}\")\\n",
        "print(f\"ðŸŽ¯ Target Achieved: {best_f1 >= 0.70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "testing"
      },
      "source": [
        "## ðŸ§ª Test the New Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_new_model"
      },
      "outputs": [],
      "source": [
        "# Test the new model with sample entries\\n",
        "def test_new_model():\\n",
        "    \"\"\"Test the new model with sample journal entries.\"\"\"\\n",
        "    print(\"ðŸ§ª Testing new expanded model...\")\\n",
        "    \\n",
        "    # Load best model\\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n",
        "    model = ExpandedEmotionClassifier(num_labels=len(label_encoder.classes_))\\n",
        "    model.load_state_dict(torch.load('best_expanded_model.pth'))\\n",
        "    model.to(device)\\n",
        "    model.eval()\\n",
        "    \\n",
        "    # Load tokenizer\\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\n",
        "    \\n",
        "    # Sample test entries\\n",
        "    test_entries = [\\n",
        "        \"I'm feeling really happy today! Everything is going well.\",\\n",
        "        \"I'm so frustrated with this project. Nothing is working.\",\\n",
        "        \"I feel anxious about the upcoming presentation.\",\\n",
        "        \"I'm grateful for all the support I've received.\",\\n",
        "        \"I'm feeling overwhelmed with all these tasks.\",\\n",
        "        \"I'm proud of what I've accomplished so far.\",\\n",
        "        \"I'm feeling sad and lonely today.\",\\n",
        "        \"I'm excited about the new opportunities ahead.\",\\n",
        "        \"I feel calm and peaceful right now.\",\\n",
        "        \"I'm hopeful that things will get better.\",\\n",
        "        \"I'm tired and need some rest.\",\\n",
        "        \"I'm content with how things are going.\"\\n",
        "    ]\\n",
        "    \\n",
        "    print(\"\\nðŸ“Š Testing Results:\")\\n",
        "    print(\"=\" * 80)\\n",
        "    \\n",
        "    for i, text in enumerate(test_entries, 1):\\n",
        "        # Tokenize\\n",
        "        encoding = tokenizer(\\n",
        "            text,\\n",
        "            truncation=True,\\n",
        "            padding='max_length',\\n",
        "            max_length=128,\\n",
        "            return_tensors='pt'\\n",
        "        )\\n",
        "        \\n",
        "        # Predict\\n",
        "        with torch.no_grad():\\n",
        "            input_ids = encoding['input_ids'].to(device)\\n",
        "            attention_mask = encoding['attention_mask'].to(device)\\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\\n",
        "            probabilities = torch.softmax(outputs, dim=1)\\n",
        "            predicted_class = torch.argmax(probabilities, dim=1).item()\\n",
        "            confidence = probabilities[0][predicted_class].item()\\n",
        "        \\n",
        "        # Get emotion label\\n",
        "        emotion = label_encoder.inverse_transform([predicted_class])[0]\\n",
        "        \\n",
        "        print(f\"\\n{i}. Text: {text}\")\\n",
        "        print(f\"   Predicted: {emotion} (confidence: {confidence:.3f})\")\\n",
        "        \\n",
        "        # Show top 3 predictions\\n",
        "        all_probs = probabilities[0].cpu().numpy()\\n",
        "        top_indices = np.argsort(all_probs)[-3:][::-1]\\n",
        "        print(\"   Top 3 predictions:\")\\n",
        "        for idx in top_indices:\\n",
        "            prob = all_probs[idx]\\n",
        "            emotion_name = label_encoder.inverse_transform([idx])[0]\\n",
        "            print(f\"     - {emotion_name}: {prob:.3f}\")\\n",
        "    \\n",
        "    print(\"\\nâœ… Model testing completed!\")\\n",
        "\\n",
        "# Test the new model\\n",
        "test_new_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download"
      },
      "source": [
        "## ðŸ’¾ Download Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_results"
      },
      "outputs": [],
      "source": [
        "# Download the trained model and results\\n",
        "from google.colab import files\\n",
        "\\n",
        "print(\"ðŸ“¥ Downloading results...\")\\n",
        "\\n",
        "# Download model\\n",
        "files.download('best_expanded_model.pth')\\n",
        "\\n",
        "# Save and download results\\n",
        "results = {\\n",
        "    'best_f1': best_f1,\\n",
        "    'target_achieved': best_f1 >= 0.70,\\n",
        "    'num_labels': len(label_encoder.classes_),\\n",
        "    'all_emotions': list(label_encoder.classes_),\\n",
        "    'training_history': training_history,\\n",
        "    'expanded_samples': len(expanded_data)\\n",
        "}\\n",
        "\\n",
        "with open('expanded_training_results.json', 'w') as f:\\n",
        "    json.dump(results, f, indent=2)\\n",
        "\\n",
        "files.download('expanded_training_results.json')\\n",
        "\\n",
        "print(\"âœ… Downloads completed!\")\\n",
        "print(f\"ðŸ“Š Final F1 Score: {best_f1:.4f}\")\\n",
        "print(f\"ðŸŽ¯ Target Achieved: {best_f1 >= 0.70}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}'''

    # Save the notebook
    with open('notebooks/expanded_dataset_training.ipynb', 'w') as f:
        f.write(notebook_content)

    print("âœ… Created Colab notebook: notebooks/expanded_dataset_training.ipynb")
    print("ðŸ“‹ Instructions:")
    print("  1. Download the notebook file")
    print("  2. Upload to Google Colab")
    print("  3. Set Runtime â†’ GPU")
    print("  4. Run all cells")
    print("  5. Expect 75-85% F1 score!")

if __name__ == "__main__":
    create_colab_notebook()
