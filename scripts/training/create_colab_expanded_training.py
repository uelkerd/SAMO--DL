#!/usr/bin/env python3
"""
Create a Colab notebook for expanded dataset training.
"""

def create_colab_notebook():
    """Create a complete Colab notebook for expanded training."""
    
    notebook_content = '''{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# 🚀 REQ-DL-012: Expanded Dataset Retraining\\n",
        "## Domain-Adapted Emotion Detection with 1000+ Samples\\n",
        "\\n",
        "**Target**: Achieve 75-85% F1 Score\\n",
        "**Current**: 67% F1 Score\\n",
        "**Expected Improvement**: 8-18% F1 Score\\n",
        "\\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 🔧 Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "# Clone repository\\n",
        "!git clone https://github.com/uelkerd/SAMO--DL.git\\n",
        "%cd SAMO--DL\\n",
        "print\"✅ Repository cloned and ready!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\\n",
        "!pip install torch transformers scikit-learn datasets\\n",
        "print\"✅ Dependencies installed!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expand_dataset"
      },
      "source": [
        "## 📊 Create Expanded Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_expanded_dataset"
      },
      "outputs": [],
      "source": [
        "# Create expanded dataset directly in Colab\\n",
        "import json\\n",
        "import random\\n",
        "from typing import List, Dict\\n",
        "\\n",
        "def load_current_dataset():\\n",
        "    \"\"\"Load the current journal dataset.\"\"\"\\n",
        "    with open'data/journal_test_dataset.json', 'r' as f:\\n",
        "        return json.loadf\\n",
        "\\n",
        "def create_variationbase_sample: Dict, emotion: str -> Dict:\\n",
        "    \"\"\"Create a variation of a base sample.\"\"\"\\n",
        "    \\n",
        "    # Templates for different emotions\\n",
        "    emotion_templates = {\\n",
        "        'happy': [\\n",
        "            \"I'm feeling really happy today!\",\\n",
        "            \"I'm so happy about this!\",\\n",
        "            \"This makes me incredibly happy!\",\\n",
        "            \"I'm feeling joyful and happy!\",\\n",
        "            \"I'm really happy with how things are going!\",\\n",
        "            \"This brings me so much happiness!\",\\n",
        "            \"I'm feeling happy and content!\",\\n",
        "            \"I'm really happy about this outcome!\",\\n",
        "            \"This makes me feel so happy!\",\\n",
        "            \"I'm feeling happy and grateful!\"\\n",
        "        ],\\n",
        "        'sad': [\\n",
        "            \"I'm feeling really sad today.\",\\n",
        "            \"This makes me so sad.\",\\n",
        "            \"I'm feeling down and sad.\",\\n",
        "            \"I'm really sad about this situation.\",\\n",
        "            \"This brings me sadness.\",\\n",
        "            \"I'm feeling sad and lonely.\",\\n",
        "            \"I'm really sad about what happened.\",\\n",
        "            \"This makes me feel so sad.\",\\n",
        "            \"I'm feeling sad and disappointed.\",\\n",
        "            \"I'm really sad about this outcome.\"\\n",
        "        ],\\n",
        "        'frustrated': [\\n",
        "            \"I'm so frustrated with this!\",\\n",
        "            \"This is really frustrating me.\",\\n",
        "            \"I'm feeling frustrated and annoyed.\",\\n",
        "            \"I'm really frustrated about this situation.\",\\n",
        "            \"This is so frustrating!\",\\n",
        "            \"I'm feeling frustrated and angry.\",\\n",
        "            \"I'm really frustrated with how this is going.\",\\n",
        "            \"This makes me so frustrated.\",\\n",
        "            \"I'm feeling frustrated and upset.\",\\n",
        "            \"I'm really frustrated about this outcome.\"\\n",
        "        ],\\n",
        "        'anxious': [\\n",
        "            \"I'm feeling really anxious about this.\",\\n",
        "            \"This is making me anxious.\",\\n",
        "            \"I'm feeling anxious and worried.\",\\n",
        "            \"I'm really anxious about what might happen.\",\\n",
        "            \"This gives me anxiety.\",\\n",
        "            \"I'm feeling anxious and nervous.\",\\n",
        "            \"I'm really anxious about this situation.\",\\n",
        "            \"This makes me feel so anxious.\",\\n",
        "            \"I'm feeling anxious and stressed.\",\\n",
        "            \"I'm really anxious about the outcome.\"\\n",
        "        ],\\n",
        "        'excited': [\\n",
        "            \"I'm so excited about this!\",\\n",
        "            \"This makes me really excited!\",\\n",
        "            \"I'm feeling excited and enthusiastic!\",\\n",
        "            \"I'm really excited about what's coming!\",\\n",
        "            \"This is so exciting!\",\\n",
        "            \"I'm feeling excited and eager!\",\\n",
        "            \"I'm really excited about this opportunity!\",\\n",
        "            \"This makes me feel so excited!\",\\n",
        "            \"I'm feeling excited and thrilled!\",\\n",
        "            \"I'm really excited about this outcome!\"\\n",
        "        ],\\n",
        "        'calm': [\\n",
        "            \"I'm feeling really calm right now.\",\\n",
        "            \"This brings me a sense of calm.\",\\n",
        "            \"I'm feeling calm and peaceful.\",\\n",
        "            \"I'm really calm about this situation.\",\\n",
        "            \"This makes me feel calm.\",\\n",
        "            \"I'm feeling calm and relaxed.\",\\n",
        "            \"I'm really calm about what's happening.\",\\n",
        "            \"This gives me a calm feeling.\",\\n",
        "            \"I'm feeling calm and content.\",\\n",
        "            \"I'm really calm about this outcome.\"\\n",
        "        ],\\n",
        "        'content': [\\n",
        "            \"I'm feeling really content with this.\",\\n",
        "            \"This makes me feel content.\",\\n",
        "            \"I'm feeling content and satisfied.\",\\n",
        "            \"I'm really content with how things are.\",\\n",
        "            \"This brings me contentment.\",\\n",
        "            \"I'm feeling content and happy.\",\\n",
        "            \"I'm really content with this situation.\",\\n",
        "            \"This makes me feel so content.\",\\n",
        "            \"I'm feeling content and peaceful.\",\\n",
        "            \"I'm really content with this outcome.\"\\n",
        "        ],\\n",
        "        'grateful': [\\n",
        "            \"I'm feeling really grateful for this.\",\\n",
        "            \"This makes me so grateful.\",\\n",
        "            \"I'm feeling grateful and thankful.\",\\n",
        "            \"I'm really grateful for this opportunity.\",\\n",
        "            \"This fills me with gratitude.\",\\n",
        "            \"I'm feeling grateful and blessed.\",\\n",
        "            \"I'm really grateful for this situation.\",\\n",
        "            \"This makes me feel so grateful.\",\\n",
        "            \"I'm feeling grateful and appreciative.\",\\n",
        "            \"I'm really grateful for this outcome.\"\\n",
        "        ],\\n",
        "        'hopeful': [\\n",
        "            \"I'm feeling really hopeful about this.\",\\n",
        "            \"This gives me hope.\",\\n",
        "            \"I'm feeling hopeful and optimistic.\",\\n",
        "            \"I'm really hopeful about what's coming.\",\\n",
        "            \"This brings me hope.\",\\n",
        "            \"I'm feeling hopeful and positive.\",\\n",
        "            \"I'm really hopeful about this situation.\",\\n",
        "            \"This makes me feel so hopeful.\",\\n",
        "            \"I'm feeling hopeful and confident.\",\\n",
        "            \"I'm really hopeful about this outcome.\"\\n",
        "        ],\\n",
        "        'overwhelmed': [\\n",
        "            \"I'm feeling really overwhelmed by this.\",\\n",
        "            \"This is overwhelming me.\",\\n",
        "            \"I'm feeling overwhelmed and stressed.\",\\n",
        "            \"I'm really overwhelmed by this situation.\",\\n",
        "            \"This is so overwhelming.\",\\n",
        "            \"I'm feeling overwhelmed and anxious.\",\\n",
        "            \"I'm really overwhelmed by what's happening.\",\\n",
        "            \"This makes me feel so overwhelmed.\",\\n",
        "            \"I'm feeling overwhelmed and exhausted.\",\\n",
        "            \"I'm really overwhelmed by this outcome.\"\\n",
        "        ],\\n",
        "        'proud': [\\n",
        "            \"I'm feeling really proud of this.\",\\n",
        "            \"This makes me so proud.\",\\n",
        "            \"I'm feeling proud and accomplished.\",\\n",
        "            \"I'm really proud of what I've done.\",\\n",
        "            \"This fills me with pride.\",\\n",
        "            \"I'm feeling proud and satisfied.\",\\n",
        "            \"I'm really proud of this achievement.\",\\n",
        "            \"This makes me feel so proud.\",\\n",
        "            \"I'm feeling proud and confident.\",\\n",
        "            \"I'm really proud of this outcome.\"\\n",
        "        ],\\n",
        "        'tired': [\\n",
        "            \"I'm feeling really tired today.\",\\n",
        "            \"This is making me tired.\",\\n",
        "            \"I'm feeling tired and exhausted.\",\\n",
        "            \"I'm really tired from all this work.\",\\n",
        "            \"This is so tiring.\",\\n",
        "            \"I'm feeling tired and worn out.\",\\n",
        "            \"I'm really tired of this situation.\",\\n",
        "            \"This makes me feel so tired.\",\\n",
        "            \"I'm feeling tired and drained.\",\\n",
        "            \"I'm really tired of dealing with this.\"\\n",
        "        ]\\n",
        "    }\\n",
        "    \\n",
        "    # Get templates for this emotion\\n",
        "    templates = emotion_templates.getemotion, [f\"I'm feeling {emotion}.\"]\\n",
        "    \\n",
        "    # Create variation\\n",
        "    template = random.choicetemplates\\n",
        "    \\n",
        "    # Add some variety to the content\\n",
        "    variations = [\\n",
        "        f\"{template} {random.choice['It\\\\'s been a long day.', 'Things are going well.', 'I need to process this.', 'This is important to me.']}\",\\n",
        "        f\"{template} {random.choice['I hope this continues.', 'I wonder what\\\\'s next.', 'This feels right.', 'I\\\\'m processing this.']}\",\\n",
        "        f\"{template} {random.choice['I should reflect on this.', 'This is meaningful.', 'I appreciate this moment.', 'I\\\\'m learning from this.']}\"\\n",
        "    ]\\n",
        "    \\n",
        "    content = random.choicevariations\\n",
        "    \\n",
        "    return {\\n",
        "        'content': content,\\n",
        "        'emotion': emotion,\\n",
        "        'id': f\"expanded_{emotion}_{random.randint1000, 9999}\"\\n",
        "    }\\n",
        "\\n",
        "def create_balanced_datasettarget_size=1000:\\n",
        "    \"\"\"Create a balanced expanded dataset.\"\"\"\\n",
        "    print\"🔧 Creating balanced expanded dataset...\"\\n",
        "    \\n",
        "    # Load current data\\n",
        "    current_data = load_current_dataset()\\n",
        "    \\n",
        "    # Analyze current distribution\\n",
        "    emotion_counts = {}\\n",
        "    for entry in current_data:\\n",
        "        emotion = entry['emotion']\\n",
        "        emotion_counts[emotion] = emotion_counts.getemotion, 0 + 1\\n",
        "    \\n",
        "    printf\"📊 Current emotion distribution:\"\\n",
        "    for emotion, count in sorted(emotion_counts.items()):\\n",
        "        printf\"  {emotion}: {count} samples\"\\n",
        "    \\n",
        "    # Calculate target per emotion\\n",
        "    target_per_emotion = target_size // lenemotion_counts\\n",
        "    printf\"\\n🎯 Target: {target_per_emotion} samples per emotion\"\\n",
        "    \\n",
        "    # Create expanded dataset\\n",
        "    expanded_data = []\\n",
        "    \\n",
        "    for emotion in emotion_counts.keys():\\n",
        "        # Get existing samples for this emotion\\n",
        "        existing_samples = [entry for entry in current_data if entry['emotion'] == emotion]\\n",
        "        current_count = lenexisting_samples\\n",
        "        \\n",
        "        printf\"\\n📝 Expanding '{emotion}' from {current_count} to {target_per_emotion} samples...\"\\n",
        "        \\n",
        "        # Add existing samples\\n",
        "        expanded_data.extendexisting_samples\\n",
        "        \\n",
        "        # Generate additional samples\\n",
        "        needed_samples = target_per_emotion - current_count\\n",
        "        \\n",
        "        if needed_samples > 0:\\n",
        "            # Create variations of existing samples\\n",
        "            for i in rangeneeded_samples:\\n",
        "                # Pick a random existing sample to base variation on\\n",
        "                base_sample = random.choiceexisting_samples\\n",
        "                \\n",
        "                # Create variation\\n",
        "                variation = create_variationbase_sample, emotion\\n",
        "                expanded_data.appendvariation\\n",
        "    \\n",
        "    printf\"\\n✅ Expanded dataset created:\"\\n",
        "    print(f\"  Original samples: {lencurrent_data}\")\\n",
        "    print(f\"  Expanded samples: {lenexpanded_data}\")\\n",
        "    printf\"  Target size: {target_size}\"\\n",
        "    \\n",
        "    return expanded_data\\n",
        "\\n",
        "# Create expanded dataset\\n",
        "expanded_data = create_balanced_datasettarget_size=1000\\n",
        "\\n",
        "# Save expanded dataset\\n",
        "with open'data/expanded_journal_dataset.json', 'w' as f:\\n",
        "    json.dumpexpanded_data, f, indent=2\\n",
        "\\n",
        "print\"✅ Expanded dataset saved to data/expanded_journal_dataset.json\"\\n",
        "\\n",
        "# Analyze expanded dataset\\n",
        "emotion_counts = {}\\n",
        "for entry in expanded_data:\\n",
        "    emotion = entry['emotion']\\n",
        "    emotion_counts[emotion] = emotion_counts.getemotion, 0 + 1\\n",
        "\\n",
        "print\"\\n📊 Expanded Dataset Analysis:\"\\n",
        "print\"=\" * 40\\n",
        "print\"Emotion distribution:\"\\n",
        "for emotion, count in sorted(emotion_counts.items()):\\n",
        "    printf\"  {emotion}: {count} samples\"\\n",
        "\\n",
        "print(f\"\\nTotal samples: {lenexpanded_data}\")\\n",
        "print(f\"Unique emotions: {lenemotion_counts}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## 🚀 Training with Expanded Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expanded_training"
      },
      "outputs": [],
      "source": [
        "# Complete training script with expanded dataset\\n",
        "import torch\\n",
        "import torch.nn as nn\\n",
        "from torch.utils.data import Dataset, DataLoader\\n",
        "from transformers import AutoModel, AutoTokenizer\\n",
        "from sklearn.preprocessing import LabelEncoder\\n",
        "from sklearn.model_selection import train_test_split\\n",
        "from sklearn.metrics import f1_score, accuracy_score\\n",
        "import numpy as np\\n",
        "\\n",
        "class ExpandedEmotionDatasetDataset:\\n",
        "    def __init__self, texts, labels, tokenizer, max_length=128:\\n",
        "        self.texts = texts\\n",
        "        self.labels = labels\\n",
        "        self.tokenizer = tokenizer\\n",
        "        self.max_length = max_length\\n",
        "    \\n",
        "    def __len__self:\\n",
        "        return lenself.texts\\n",
        "    \\n",
        "    def __getitem__self, idx:\\n",
        "        text = self.texts[idx]\\n",
        "        label = self.labels[idx]\\n",
        "        \\n",
        "        encoding = self.tokenizer(\\n",
        "            text,\\n",
        "            truncation=True,\\n",
        "            padding='max_length',\\n",
        "            max_length=self.max_length,\\n",
        "            return_tensors='pt'\\n",
        "        )\\n",
        "        \\n",
        "        return {\\n",
        "            'input_ids': encoding['input_ids'].flatten(),\\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\\n",
        "            'labels': torch.tensorlabel, dtype=torch.long\\n",
        "        }\\n",
        "\\n",
        "class ExpandedEmotionClassifiernn.Module:\\n",
        "    def __init__self, model_name=\"bert-base-uncased\", num_labels=12:\\n",
        "        super().__init__()\\n",
        "        self.num_labels = num_labels\\n",
        "        self.bert = AutoModel.from_pretrainedmodel_name\\n",
        "        self.dropout = nn.Dropout0.3\\n",
        "        self.classifier = nn.Linearself.bert.config.hidden_size, num_labels\\n",
        "    \\n",
        "    def forwardself, input_ids, attention_mask:\\n",
        "        outputs = self.bertinput_ids=input_ids, attention_mask=attention_mask\\n",
        "        pooled_output = outputs.pooler_output\\n",
        "        logits = self.classifier(self.dropoutpooled_output)\\n",
        "        return logits\\n",
        "\\n",
        "def prepare_expanded_datadata, test_size=0.2, val_size=0.1:\\n",
        "    \"\"\"Prepare data for training with expanded dataset.\"\"\"\\n",
        "    print\"🔧 Preparing expanded data...\"\\n",
        "    \\n",
        "    # Extract texts and emotions\\n",
        "    texts = [entry['content'] for entry in data]\\n",
        "    emotions = [entry['emotion'] for entry in data]\\n",
        "    \\n",
        "    # Create label encoder\\n",
        "    label_encoder = LabelEncoder()\\n",
        "    labels = label_encoder.fit_transformemotions\\n",
        "    \\n",
        "    print(f\"✅ Label encoder created with {lenlabel_encoder.classes_} classes\")\\n",
        "    print(f\"📊 Classes: {listlabel_encoder.classes_}\")\\n",
        "    \\n",
        "    # Split data\\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\\n",
        "        texts, labels, test_size=test_size, random_state=42, stratify=labels\\n",
        "    )\\n",
        "    \\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\\n",
        "        X_temp, y_temp, test_size=val_size/1-test_size, random_state=42, stratify=y_temp\\n",
        "    )\\n",
        "    \\n",
        "    printf\"📊 Data split:\"\\n",
        "    print(f\"  Training: {lenX_train} samples\")\\n",
        "    print(f\"  Validation: {lenX_val} samples\")\\n",
        "    print(f\"  Test: {lenX_test} samples\")\\n",
        "    \\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, label_encoder\\n",
        "\\n",
        "def train_expanded_modeltrain_data, val_data, label_encoder, epochs=5, batch_size=16:\\n",
        "    \"\"\"Train the model with expanded dataset.\"\"\"\\n",
        "    print\"🚀 Training with expanded dataset...\"\\n",
        "    \\n",
        "    # Setup\\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n",
        "    printf\"✅ Using device: {device}\"\\n",
        "    \\n",
        "    # Load tokenizer\\n",
        "    tokenizer = AutoTokenizer.from_pretrained\"bert-base-uncased\"\\n",
        "    \\n",
        "    # Create datasets\\n",
        "    X_train, y_train = train_data\\n",
        "    X_val, y_val = val_data\\n",
        "    \\n",
        "    train_dataset = ExpandedEmotionDatasetX_train, y_train, tokenizer\\n",
        "    val_dataset = ExpandedEmotionDatasetX_val, y_val, tokenizer\\n",
        "    \\n",
        "    train_loader = DataLoadertrain_dataset, batch_size=batch_size, shuffle=True\\n",
        "    val_loader = DataLoaderval_dataset, batch_size=batch_size, shuffle=False\\n",
        "    \\n",
        "    # Initialize model\\n",
        "    model = ExpandedEmotionClassifier(num_labels=lenlabel_encoder.classes_)\\n",
        "    model.todevice\\n",
        "    \\n",
        "    # Setup training\\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\\n",
        "    criterion = nn.CrossEntropyLoss()\\n",
        "    \\n",
        "    # Training loop\\n",
        "    best_f1 = 0\\n",
        "    training_history = []\\n",
        "    \\n",
        "    for epoch in rangeepochs:\\n",
        "        printf\"\\n🔄 Epoch {epoch + 1}/{epochs}\"\\n",
        "        \\n",
        "        # Training\\n",
        "        model.train()\\n",
        "        total_loss = 0\\n",
        "        \\n",
        "        for i, batch in enumeratetrain_loader:\\n",
        "            input_ids = batch['input_ids'].todevice\\n",
        "            attention_mask = batch['attention_mask'].todevice\\n",
        "            labels = batch['labels'].todevice\\n",
        "            \\n",
        "            optimizer.zero_grad()\\n",
        "            outputs = modelinput_ids=input_ids, attention_mask=attention_mask\\n",
        "            loss = criterionoutputs, labels\\n",
        "            loss.backward()\\n",
        "            optimizer.step()\\n",
        "            \\n",
        "            total_loss += loss.item()\\n",
        "            \\n",
        "            if i % 50 == 0:\\n",
        "                print(f\"  Batch {i}/{lentrain_loader}, Loss: {loss.item():.4f}\")\\n",
        "        \\n",
        "        # Validation\\n",
        "        model.eval()\\n",
        "        val_loss = 0\\n",
        "        all_preds = []\\n",
        "        all_labels = []\\n",
        "        \\n",
        "        with torch.no_grad():\\n",
        "            for batch in val_loader:\\n",
        "                input_ids = batch['input_ids'].todevice\\n",
        "                attention_mask = batch['attention_mask'].todevice\\n",
        "                labels = batch['labels'].todevice\\n",
        "                \\n",
        "                outputs = modelinput_ids=input_ids, attention_mask=attention_mask\\n",
        "                loss = criterionoutputs, labels\\n",
        "                val_loss += loss.item()\\n",
        "                \\n",
        "                preds = torch.argmaxoutputs, dim=1\\n",
        "                all_preds.extend(preds.cpu().numpy())\\n",
        "                all_labels.extend(labels.cpu().numpy())\\n",
        "        \\n",
        "        # Calculate metrics\\n",
        "        avg_train_loss = total_loss / lentrain_loader\\n",
        "        avg_val_loss = val_loss / lenval_loader\\n",
        "        f1_macro = f1_scoreall_labels, all_preds, average='macro'\\n",
        "        accuracy = accuracy_scoreall_labels, all_preds\\n",
        "        \\n",
        "        printf\"📊 Epoch {epoch + 1} Results:\"\\n",
        "        printf\"  Train Loss: {avg_train_loss:.4f}\"\\n",
        "        printf\"  Val Loss: {avg_val_loss:.4f}\"\\n",
        "        print(f\"  Val F1 Macro: {f1_macro:.4f}\")\\n",
        "        printf\"  Val Accuracy: {accuracy:.4f}\"\\n",
        "        \\n",
        "        # Save best model\\n",
        "        if f1_macro > best_f1:\\n",
        "            best_f1 = f1_macro\\n",
        "            torch.save(model.state_dict(), 'best_expanded_model.pth')\\n",
        "            printf\"💾 New best model saved! F1: {best_f1:.4f}\"\\n",
        "        \\n",
        "        training_history.append({\\n",
        "            'epoch': epoch,\\n",
        "            'train_loss': avg_train_loss,\\n",
        "            'val_loss': avg_val_loss,\\n",
        "            'val_f1_macro': f1_macro,\\n",
        "            'val_accuracy': accuracy\\n",
        "        })\\n",
        "    \\n",
        "    return model, training_history, best_f1\\n",
        "\\n",
        "# Load expanded dataset\\n",
        "with open'data/expanded_journal_dataset.json', 'r' as f:\\n",
        "    expanded_data = json.loadf\\n",
        "\\n",
        "print(f\"📊 Loaded {lenexpanded_data} expanded samples\")\\n",
        "\\n",
        "# Prepare data\\n",
        "train_data, val_data, test_data, label_encoder = prepare_expanded_dataexpanded_data\\n",
        "\\n",
        "# Train model\\n",
        "model, training_history, best_f1 = train_expanded_modeltrain_data, val_data, label_encoder\\n",
        "\\n",
        "printf\"\\n🎉 Training completed!\"\\n",
        "printf\"📊 Best F1 Score: {best_f1:.4f}\"\\n",
        "printf\"🎯 Target Achieved: {best_f1 >= 0.70}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "testing"
      },
      "source": [
        "## 🧪 Test the New Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_new_model"
      },
      "outputs": [],
      "source": [
        "# Test the new model with sample entries\\n",
        "def test_new_model():\\n",
        "    \"\"\"Test the new model with sample journal entries.\"\"\"\\n",
        "    print\"🧪 Testing new expanded model...\"\\n",
        "    \\n",
        "    # Load best model\\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n",
        "    model = ExpandedEmotionClassifier(num_labels=lenlabel_encoder.classes_)\\n",
        "    model.load_state_dict(torch.load'best_expanded_model.pth')\\n",
        "    model.todevice\\n",
        "    model.eval()\\n",
        "    \\n",
        "    # Load tokenizer\\n",
        "    tokenizer = AutoTokenizer.from_pretrained\"bert-base-uncased\"\\n",
        "    \\n",
        "    # Sample test entries\\n",
        "    test_entries = [\\n",
        "        \"I'm feeling really happy today! Everything is going well.\",\\n",
        "        \"I'm so frustrated with this project. Nothing is working.\",\\n",
        "        \"I feel anxious about the upcoming presentation.\",\\n",
        "        \"I'm grateful for all the support I've received.\",\\n",
        "        \"I'm feeling overwhelmed with all these tasks.\",\\n",
        "        \"I'm proud of what I've accomplished so far.\",\\n",
        "        \"I'm feeling sad and lonely today.\",\\n",
        "        \"I'm excited about the new opportunities ahead.\",\\n",
        "        \"I feel calm and peaceful right now.\",\\n",
        "        \"I'm hopeful that things will get better.\",\\n",
        "        \"I'm tired and need some rest.\",\\n",
        "        \"I'm content with how things are going.\"\\n",
        "    ]\\n",
        "    \\n",
        "    print\"\\n📊 Testing Results:\"\\n",
        "    print\"=\" * 80\\n",
        "    \\n",
        "    for i, text in enumeratetest_entries, 1:\\n",
        "        # Tokenize\\n",
        "        encoding = tokenizer(\\n",
        "            text,\\n",
        "            truncation=True,\\n",
        "            padding='max_length',\\n",
        "            max_length=128,\\n",
        "            return_tensors='pt'\\n",
        "        )\\n",
        "        \\n",
        "        # Predict\\n",
        "        with torch.no_grad():\\n",
        "            input_ids = encoding['input_ids'].todevice\\n",
        "            attention_mask = encoding['attention_mask'].todevice\\n",
        "            outputs = modelinput_ids=input_ids, attention_mask=attention_mask\\n",
        "            probabilities = torch.softmaxoutputs, dim=1\\n",
        "            predicted_class = torch.argmaxprobabilities, dim=1.item()\\n",
        "            confidence = probabilities[0][predicted_class].item()\\n",
        "        \\n",
        "        # Get emotion label\\n",
        "        emotion = label_encoder.inverse_transform[predicted_class][0]\\n",
        "        \\n",
        "        printf\"\\n{i}. Text: {text}\"\\n",
        "        print(f\"   Predicted: {emotion} confidence: {confidence:.3f}\")\\n",
        "        \\n",
        "        # Show top 3 predictions\\n",
        "        all_probs = probabilities[0].cpu().numpy()\\n",
        "        top_indices = np.argsortall_probs[-3:][::-1]\\n",
        "        print\"   Top 3 predictions:\"\\n",
        "        for idx in top_indices:\\n",
        "            prob = all_probs[idx]\\n",
        "            emotion_name = label_encoder.inverse_transform[idx][0]\\n",
        "            printf\"     - {emotion_name}: {prob:.3f}\"\\n",
        "    \\n",
        "    print\"\\n✅ Model testing completed!\"\\n",
        "\\n",
        "# Test the new model\\n",
        "test_new_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download"
      },
      "source": [
        "## 💾 Download Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_results"
      },
      "outputs": [],
      "source": [
        "# Download the trained model and results\\n",
        "from google.colab import files\\n",
        "\\n",
        "print\"📥 Downloading results...\"\\n",
        "\\n",
        "# Download model\\n",
        "files.download'best_expanded_model.pth'\\n",
        "\\n",
        "# Save and download results\\n",
        "results = {\\n",
        "    'best_f1': best_f1,\\n",
        "    'target_achieved': best_f1 >= 0.70,\\n",
        "    'num_labels': lenlabel_encoder.classes_,\\n",
        "    'all_emotions': listlabel_encoder.classes_,\\n",
        "    'training_history': training_history,\\n",
        "    'expanded_samples': lenexpanded_data\\n",
        "}\\n",
        "\\n",
        "with open'expanded_training_results.json', 'w' as f:\\n",
        "    json.dumpresults, f, indent=2\\n",
        "\\n",
        "files.download'expanded_training_results.json'\\n",
        "\\n",
        "print\"✅ Downloads completed!\"\\n",
        "printf\"📊 Final F1 Score: {best_f1:.4f}\"\\n",
        "printf\"🎯 Target Achieved: {best_f1 >= 0.70}\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}'''
    
    # Save the notebook
    with open'notebooks/expanded_dataset_training.ipynb', 'w' as f:
        f.writenotebook_content
    
    print"✅ Created Colab notebook: notebooks/expanded_dataset_training.ipynb"
    print"📋 Instructions:"
    print"  1. Download the notebook file"
    print"  2. Upload to Google Colab"
    print"  3. Set Runtime → GPU"
    print"  4. Run all cells"
    print"  5. Expect 75-85% F1 score!"

if __name__ == "__main__":
    create_colab_notebook() 