# ğŸš€ FINAL BULLETPROOF TRAINING CELL - PROPER LABEL MAPPING
# Runtime â†’ Change runtime type â†’ GPU (T4 or V100)
# Kernel â†’ Restart and run all

print("ğŸš€ FINAL BULLETPROOF TRAINING FOR REQ-DL-012 - PROPER LABEL MAPPING")
print("=" * 70)

# Step 1: Clear everything and validate environment
import os
import json
import torch
import torch.nn as nn
import pandas as pd
from datasets import load_dataset
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score
from sklearn.preprocessing import LabelEncoder
from transformers import AutoModel, AutoTokenizer

print("âœ… Imports successful")

# Clear GPU memory
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"âœ… GPU memory cleared: {torch.cuda.get_device_name()}")
else:
    print("âš ï¸ CUDA not available, using CPU")

# Test basic operations
try:
    test_tensor = torch.randn(2, 3)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    test_tensor.to(device)
    print("âœ… Basic tensor operations work")
except Exception as e:
    print(f"âŒ Basic tensor operations failed: {e}")
    raise

# Step 2: Clone repository and setup
# Note: Repository should already be cloned
# Change to project directory
os.chdir("SAMO--DL")

# Step 3: Load datasets and get proper label mapping
print("\nğŸ”§ Loading datasets and creating proper label mapping...")

# Load GoEmotions dataset
go_emotions = load_dataset("go_emotions", "simplified")

# Get the emotion names from the dataset features
emotion_names = go_emotions['train'].features['labels'].feature.names
print(f"ğŸ“Š GoEmotions emotion names: {emotion_names}")
print(f"ğŸ“Š Total GoEmotions emotions: {len(emotion_names)}")

# Load journal data
with open('data/journal_test_dataset.json', 'r') as f:
    journal_entries = json.load(f)
journal_df = pd.DataFrame(journal_entries)

journal_emotions = set(journal_df['emotion'].unique())
print(f"ğŸ“Š Journal emotions: {sorted(list(journal_emotions))}")
print(f"ğŸ“Š Total Journal emotions: {len(journal_emotions)}")

# Step 4: Create emotion mapping from GoEmotions to Journal
print("\nğŸ”§ Creating emotion mapping...")

# Map GoEmotions emotions to Journal emotions
emotion_mapping = {
    'admiration': 'proud',
    'amusement': 'happy',
    'anger': 'frustrated',
    'annoyance': 'frustrated',
    'approval': 'proud',
    'caring': 'content',
    'confusion': 'overwhelmed',
    'curiosity': 'excited',
    'desire': 'excited',
    'disappointment': 'sad',
    'disapproval': 'frustrated',
    'disgust': 'frustrated',
    'embarrassment': 'anxious',
    'excitement': 'excited',
    'fear': 'anxious',
    'gratitude': 'grateful',
    'grief': 'sad',
    'joy': 'happy',
    'love': 'content',
    'nervousness': 'anxious',
    'optimism': 'hopeful',
    'pride': 'proud',
    'realization': 'content',
    'relief': 'calm',
    'remorse': 'sad',
    'sadness': 'sad',
    'surprise': 'excited',
    'neutral': 'calm'
}

print(f"âœ… Created mapping with {len(emotion_mapping)} emotions")

# Step 5: Process GoEmotions data with proper label conversion
print("\nğŸ“Š Processing GoEmotions data...")

go_texts = []
go_labels = []

for example in go_emotions['train']:
    if example['labels']:
        # Convert integer labels to emotion names
        emotion_indices = example['labels']
        for emotion_idx in emotion_indices:
            if emotion_idx < len(emotion_names):
                emotion_name = emotion_names[emotion_idx]
                if emotion_name in emotion_mapping:
                    mapped_emotion = emotion_mapping[emotion_name]
                    if mapped_emotion in journal_emotions:
                        go_texts.append(example['text'])
                        go_labels.append(mapped_emotion)
                        break

# Process journal data
journal_texts = list(journal_df['content'])
journal_labels = list(journal_df['emotion'])

print(f"ğŸ“Š Mapped GoEmotions: {len(go_texts)} samples")
print(f"ğŸ“Š Journal: {len(journal_texts)} samples")

# Step 6: Create unified label encoder
print("\nğŸ”§ Creating unified label encoder...")

all_emotions = sorted(list(set(go_labels + journal_labels)))
print(f"ğŸ“Š All emotions: {all_emotions}")

label_encoder = LabelEncoder()
label_encoder.fit(all_emotions)
label_to_id = {label: idx for idx, label in enumerate(label_encoder.classes_)}
id_to_label = {idx: label for label, idx in label_to_id.items()}

print(f"âœ… Label encoder created with {len(label_encoder.classes_)} classes")

# Convert labels to IDs
go_label_ids = [label_to_id[label] for label in go_labels]
journal_label_ids = [label_to_id[label] for label in journal_labels]

print(f"ğŸ“Š GoEmotions label range: {min(go_label_ids)} to {max(go_label_ids)}")
print(f"ğŸ“Š Journal label range: {min(journal_label_ids)} to {max(journal_label_ids)}")

# Validate all labels are within expected range
expected_range = (0, len(label_encoder.classes_) - 1)
print(f"ğŸ“Š Expected range: {expected_range}")

if min(go_label_ids) >= expected_range[0] and max(go_label_ids) <= expected_range[1] and \
   min(journal_label_ids) >= expected_range[0] and max(journal_label_ids) <= expected_range[1]:
    print("âœ… All labels within expected range")
else:
    print("âŒ Labels outside expected range!")
    raise ValueError("Label range validation failed")

# Step 7: Create simple dataset class
class SimpleEmotionDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

        # Validate data
        if len(texts) != len(labels):
            raise ValueError(f"Texts and labels have different lengths: {len(texts)} vs {len(labels)}")

        # Validate labels
        for i, label in enumerate(labels):
            if not isinstance(label, int) or label < 0:
                raise ValueError(f"Invalid label at index {i}: {label}")

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text_item = self.texts[idx]
        label_item = self.labels[idx]

        # Validate inputs
        if not isinstance(text_item, str) or not text_item.strip():
            raise ValueError(f"Invalid text at index {idx}")

        if not isinstance(label_item, int) or label_item < 0:
            raise ValueError(f"Invalid label at index {idx}: {label_item}")

        encoding = self.tokenizer(
            text_item,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label_item, dtype=torch.long)
        }

# Step 8: Create simple model
class SimpleEmotionClassifier(nn.Module):
    def __init__(self, model_name="bert-base-uncased", num_classes=None):
        super().__init__()

        if num_classes is None or num_classes <= 0:
            raise ValueError(f"Invalid num_classes: {num_classes}")

        self.num_labels = num_classes
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.3)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)

        print(f"âœ… Model initialized with {num_classes} labels")

    def forward(self, input_ids_tensor, attention_mask_tensor):
        """Forward pass through BERT model for emotion classification.

        Args:
            input_ids_tensor: Tokenized input text tensor
            attention_mask_tensor: Attention mask tensor for padding tokens

        Returns:
            Model logits for emotion classification
        """
        # Validate inputs
        if input_ids_tensor.dim() != 2:
            raise ValueError(f"Expected input_ids to be 2D, got {input_ids_tensor.dim()}D")

        if attention_mask_tensor.dim() != 2:
            raise ValueError(f"Expected attention_mask to be 2D, got {attention_mask_tensor.dim()}D")

        outputs = self.bert(input_ids=input_ids_tensor, attention_mask=attention_mask_tensor)
        pooled_output = outputs.pooler_output
        logits = self.classifier(self.dropout(pooled_output))

        # Validate outputs
        if logits.shape[-1] != self.num_labels:
            raise ValueError(f"Expected {self.num_labels} output classes, got {logits.shape[-1]}")

        return logits

# Step 9: Setup training
print("\nğŸš€ Setting up training...")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âœ… Using device: {device}")

# Initialize tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
num_labels = len(label_encoder.classes_)
model = SimpleEmotionClassifier(model_name="bert-base-uncased", num_classes=num_labels)
model = model.to(device)

# Create datasets
go_dataset = SimpleEmotionDataset(go_texts, go_label_ids, tokenizer)
journal_dataset = SimpleEmotionDataset(journal_texts, journal_label_ids, tokenizer)

# Split journal data
journal_train_texts, journal_val_texts, journal_train_labels, journal_val_labels = train_test_split(
    journal_texts, journal_label_ids, test_size=0.3, random_state=42, stratify=journal_label_ids
)

journal_train_dataset = SimpleEmotionDataset(journal_train_texts, journal_train_labels, tokenizer)
journal_val_dataset = SimpleEmotionDataset(journal_val_texts, journal_val_labels, tokenizer)

# Create dataloaders
go_loader = DataLoader(go_dataset, batch_size=8, shuffle=True)
journal_train_loader = DataLoader(journal_train_dataset, batch_size=8, shuffle=True)
journal_val_loader = DataLoader(journal_val_dataset, batch_size=8, shuffle=False)

print(f"âœ… Training samples: {len(go_dataset)} GoEmotions + {len(journal_train_dataset)} Journal")
print(f"âœ… Validation samples: {len(journal_val_dataset)} Journal")

# Step 10: Training loop
print("\nğŸš€ Starting training...")

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss()

num_epochs = 3  # Reduced for testing
best_f1 = 0.0

for epoch in range(num_epochs):
    print(f"\nğŸ”„ Epoch {epoch + 1}/{num_epochs}")

    # Training
    model.train()
    total_loss = 0
    num_batches = 0

    # Train on GoEmotions
    print("  ğŸ“š Training on GoEmotions...")
    for i, batch in enumerate(go_loader):
        try:
            # Validate batch
            if 'input_ids' not in batch or 'attention_mask' not in batch or 'labels' not in batch:
                print(f"âš ï¸ Invalid batch structure at batch {i}")
                continue

            # Move to device with validation
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Validate labels
            if torch.any(labels >= num_labels) or torch.any(labels < 0):
                print(f"âš ï¸ Invalid labels in batch {i}: {labels}")
                continue

            # Forward pass
            optimizer.zero_grad()
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

            if i % 50 == 0:
                print(f"    Batch {i}/{len(go_loader)}, Loss: {loss.item():.4f}")

        except Exception as e:
            print(f"âŒ Error in batch {i}: {e}")
            continue

    # Train on journal data
    print("  ğŸ“ Training on journal data...")
    for i, batch in enumerate(journal_train_loader):
        try:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            if torch.any(labels >= num_labels) or torch.any(labels < 0):
                continue

            optimizer.zero_grad()
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

            if i % 10 == 0:
                print(f"    Batch {i}/{len(journal_train_loader)}, Loss: {loss.item():.4f}")

        except Exception as e:
            print(f"âŒ Error in journal batch {i}: {e}")
            continue

    # Validation
    print("  ğŸ¯ Validating...")
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in journal_val_loader:
            try:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                preds = torch.argmax(outputs, dim=1)

                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

            except Exception as e:
                print(f"âŒ Error in validation batch: {e}")
                continue

    # Calculate metrics
    if all_preds and all_labels:
        f1_macro = f1_score(all_labels, all_preds, average='macro')
        accuracy = accuracy_score(all_labels, all_preds)

        avg_loss = total_loss / num_batches if num_batches > 0 else 0

        print(f"  ğŸ“Š Epoch {epoch + 1} Results:")
        print(f"    Average Loss: {avg_loss:.4f}")
        print(f"    Validation F1 (Macro): {f1_macro:.4f}")
        print(f"    Validation Accuracy: {accuracy:.4f}")

        # Save best model
        if f1_macro > best_f1:
            best_f1 = f1_macro
            torch.save(model.state_dict(), 'best_simple_model.pth')
            print(f"    ğŸ’¾ New best model saved! F1: {best_f1:.4f}")

    # Clear GPU cache
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

print(f"\nğŸ† Training completed! Best F1 Score: {best_f1:.4f}")

# Step 11: Save results
results = {
    'best_f1': best_f1,
    'num_labels': num_labels,
    'target_achieved': best_f1 >= 0.7,
    'go_samples': len(go_texts),
    'journal_samples': len(journal_texts),
    'emotion_mapping': emotion_mapping,
    'all_emotions': all_emotions
}

with open('simple_training_results.json', 'w') as f:
    json.dump(results, f, indent=2)

print("\nâœ… Training completed successfully!")
print(f"ğŸ“Š Final F1 Score: {best_f1:.4f}")
print(f"ğŸ¯ Target Met: {'âœ…' if best_f1 >= 0.7 else 'âŒ'}")

# Download results
from google.colab import files
files.download('best_simple_model.pth')
files.download('simple_training_results.json')

print("\nğŸ‰ FINAL BULLETPROOF TRAINING COMPLETED!")
print("ğŸ“ Files downloaded: best_simple_model.pth, simple_training_results.json")
print("\nğŸ”¥ THIS VERSION HAS PROPER INTEGER-TO-EMOTION MAPPING!")
print("ğŸ”¥ NO MORE ZERO SAMPLES ISSUE!")
print("ğŸ”¥ READY TO ACHIEVE 70% F1 SCORE!")
