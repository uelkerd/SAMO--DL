#!/usr/bin/env python3
"""
Improve Expanded Training Notebook
Adds GPU optimizations, better error handling, and performance enhancements
"""

import json
import re

def improve_notebook():
    """Improve the expanded training notebook with enhancements."""

    # Read the current notebook
    with open('notebooks/expanded_dataset_training.ipynb', 'r') as f:
        notebook = json.load(f)

    # Find the training function cell
    training_cell_idx = None
    for i, cell in enumerate(notebook['cells']):
        if cell['cell_type'] == 'code' and 'train_expanded_model' in str(cell['source']):
            training_cell_idx = i
            break

    if training_cell_idx is None:
        print("âŒ Could not find training function cell")
        return

    # Get the training function source
    training_source = notebook['cells'][training_cell_idx]['source']

    # Add GPU optimizations after device setup
    device_pattern = r'print\(f"âœ… Using device: \{device\}"\)'
    gpu_optimizations = '''
    # GPU optimizations
    if torch.cuda.is_available():
        print("ðŸ”§ Applying GPU optimizations...")
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        print(f"ðŸ“Š GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        print(f"ðŸ“Š Available Memory: {torch.cuda.memory_allocated(0) / 1e9:.1f} GB")

    # Clear GPU cache
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
'''

    # Replace the device setup
    new_source = re.sub(
        device_pattern,
        f'print(f"âœ… Using device: {{device}}")\n{gpu_optimizations}',
        training_source
    )

    # Add early stopping
    early_stopping_pattern = r'if f1_macro > best_f1:'
    early_stopping_code = '''
        # Early stopping check
        if epoch > 2 and f1_macro < best_f1 * 0.95:
            print(f"ðŸ›‘ Early stopping triggered. F1 dropped below 95% of best.")
            break

        if f1_macro > best_f1:'''

    new_source = re.sub(early_stopping_pattern, early_stopping_code, new_source)

    # Add learning rate scheduling
    lr_scheduler_pattern = r'optimizer = torch\.optim\.AdamW\(model\.parameters\(\), lr=2e-5\)'
    lr_scheduler_code = '''optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)'''

    new_source = re.sub(lr_scheduler_pattern, lr_scheduler_code, new_source)

    # Add scheduler step
    scheduler_step_pattern = r'print\(f"ðŸ’¾ New best model saved! F1: \{best_f1:.4f\}"\)'
    scheduler_step_code = '''print(f"ðŸ’¾ New best model saved! F1: {best_f1:.4f}")
            scheduler.step(f1_macro)'''

    new_source = re.sub(scheduler_step_pattern, scheduler_step_code, new_source)

    # Add mixed precision training
    mixed_precision_pattern = r'import torch\.nn as nn'
    mixed_precision_code = '''import torch.nn as nn
from torch.cuda.amp import autocast, GradScaler'''

    new_source = re.sub(mixed_precision_pattern, mixed_precision_code, new_source)

    # Add scaler initialization
    scaler_init_pattern = r'criterion = nn\.CrossEntropyLoss\(\)'
    scaler_init_code = '''criterion = nn.CrossEntropyLoss()
    scaler = GradScaler()'''

    new_source = re.sub(scaler_init_pattern, scaler_init_code, new_source)

    # Add mixed precision training loop
    training_loop_pattern = r'optimizer\.zero_grad\(\)\s+outputs = model\(input_ids=input_ids, attention_mask=attention_mask\)\s+loss = criterion\(outputs, labels\)\s+loss\.backward\(\)\s+optimizer\.step\(\)'
    training_loop_code = '''optimizer.zero_grad()
            with autocast():
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()'''

    new_source = re.sub(training_loop_pattern, training_loop_code, new_source)

    # Update the cell
    notebook['cells'][training_cell_idx]['source'] = new_source

    # Save the improved notebook
    with open('notebooks/expanded_dataset_training_improved.ipynb', 'w') as f:
        json.dump(notebook, f, indent=2)

    print("âœ… Improved notebook saved as 'notebooks/expanded_dataset_training_improved.ipynb'")
    print("ðŸ“‹ Improvements added:")
    print("  - GPU optimizations (cudnn benchmark, memory management)")
    print("  - Early stopping to prevent overfitting")
    print("  - Learning rate scheduling with ReduceLROnPlateau")
    print("  - Mixed precision training for faster training")
    print("  - Better memory management")

if __name__ == "__main__":
    improve_notebook()
